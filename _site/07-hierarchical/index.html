






<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta http-equiv="last-modified" content="2024-02-27 09:14:21 +0000">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- meta "search-domain" used for google site search function google_search() -->
    <meta name="search-domain" value="">
    <link rel="stylesheet" type="text/css" href="../assets/css/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/lesson.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/syntax.css" />
     <link rel="stylesheet" type="text/css" href="../assets/css/fonts.css" />
    
    <link rel="stylesheet" type="text/css" href="../assets/css/katex.min.css" />
    
    <link rel="license" href="#license-info" />

    



    <!-- Favicons for everyone -->
    <link rel="apple-touch-icon-precomposed" sizes="57x57" href="../assets/favicons/incubator/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="../assets/favicons/incubator/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="../assets/favicons/incubator/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="../assets/favicons/incubator/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon-precomposed" sizes="60x60" href="../assets/favicons/incubator/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon-precomposed" sizes="120x120" href="../assets/favicons/incubator/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon-precomposed" sizes="76x76" href="../assets/favicons/incubator/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../assets/favicons/incubator/apple-touch-icon-152x152.png" />
    <link rel="icon" type="image/png" href="../assets/favicons/incubator/favicon-196x196.png" sizes="196x196" />
    <link rel="icon" type="image/png" href="../assets/favicons/incubator/favicon-96x96.png" sizes="96x96" />
    <link rel="icon" type="image/png" href="../assets/favicons/incubator/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="../assets/favicons/incubator/favicon-16x16.png" sizes="16x16" />
    <link rel="icon" type="image/png" href="../assets/favicons/incubator/favicon-128.png" sizes="128x128" />
    <meta name="application-name" content="The Carpentries Incubator - High dimensional statistics with R"/>
    <meta name="msapplication-TileColor" content="#FFFFFF" />
    <meta name="msapplication-TileImage" content="../assets/favicons/incubator/mstile-144x144.png" />
    <meta name="msapplication-square70x70logo" content="../assets/favicons/incubator/mstile-70x70.png" />
    <meta name="msapplication-square150x150logo" content="../assets/favicons/incubator/mstile-150x150.png" />
    <meta name="msapplication-wide310x150logo" content="../assets/favicons/incubator/mstile-310x150.png" />
    <meta name="msapplication-square310x310logo" content="../assets/favicons/incubator/mstile-310x310.png" />


    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
	<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
	<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
	<![endif]-->
  <title>
  Hierarchical clustering &ndash; High dimensional statistics with R
  </title>

  </head>
  <body>
    


<div class="panel panel-default life-cycle">
  <div id="life-cycle" class="panel-body alpha">
    This lesson is in the early stages of development (Alpha version)
  </div>
</div>





    <div class="container">
      
















  
  










<nav class="navbar navbar-default">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

      
      
      <a href="../index.html" class="pull-left">
        <img class="navbar-logo" src="../assets/img/incubator-logo-blue.svg" alt="The Carpentries Incubator logo" />
      </a>
      

      
      <a class="navbar-brand" href="../index.html">Home</a>

    </div>
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">

	
        <li><a href="../CODE_OF_CONDUCT.html">Code of Conduct</a></li>

        
	
        <li><a href="../setup.html">Setup</a></li>

        
        
        <li class="dropdown">
          <a href="../" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Episodes <span class="caret"></span></a>
          <ul class="dropdown-menu">
            
            
            <li><a href="../01-introduction-to-high-dimensional-data/index.html">Introduction to high-dimensional data</a></li>
            
            
            <li><a href="../02-high-dimensional-regression/index.html">Regression with many outcomes</a></li>
            
            
            <li><a href="../03-regression-regularisation/index.html">Regularised regression</a></li>
            
            
            <li><a href="../04-principal-component-analysis/index.html">Principal component analysis</a></li>
            
            
            <li><a href="../05-factor-analysis/index.html">Factor analysis</a></li>
            
            
            <li><a href="../06-k-means/index.html">K-means</a></li>
            
            
            <li><a href="../07-hierarchical/index.html">Hierarchical clustering</a></li>
            
	    <li role="separator" class="divider"></li>
            <li><a href="../aio/index.html">All in one page (Beta)</a></li>
          </ul>
        </li>
        
	

	
	
        <li class="dropdown">
          <a href="../" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Extras <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="../reference.html">Reference</a></li>
            
            
            <li><a href="../about/index.html">About</a></li>
            
            
            <li><a href="../figures/index.html">Figures</a></li>
            
            
            <li><a href="../guide/index.html">Instructor Notes</a></li>
            
            
            <li><a href="../slides/index.html">Lecture slides</a></li>
            
          </ul>
        </li>
	

	
        <li><a href="../LICENSE.html">License</a></li>
	
	
	<li><a href="/edit//_episodes_rmd/07-hierarchical.Rmd" data-checker-ignore>Improve this page <span class="glyphicon glyphicon-pencil" aria-hidden="true"></span></a></li>
	
	
      </ul>
      <form class="navbar-form navbar-right" role="search" id="search" onsubmit="google_search(); return false;">
        <div class="form-group">
          <input type="text" id="google-search" placeholder="Search..." aria-label="Google site search">
        </div>
      </form>
    </div>
  </div>
</nav>

      


<div class="alert alert-info text-center" role="alert">
  This lesson is part of
  <a href="https://github.com/carpentries-incubator/proposals/#the-carpentries-incubator" data-checker-ignore>
    The Carpentries Incubator</a>, a place to share and use each other's
  Carpentries-style lessons. <strong>This lesson has not been reviewed by and is
  not endorsed by The Carpentries</strong>.
</div>




      

















  
  











<div class="row">
  <div class="col-xs-1">
    <h3 class="text-left">
      
      <a href="../06-k-means/index.html"><span class="glyphicon glyphicon-menu-left" aria-hidden="true"></span><span class="sr-only">previous episode</span></a>
      
    </h3>
  </div>
  <div class="col-xs-10">
    
    <h3 class="maintitle"><a href="../">High dimensional statistics with R</a></h3>
    
  </div>
  <div class="col-xs-1">
    <h3 class="text-right">
      
      <a href="../"><span class="glyphicon glyphicon-menu-up" aria-hidden="true"></span><span class="sr-only">lesson home</span></a>
      
    </h3>
  </div>
</div>

<article>
<div class="row">
  <div class="col-md-1">
  </div>
  <div class="col-md-10">
    <h1 class="maintitle">Hierarchical clustering</h1>
  </div>
  <div class="col-md-1">
  </div>
</div>












<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 60 min
      <br/>
      <strong>Exercises:</strong> 10 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>What is hierarchical clustering and how does it differ from other clustering methods?</p>
</li>
	
	<li><p>How do we carry out hierarchical clustering in R?</p>
</li>
	
	<li><p>What distance matrix and linkage methods should we use?</p>
</li>
	
	<li><p>How can we validate identified clusters?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Understand when to use hierarchical clustering on high-dimensional data.</p>
</li>
	
	<li><p>Perform hierarchical clustering on high-dimensional data and evaluate dendrograms.</p>
</li>
	
	<li><p>Explore different distance matrix and linkage methods.</p>
</li>
	
	<li><p>Use the Dunn index to validate clustering methods.</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h1 id="why-use-hierarchical-clustering-on-high-dimensional-data">Why use hierarchical clustering on high-dimensional data?</h1>

<p>When analysing high-dimensional data in the life sciences, it is often useful
to identify groups of similar data points to understand more about the relationships
within the dataset. In <em>hierarchical clustering</em> an algorithm groups similar
data points (or observations) into groups (or clusters). This results in a set
of clusters, where each cluster is distinct, and the data points within each
cluster have similar characteristics. The clustering algorithm works by iteratively
grouping data points so that different clusters may exist at different stages
of the algorithm’s progression.</p>

<p>Unlike K-means clustering, <em>hierarchical clustering</em> does not require the
number of clusters $k$ to be specified by the user before the analysis is carried
out. Hierarchical clustering also provides an attractive <em>dendrogram</em>, a
tree-like diagram showing the degree of similarity between clusters.</p>

<p>The dendrogram is a key feature of hierarchical clustering. This tree-shaped graph allows
the similarity between data points in a dataset to be visualised and the
arrangement of clusters produced by the analysis to be illustrated. Dendrograms are created
using a distance (or dissimilarity) that quantify how different are pairs of observations,
and a clustering algorithm to fuse groups of similar data points together.</p>

<p>In this episode we will explore hierarchical clustering for identifying
clusters in high-dimensional data. We will use <em>agglomerative</em> hierarchical
clustering (see box) in this episode.</p>

<blockquote class="callout">
  <h2 id="agglomerative-and-divisive-hierarchical-clustering">Agglomerative and Divisive hierarchical clustering</h2>

  <p>There are two main methods of carrying out hierarchical clustering:
agglomerative clustering and divisive clustering. 
The former is a ‘bottom-up’ approach to clustering whereby the clustering
approach begins with each data point (or observation) 
being regarded as being in its own separate cluster. Pairs of data points are
merged as we move up the tree. 
Divisive clustering is a ‘top-down’ approach in which all data points start
in a single cluster and an algorithm is used to split groups of data points
from this main group.</p>
</blockquote>

<h1 id="the-agglomerative-hierarchical-clustering-algorithm">The agglomerative hierarchical clustering algorithm</h1>

<p>To start with, we measure distance
(or dissimilarity) between pairs of observations. Initially, and at the bottom
of the dendrogram, each observation is considered to be in its own individual
cluster. We start the clustering procedure by fusing the two observations that
are most similar according to a distance matrix. Next, the next-most similar observations are fused
so that the total number of clusters is <em>number of observations</em> - 2 (see
panel below). Groups of observations may then be merged into a larger cluster
(see next panel below, green box). This process continues until all the observations are included
in a single cluster.</p>

<div class="figure" style="text-align: center">
<img src="../fig/hierarchical_clustering_1.png" alt="Figure 1a: Example data showing two clusters of observation pairs" width="500px" />
<p class="caption">Figure 1a: Example data showing two clusters of observation pairs</p>
</div>

<div class="figure" style="text-align: center">
<img src="../fig/hierarchical_clustering_2.png" alt="Figure 1b: Example data showing fusing of one observation into larger cluster" width="500px" />
<p class="caption">Figure 1b: Example data showing fusing of one observation into larger cluster</p>
</div>

<h1 id="a-motivating-example">A motivating example</h1>

<p>To motivate this lesson, let’s first look at an example where hierarchical
clustering is really useful, and then we can understand how to apply it in more
detail. To do this, we’ll return to the large methylation dataset we worked
with in the regression lessons. Let’s load the data and look at it.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"minfi"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"here"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"ComplexHeatmap"</span><span class="p">)</span><span class="w">

</span><span class="n">methyl</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readRDS</span><span class="p">(</span><span class="n">here</span><span class="p">(</span><span class="s2">"data/methylation.rds"</span><span class="p">))</span><span class="w">

</span><span class="c1"># transpose this Bioconductor dataset to show features in columns</span><span class="w">
</span><span class="n">methyl_mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">assay</span><span class="p">(</span><span class="n">methyl</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p>Looking at a heatmap of these data, we may spot some patterns – many columns
appear to have a similar methylation levels across all rows. However, they are
all quite jumbled at the moment, so it’s hard to tell how many line up exactly.</p>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-heatmap-noclust-1.png" alt="plot of chunk heatmap-noclust" width="432" />
<p class="caption">plot of chunk heatmap-noclust</p>
</div>

<p>We can order these data to make the patterns more clear using hierarchical
clustering. To do this, we can change the arguments we pass to 
<code class="language-plaintext highlighter-rouge">Heatmap()</code> from the <strong><code class="language-plaintext highlighter-rouge">ComplexHeatmap</code></strong> package. <code class="language-plaintext highlighter-rouge">Heatmap()</code>
groups features based on dissimilarity (here, Euclidean distance) and orders
rows and columns to show clustering of features and observations.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Heatmap</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">,</span><span class="w">
  </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Methylation level"</span><span class="p">,</span><span class="w">
  </span><span class="n">cluster_rows</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">cluster_columns</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w">
  </span><span class="n">row_dend_width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">unit</span><span class="p">(</span><span class="m">0.2</span><span class="p">,</span><span class="w"> </span><span class="s2">"npc"</span><span class="p">),</span><span class="w">
  </span><span class="n">column_dend_height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">unit</span><span class="p">(</span><span class="m">0.2</span><span class="p">,</span><span class="w"> </span><span class="s2">"npc"</span><span class="p">),</span><span class="w">
  </span><span class="n">show_row_names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">show_column_names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w">
  </span><span class="n">row_title</span><span class="o">=</span><span class="s2">"Individuals"</span><span class="p">,</span><span class="w"> </span><span class="n">column_title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Methylation sites"</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-heatmap-clust-1.png" alt="plot of chunk heatmap-clust" width="432" />
<p class="caption">plot of chunk heatmap-clust</p>
</div>

<p>We can see that clustering the features (CpG sites) results in an overall
gradient of high to low methylation levels from left to right. Maybe more
interesting is the fact that the rows (corresponding to individuals) are now
grouped according to methylation patterns. For example, 12 samples seem to have
lower methylation levels for a small subset of CpG sites in the middle, relative
to all the other samples. It’s not clear without investigating further what the
cause of this is – it could be a batch effect, or a known grouping (e.g., old
vs young samples). However, clustering like this can be a useful part of
exploratory analysis of data to build hypotheses.</p>

<p>Now, let’s cover the inner workings of hierarchical clustering in more detail.
There are two things to consider before carrying out clustering:</p>
<ul>
  <li>how to define dissimilarity between observations using a distance matrix, and</li>
  <li>how to define dissimilarity between clusters and when to fuse separate clusters.</li>
</ul>

<h1 id="creating-the-distance-matrix">Creating the distance matrix</h1>
<p>Agglomerative hierarchical clustering is performed in two steps: calculating
the distance matrix (containing distances between pairs of observations) and
iteratively grouping observations into clusters using this matrix.</p>

<p>There are different ways to
specify a distance matrix for clustering:</p>

<ul>
  <li>Specify distance as a pre-defined option using the <code class="language-plaintext highlighter-rouge">method</code> argument in
<code class="language-plaintext highlighter-rouge">dist()</code>. Methods include <code class="language-plaintext highlighter-rouge">euclidean</code> (default), <code class="language-plaintext highlighter-rouge">maximum</code> and <code class="language-plaintext highlighter-rouge">manhattan</code>.</li>
  <li>Create a self-defined function which calculates distance from a matrix or
from two vectors. The function should only contain one argument.</li>
</ul>

<p>Of pre-defined methods of calculating the distance matrix, Euclidean is one of
the most commonly used. This method calculates the shortest straight-line
distances between pairs of observations.</p>

<p>Another option is to use a correlation matrix as the input matrix to the
clustering algorithm. The type of distance matrix used in hierarchical
clustering can have a big effect on the resulting tree. The decision of which
distance matrix to use before carrying out hierarchical clustering depends on the
type of data and question to be addressed.</p>

<h1 id="linkage-methods">Linkage methods</h1>

<p>The second step in performing hierarchical clustering after defining the
distance matrix (or another function defining similarity between data points)
is determining how to fuse different clusters.</p>

<p><em>Linkage</em> is used to define dissimilarity between groups of observations
(or clusters) and is used to create the hierarchical structure in the
dendrogram. Different linkage methods of creating a dendrogram are discussed
below.</p>

<p><code class="language-plaintext highlighter-rouge">hclust()</code> supports various linkage methods (e.g <code class="language-plaintext highlighter-rouge">complete</code>,
<code class="language-plaintext highlighter-rouge">single</code>, <code class="language-plaintext highlighter-rouge">ward D</code>, <code class="language-plaintext highlighter-rouge">ward D2</code>, <code class="language-plaintext highlighter-rouge">average</code>, <code class="language-plaintext highlighter-rouge">median</code>) and these are also supported
within the <code class="language-plaintext highlighter-rouge">Heatmap()</code> function. The method used to perform hierarchical
clustering in <code class="language-plaintext highlighter-rouge">Heatmap()</code> can be specified by the arguments
<code class="language-plaintext highlighter-rouge">clustering_method_rows</code> and <code class="language-plaintext highlighter-rouge">clustering_method_columns</code>. Each linkage method
uses a slightly different algorithm to calculate how clusters are fused together
and therefore different clustering decisions are made depending on the linkage
method used.</p>

<p>Complete linkage (the default in <code class="language-plaintext highlighter-rouge">hclust()</code>) works by computing all pairwise
dissimilarities between data points in different clusters. For each pair of two clusters,
it sets their dissimilarity ($d$) to the maximum dissimilarity value observed
between any of these clusters’ constituent points. The two clusters
with smallest value of $d$ are then fused.</p>

<h1 id="computing-a-dendrogram">Computing a dendrogram</h1>

<p>Dendograms are useful tools to visualise the grouping of points and clusters into bigger clusters.
We can create and plot dendrograms in R using <code class="language-plaintext highlighter-rouge">hclust()</code> which takes
a distance matrix as input and creates the associated tree using hierarchical
clustering. Here we create some example data to carry out hierarchical
clustering.</p>

<p>Let’s generate 20 data points in 2D space. Each
point belongs to one of three classes. Suppose we did not know which class
data points belonged to and we want to identify these via cluster analysis.
Hierarchical clustering carried out on the data can be used to produce a
dendrogram showing how the data is partitioned into clusters. But how do we
interpret this dendrogram? Let’s explore this using our example data.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#First, create some example data with two variables x1 and x2</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">450</span><span class="p">)</span><span class="w">
</span><span class="n">example_data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="w">
    </span><span class="n">x1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="m">8</span><span class="p">,</span><span class="w"> </span><span class="m">4.5</span><span class="p">),</span><span class="w">
    </span><span class="n">x2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="m">6</span><span class="p">,</span><span class="w"> </span><span class="m">3.4</span><span class="p">)</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="c1">#plot the data and name data points by row numbers</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">example_data</span><span class="o">$</span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">example_data</span><span class="o">$</span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"n"</span><span class="p">)</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="w">
    </span><span class="n">example_data</span><span class="o">$</span><span class="n">x1</span><span class="p">,</span><span class="w">
    </span><span class="n">example_data</span><span class="o">$</span><span class="n">x2</span><span class="p">,</span><span class="w">
    </span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rownames</span><span class="p">(</span><span class="n">example_data</span><span class="p">),</span><span class="w">
    </span><span class="n">cex</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.7</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plotexample-1.png" alt="plot of chunk plotexample" width="432" />
<p class="caption">plot of chunk plotexample</p>
</div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## calculate distance matrix using euclidean distance</span><span class="w">
</span><span class="n">dist_m</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dist</span><span class="p">(</span><span class="n">example_data</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"euclidean"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<blockquote class="challenge">
  <h2 id="challenge-1">Challenge 1</h2>

  <p>Use <code class="language-plaintext highlighter-rouge">hclust()</code> to implement hierarchical clustering using the
distance matrix <code class="language-plaintext highlighter-rouge">dist_m</code> and 
the <code class="language-plaintext highlighter-rouge">complete</code> linkage method and plot the results as a dendrogram using
<code class="language-plaintext highlighter-rouge">plot()</code>.</p>

  <blockquote class="solution">
    <h2 id="solution">Solution:</h2>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">clust</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">hclust</span><span class="p">(</span><span class="n">dist_m</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"complete"</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">clust</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plotclustex-1.png" alt="plot of chunk plotclustex" width="432" />
<p class="caption">plot of chunk plotclustex</p>
</div>
  </blockquote>
</blockquote>

<p>This dendrogram shows similarities/differences in distances between data points.
Each leaf of the dendrogram represents one of the 20 data points. These leaves
fuse into branches as the height increases. Observations that are similar fuse into
the same branches. The height at which any two
data points fuse indicates how different these two points are. Points that fuse
at the top of the tree are very different from each other compared with two
points that fuse at the bottom of the tree, which are quite similar. You can
see this by comparing the position of similar/dissimilar points according to
the scatterplot with their position on the tree.</p>

<h1 id="identifying-clusters-based-on-the-dendrogram">Identifying clusters based on the dendrogram</h1>

<p>To do this, we can make a horizontal cut through the dendrogram at a user-defined height. 
The sets of observations beneath this cut can be thought of as distinct clusters. For
example, a cut at height 10 produces two downstream clusters while a cut at
height 4 produces six downstream clusters.</p>

<p>We can cut the dendrogram to determine number of clusters at different heights
using <code class="language-plaintext highlighter-rouge">cutree()</code>. This function cuts a dendrogram into several
groups (or clusters) where the number of desired groups is controlled by the
user, by defining either <code class="language-plaintext highlighter-rouge">k</code> (number of groups) or <code class="language-plaintext highlighter-rouge">h</code> (height at which tree is
cut).</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## k is a user defined parameter determining</span><span class="w">
</span><span class="c1">## the desired number of clusters at which to cut the treee</span><span class="w">
</span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code> [1] 1 1 1 2 1 1 1 1 1 2 2 1 1 3 1 1 2 2 1 2
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## h is a user defined parameter determining</span><span class="w">
</span><span class="c1">## the numeric height at which to cut the tree</span><span class="w">
</span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code> [1] 1 1 1 2 1 1 1 1 1 2 2 1 1 3 1 1 2 2 1 2
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## both give same results </span><span class="w">

</span><span class="n">four_cut</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="p">)</span><span class="w">

</span><span class="c1">## we can produce the cluster each observation belongs to</span><span class="w">
</span><span class="c1">## using the mutate and count functions</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">
</span><span class="n">example_cl</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mutate</span><span class="p">(</span><span class="n">example_data</span><span class="p">,</span><span class="w"> </span><span class="n">cluster</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">four_cut</span><span class="p">)</span><span class="w">
</span><span class="n">count</span><span class="p">(</span><span class="n">example_cl</span><span class="p">,</span><span class="w"> </span><span class="n">cluster</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  cluster n
1       1 2
2       2 4
3       3 1
4       4 3
5       5 4
6       6 2
7       7 3
8       8 1
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#plot cluster each point belongs to on original scatterplot</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">example_cl</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">factor</span><span class="p">(</span><span class="n">cluster</span><span class="p">)))</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">geom_point</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-cutree-1.png" alt="plot of chunk cutree" width="432" />
<p class="caption">plot of chunk cutree</p>
</div>

<p>Note that this cut produces 8 clusters (two before the cut and another six
downstream of the cut).</p>

<blockquote class="challenge">
  <h2 id="challenge-2">Challenge 2:</h2>

  <p>Identify the value of <code class="language-plaintext highlighter-rouge">k</code> in <code class="language-plaintext highlighter-rouge">cutree()</code> that gives the same
output as <code class="language-plaintext highlighter-rouge">h = 5</code></p>

  <blockquote class="solution">
    <h2 id="solution-1">Solution:</h2>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">clust</span><span class="p">)</span><span class="w">
</span><span class="c1">## create horizontal line at height = 5</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-h-k-ex-plot-1.png" alt="plot of chunk h-k-ex-plot" width="432" />
<p class="caption">plot of chunk h-k-ex-plot</p>
</div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code> [1] 1 2 2 3 4 5 2 5 1 6 6 2 5 7 4 4 6 6 5 6
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">7</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code> [1] 1 2 2 3 4 5 2 5 1 6 6 2 5 7 4 4 6 6 5 6
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">five_cut</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w">

</span><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">
</span><span class="n">example_cl</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mutate</span><span class="p">(</span><span class="n">example_data</span><span class="p">,</span><span class="w"> </span><span class="n">cluster</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">five_cut</span><span class="p">)</span><span class="w">
</span><span class="n">count</span><span class="p">(</span><span class="n">example_cl</span><span class="p">,</span><span class="w"> </span><span class="n">cluster</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  cluster n
1       1 2
2       2 4
3       3 1
4       4 3
5       5 4
6       6 5
7       7 1
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">example_cl</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">factor</span><span class="p">(</span><span class="n">cluster</span><span class="p">)))</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">geom_point</span><span class="p">()</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-h-k-ex-plot-2.png" alt="plot of chunk h-k-ex-plot" width="432" />
<p class="caption">plot of chunk h-k-ex-plot</p>
</div>

    <p>Seven clusters (<code class="language-plaintext highlighter-rouge">k = 7</code>) gives similar results to <code class="language-plaintext highlighter-rouge">h = 5</code>. You can plot a
horizontal line on the dendrogram at <code class="language-plaintext highlighter-rouge">h = 5</code> to help identify
corresponding value of <code class="language-plaintext highlighter-rouge">k</code>.</p>
  </blockquote>
</blockquote>

<h1 id="highlighting-dendrogram-branches">Highlighting dendrogram branches</h1>

<p>In addition to visualising cluster identity in scatter plots, it is also possible to
highlight branches in dentrograms. In this example, we calculate a distance matrix between
samples in the <code class="language-plaintext highlighter-rouge">methyl_mat</code> dataset. We then draw boxes round clusters obtained with <code class="language-plaintext highlighter-rouge">cutree</code>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## create a distance matrix using euclidean method</span><span class="w">
</span><span class="n">distmat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dist</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">)</span><span class="w">
</span><span class="c1">## hierarchical clustering using complete method</span><span class="w">
</span><span class="n">clust</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">hclust</span><span class="p">(</span><span class="n">distmat</span><span class="p">)</span><span class="w">
</span><span class="c1">## plot resulting dendrogram</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">clust</span><span class="p">)</span><span class="w">

</span><span class="c1">## draw border around three clusters</span><span class="w">
</span><span class="n">rect.hclust</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">border</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="o">:</span><span class="m">6</span><span class="p">)</span><span class="w">
</span><span class="c1">## draw border around two clusters</span><span class="w">
</span><span class="n">rect.hclust</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">border</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="o">:</span><span class="m">6</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-method-1.png" alt="plot of chunk plot-clust-method" width="432" />
<p class="caption">plot of chunk plot-clust-method</p>
</div>
<p>We can also colour clusters downstream of a specified cut using <code class="language-plaintext highlighter-rouge">color_branches()</code>
from the <strong><code class="language-plaintext highlighter-rouge">dendextend</code></strong> package.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## cut tree at height = 4</span><span class="w">
</span><span class="n">cut</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">50</span><span class="p">)</span><span class="w">

</span><span class="n">library</span><span class="p">(</span><span class="s2">"dendextend"</span><span class="p">)</span><span class="w">
</span><span class="n">avg_dend_obj</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.dendrogram</span><span class="p">(</span><span class="n">clust</span><span class="p">)</span><span class="w">      
</span><span class="c1">## colour branches of dendrogram depending on clusters</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">color_branches</span><span class="p">(</span><span class="n">avg_dend_obj</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">50</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-coloured-branches-1.png" alt="plot of chunk plot-coloured-branches" width="432" />
<p class="caption">plot of chunk plot-coloured-branches</p>
</div>

<h1 id="the-effect-of-different-linkage-methods">The effect of different linkage methods</h1>
<p>Now let us look into changing the default behaviour of <code class="language-plaintext highlighter-rouge">hclust()</code>. Imagine we have two crescent-shaped point clouds as shown below.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># These two functions are to help us make crescents. Don't worry it you do not understand all this code.</span><span class="w">
</span><span class="c1"># The importent bit is the object "cres", which consists of two columns (x and y coordinates of two crescents).</span><span class="w">
</span><span class="n">is.insideCircle</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">co</span><span class="p">,</span><span class="w"> </span><span class="n">r</span><span class="o">=</span><span class="m">0.5</span><span class="p">,</span><span class="w"> </span><span class="n">offs</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">0</span><span class="p">)){</span><span class="w">
  </span><span class="nf">sqrt</span><span class="p">((</span><span class="n">co</span><span class="p">[,</span><span class="m">1</span><span class="p">]</span><span class="o">+</span><span class="n">offs</span><span class="p">[</span><span class="m">1</span><span class="p">])</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">co</span><span class="p">[,</span><span class="m">2</span><span class="p">]</span><span class="o">+</span><span class="n">offs</span><span class="p">[</span><span class="m">2</span><span class="p">])</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">r</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="n">make.crescent</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">n</span><span class="p">){</span><span class="w">
  </span><span class="n">raw</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cbind</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">runif</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="m">-0.5</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">runif</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="m">-0.5</span><span class="p">)</span><span class="w">
  </span><span class="n">raw</span><span class="p">[</span><span class="n">is.insideCircle</span><span class="p">(</span><span class="n">raw</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="o">!</span><span class="n">is.insideCircle</span><span class="p">(</span><span class="n">raw</span><span class="p">,</span><span class="w"> </span><span class="n">offs</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">-0.2</span><span class="p">)),]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="c1"># make x/y data in shape of two crescents</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span><span class="w">
</span><span class="n">cres1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">make.crescent</span><span class="p">(</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="c1"># 1st crescent</span><span class="w">
</span><span class="n">cres2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">make.crescent</span><span class="p">(</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="c1"># 2nd crescent</span><span class="w">
</span><span class="n">cres2</span><span class="p">[,</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="o">-</span><span class="n">cres2</span><span class="p">[,</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="m">-0.1</span><span class="w"> </span><span class="c1"># flip 2nd crescent upside-down and shift down</span><span class="w">
</span><span class="n">cres2</span><span class="p">[,</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cres2</span><span class="p">[,</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0.5</span><span class="w"> </span><span class="c1"># shift second crescent to the right</span><span class="w">

</span><span class="n">cres</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rbind</span><span class="p">(</span><span class="n">cres1</span><span class="p">,</span><span class="w"> </span><span class="n">cres2</span><span class="p">)</span><span class="w"> </span><span class="c1"># concatente x/y values</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-crescents-1.png" alt="plot of chunk crescents" width="432" />
<p class="caption">plot of chunk crescents</p>
</div>
<p>We might expect that the crescents are resolved into separate clusters. But if we
run hierarchical clustering with the default arguments, we get this:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cresClass</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">cres</span><span class="p">)),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="c1"># save partition for colouring</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">cresClass</span><span class="p">)</span><span class="w"> </span><span class="c1"># colour scatterplot by partition</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-cresClustDefault-1.png" alt="plot of chunk cresClustDefault" width="432" />
<p class="caption">plot of chunk cresClustDefault</p>
</div>

<blockquote class="challenge">
  <h2 id="challenge-3">Challenge 3</h2>

  <p>Carry out hierarchical clustering on the <code class="language-plaintext highlighter-rouge">cres</code> data that we generated above.
Try out different linkage methods and use <code class="language-plaintext highlighter-rouge">cutree()</code> to split each resulting
dendrogram into two clusters. Plot the results colouring the dots according to
their inferred cluster identity.</p>

  <p>Which method(s) give you the expected clustering outcome?</p>

  <p>Hint: Check <code class="language-plaintext highlighter-rouge">?hclust</code> to see the possible values of the argument <code class="language-plaintext highlighter-rouge">method</code> (the linkage method used).</p>

  <blockquote class="solution">
    <h2 id="solution-2">Solution:</h2>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#?hclust</span><span class="w">
</span><span class="c1"># "complete", "single", "ward.D", "ward.D2", "average", "mcquitty", "median" or "centroid"</span><span class="w">
</span><span class="n">cresClassSingle</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">cres</span><span class="p">),</span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"single"</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">cresClassSingle</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"single"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-comp-1.png" alt="plot of chunk plot-clust-comp" width="432" />
<p class="caption">plot of chunk plot-clust-comp</p>
</div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cresClassWard.D</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">cres</span><span class="p">),</span><span class="w"> </span><span class="n">method</span><span class="o">=</span><span class="s2">"ward.D"</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">cresClassWard.D</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"ward.D"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-wardD-1.png" alt="plot of chunk plot-clust-wardD" width="432" />
<p class="caption">plot of chunk plot-clust-wardD</p>
</div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cresClassWard.D2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">cres</span><span class="p">),</span><span class="w"> </span><span class="n">method</span><span class="o">=</span><span class="s2">"ward.D2"</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">cresClassWard.D2</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"ward.D2"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-wardD2-1.png" alt="plot of chunk plot-clust-wardD2" width="432" />
<p class="caption">plot of chunk plot-clust-wardD2</p>
</div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cresClassAverage</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">cres</span><span class="p">),</span><span class="w"> </span><span class="n">method</span><span class="o">=</span><span class="s2">"average"</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">cresClassAverage</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"average"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-average-1.png" alt="plot of chunk plot-clust-average" width="432" />
<p class="caption">plot of chunk plot-clust-average</p>
</div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cresClassMcquitty</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">cres</span><span class="p">),</span><span class="w"> </span><span class="n">method</span><span class="o">=</span><span class="s2">"mcquitty"</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">cresClassMcquitty</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"mcquitty"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-mcq-1.png" alt="plot of chunk plot-clust-mcq" width="432" />
<p class="caption">plot of chunk plot-clust-mcq</p>
</div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cresClassMedian</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">cres</span><span class="p">),</span><span class="w"> </span><span class="n">method</span><span class="o">=</span><span class="s2">"median"</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w"> 
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">cresClassMedian</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"median"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-median-1.png" alt="plot of chunk plot-clust-median" width="432" />
<p class="caption">plot of chunk plot-clust-median</p>
</div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cresClassCentroid</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">cres</span><span class="p">),</span><span class="w"> </span><span class="n">method</span><span class="o">=</span><span class="s2">"centroid"</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">cresClassCentroid</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"centroid"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-centroid-1.png" alt="plot of chunk plot-clust-centroid" width="432" />
<p class="caption">plot of chunk plot-clust-centroid</p>
</div>

    <p>The linkage methods <code class="language-plaintext highlighter-rouge">single</code>, <code class="language-plaintext highlighter-rouge">ward.D</code>, and <code class="language-plaintext highlighter-rouge">average</code> resolve each crescent as a separate cluster.</p>

  </blockquote>
</blockquote>

<p>The help page of <code class="language-plaintext highlighter-rouge">hclust()</code> gives some intuition on linkage methods. It describes <code class="language-plaintext highlighter-rouge">complete</code>
(the default) and <code class="language-plaintext highlighter-rouge">single</code> as opposite ends of a spectrum with all other methods in between.
When using complete linkage, the distance between two clusters is assumed to be the distance
between both clusters’ most distant points. This opposite it true for single linkage, where
the minimum distance between any two points, one from each of two clusters is used. Single
linkage is described as friends-of-friends appporach - and really, it groups all close-together
points into the same cluster (thus resolving one cluster per crescent). Complete linkage on the
other hand recognises that some points a the tip of a crescent are much closer to points in the
other crescent and so it splits both crescents.</p>

<h1 id="using-different-distance-methods">Using different distance methods</h1>

<p>So far, we’ve been using Euclidean distance to define the dissimilarity
or distance between observations. However, this isn’t always the best
metric for how dissimilar different observations are. Let’s make an
example to demonstrate. Here, we’re creating two samples each with
ten observations of random noise:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">20</span><span class="p">)</span><span class="w">
</span><span class="n">cor_example</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="w">
  </span><span class="n">sample_a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="m">10</span><span class="p">),</span><span class="w">
  </span><span class="n">sample_b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="m">10</span><span class="p">)</span><span class="w">
</span><span class="p">)</span><span class="w">
</span><span class="n">rownames</span><span class="p">(</span><span class="n">cor_example</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">paste</span><span class="p">(</span><span class="w">
  </span><span class="s2">"Feature"</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">nrow</span><span class="p">(</span><span class="n">cor_example</span><span class="p">)</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>Now, let’s create a new sample that has exactly the same pattern across all
our features as <code class="language-plaintext highlighter-rouge">sample_a</code>, just offset by 5:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cor_example</span><span class="o">$</span><span class="n">sample_c</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cor_example</span><span class="o">$</span><span class="n">sample_a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">5</span><span class="w">
</span></code></pre></div></div>

<p>You can see that this is a lot like the <code class="language-plaintext highlighter-rouge">assay()</code> of our methylation object
from earlier, where columns are observations or samples, and rows are features:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">head</span><span class="p">(</span><span class="n">cor_example</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            sample_a    sample_b sample_c
Feature 1  1.1626853 -0.02013537 6.162685
Feature 2 -0.5859245 -0.15038222 4.414076
Feature 3  1.7854650 -0.62812676 6.785465
Feature 4 -1.3325937  1.32322085 3.667406
Feature 5 -0.4465668 -1.52135057 4.553433
Feature 6  0.5696061 -0.43742787 5.569606
</code></pre></div></div>

<p>If we plot a heatmap of this, we can see that <code class="language-plaintext highlighter-rouge">sample_a</code> and <code class="language-plaintext highlighter-rouge">sample_b</code> are
grouped together because they have a small distance to each other, despite
being quite different in their pattern across the different features.
In contrast, <code class="language-plaintext highlighter-rouge">sample_a</code> and <code class="language-plaintext highlighter-rouge">sample_c</code> are very distant, despite having
<em>exactly</em> the same pattern across the different features.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Heatmap</span><span class="p">(</span><span class="n">as.matrix</span><span class="p">(</span><span class="n">cor_example</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-heatmap-cor-example-1.png" alt="plot of chunk heatmap-cor-example" width="432" />
<p class="caption">plot of chunk heatmap-cor-example</p>
</div>

<p>We can see that more clearly if we do a line plot:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## create a blank plot (type = "n" means don't draw anything)</span><span class="w">
</span><span class="c1">## with an x range to hold the number of features we have.</span><span class="w">
</span><span class="c1">## the range of y needs to be enough to show all the values for every feature</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="w">
  </span><span class="m">1</span><span class="o">:</span><span class="n">nrow</span><span class="p">(</span><span class="n">cor_example</span><span class="p">),</span><span class="w">
  </span><span class="nf">rep</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">cor_example</span><span class="p">),</span><span class="w"> </span><span class="m">5</span><span class="p">),</span><span class="w">
  </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"n"</span><span class="w">
</span><span class="p">)</span><span class="w">
</span><span class="c1">## draw a red line for sample_a</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="n">cor_example</span><span class="o">$</span><span class="n">sample_a</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"firebrick"</span><span class="p">)</span><span class="w">
</span><span class="c1">## draw a blue line for sample_b</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="n">cor_example</span><span class="o">$</span><span class="n">sample_b</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dodgerblue"</span><span class="p">)</span><span class="w">
</span><span class="c1">## draw a green line for sample_c</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="n">cor_example</span><span class="o">$</span><span class="n">sample_c</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"forestgreen"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-lineplot-cor-example-1.png" alt="plot of chunk lineplot-cor-example" width="432" />
<p class="caption">plot of chunk lineplot-cor-example</p>
</div>

<p>We can see that <code class="language-plaintext highlighter-rouge">sample_a</code> and <code class="language-plaintext highlighter-rouge">sample_c</code> have exactly the same pattern across
all of the different features. However, due to the overall difference between
the values, they have a high distance to each other.
We can see that if we cluster and plot the data ourselves using Euclidean
distance:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">clust_dist</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">cor_example</span><span class="p">)))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">clust_dist</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-clust-euc-cor-example-1.png" alt="plot of chunk clust-euc-cor-example" width="432" />
<p class="caption">plot of chunk clust-euc-cor-example</p>
</div>

<p>In some cases, we might want to ensure that samples that have similar patterns,
whether that be of gene expression, or DNA methylation, have small distances
to each other. Correlation is a measure of this kind of similarity in pattern.
However, high correlations indicate similarity, while for a distance measure
we know that high distances indicate dissimilarity. Therefore, if we wanted
to cluster observations based on the correlation, or the similarity of patterns,
we can use <code class="language-plaintext highlighter-rouge">1 - cor(x)</code> as the distance metric.
The input to <code class="language-plaintext highlighter-rouge">hclust()</code> must be a <code class="language-plaintext highlighter-rouge">dist</code> object, so we also need to call
<code class="language-plaintext highlighter-rouge">as.dist()</code> on it before passing it in.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cor_as_dist</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.dist</span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">cor</span><span class="p">(</span><span class="n">cor_example</span><span class="p">))</span><span class="w">
</span><span class="n">clust_cor</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">hclust</span><span class="p">(</span><span class="n">cor_as_dist</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">clust_cor</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-clust-cor-cor-example-1.png" alt="plot of chunk clust-cor-cor-example" width="432" />
<p class="caption">plot of chunk clust-cor-cor-example</p>
</div>

<p>Now, <code class="language-plaintext highlighter-rouge">sample_a</code> and <code class="language-plaintext highlighter-rouge">sample_c</code> that have identical patterns across the features
are grouped together, while <code class="language-plaintext highlighter-rouge">sample_b</code> is seen as distant because it has a
different pattern, even though its values are closer to <code class="language-plaintext highlighter-rouge">sample_a</code>.
Using your own distance function is often useful, especially if you have missing
or unusual data. It’s often possible to use correlation and other custom
distance functions to functions that perform hierarchical clustering, such as
<code class="language-plaintext highlighter-rouge">pheatmap()</code> and <code class="language-plaintext highlighter-rouge">stats::heatmap()</code>:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## pheatmap allows you to select correlation directly</span><span class="w">
</span><span class="n">pheatmap</span><span class="p">(</span><span class="n">as.matrix</span><span class="p">(</span><span class="n">cor_example</span><span class="p">),</span><span class="w"> </span><span class="n">clustering_distance_cols</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"correlation"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-heatmap-cor-cor-example-1.png" alt="plot of chunk heatmap-cor-cor-example" width="432" />
<p class="caption">plot of chunk heatmap-cor-cor-example</p>
</div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Using the built-in stats::heatmap </span><span class="w">
</span><span class="n">heatmap</span><span class="p">(</span><span class="w">
  </span><span class="n">as.matrix</span><span class="p">(</span><span class="n">cor_example</span><span class="p">),</span><span class="w">
  </span><span class="n">distfun</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="n">as.dist</span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">cor</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-heatmap-cor-cor-example-2.png" alt="plot of chunk heatmap-cor-cor-example" width="432" />
<p class="caption">plot of chunk heatmap-cor-cor-example</p>
</div>

<h1 id="validating-clusters">Validating clusters</h1>

<p>Now that we know how to carry out hierarchical clustering, how do we know how
many clusters are optimal for the dataset?</p>

<p>Hierarchical clustering carried out on any dataset will produce clusters,
even when there are no ‘real’ clusters in the data! We need to be able to
determine whether identified clusters represent true groups in the data, or
whether clusters have been identified just due to chance. In the last episode,
we have introduced silhouette scores as a measure of cluster compactness and
bootstrapping to assess cluster robustness. Such tests can be used to compare
different clustering algorithms, for example, those fitted using different linkage
methods.</p>

<p>Here, we introduce the Dunn index, which is a measure of cluster compactness. The
Dunn index is the ratio of the smallest distance between any two clusters
and to the largest intra-cluster distance found within any cluster. This can be 
seen as a family of indices which differ depending on the method used to compute
distances. The Dunn index is a metric that penalises clusters that have
larger intra-cluster variance and smaller inter-cluster variance. The higher the
Dunn index, the better defined the clusters.</p>

<p>Let’s calculate the Dunn index for clustering carried out on the
<code class="language-plaintext highlighter-rouge">methyl_mat</code> dataset using the <strong><code class="language-plaintext highlighter-rouge">clValid</code></strong> package.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## calculate dunn index</span><span class="w">
</span><span class="c1">## (ratio of the smallest distance between obs not in the same cluster</span><span class="w">
</span><span class="c1">## to the largest intra-cluster distance)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"clValid"</span><span class="p">)</span><span class="w">
</span><span class="c1">## calculate euclidean distance between points </span><span class="w">
</span><span class="n">distmat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dist</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">)</span><span class="w">  
</span><span class="n">clust</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">hclust</span><span class="p">(</span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"complete"</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">clust</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-dunn-1.png" alt="plot of chunk plot-clust-dunn" width="432" />
<p class="caption">plot of chunk plot-clust-dunn</p>
</div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cut</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">50</span><span class="p">)</span><span class="w">

</span><span class="c1">## retrieve Dunn's index for given matrix and clusters</span><span class="w">
</span><span class="n">dunn</span><span class="p">(</span><span class="n">distance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">cut</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 0.8823501
</code></pre></div></div>

<p>The value of the Dunn index has no meaning in itself, but is used to compare
between sets of clusters with larger values being preferred.</p>

<blockquote class="challenge">
  <h2 id="challenge-4">Challenge 4</h2>

  <p>Examine how changing the <code class="language-plaintext highlighter-rouge">h</code> or <code class="language-plaintext highlighter-rouge">k</code> arguments in <code class="language-plaintext highlighter-rouge">cutree()</code>
affects the value of the Dunn index.</p>

  <blockquote class="solution">
    <h2 id="solution-3">Solution:</h2>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"clValid"</span><span class="p">)</span><span class="w">

</span><span class="n">distmat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dist</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">)</span><span class="w">
</span><span class="n">clust</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">hclust</span><span class="p">(</span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"complete"</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">clust</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-dunn-ex-1.png" alt="plot of chunk dunn-ex" width="432" />
<p class="caption">plot of chunk dunn-ex</p>
</div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Varying h</span><span class="w">
</span><span class="c1">## Obtaining the clusters</span><span class="w">
</span><span class="n">cut_h_20</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">)</span><span class="w">
</span><span class="n">cut_h_30</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">30</span><span class="p">)</span><span class="w">

</span><span class="c1">## How many clusters?</span><span class="w">
</span><span class="nf">length</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="n">cut_h_20</span><span class="p">))</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 36
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">length</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="n">cut_h_30</span><span class="p">))</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 14
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dunn</span><span class="p">(</span><span class="n">distance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">cut_h_20</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 1.61789
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dunn</span><span class="p">(</span><span class="n">distance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">cut_h_30</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 0.8181846
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Varying k</span><span class="w">
</span><span class="c1">## Obtaining the clusters</span><span class="w">
</span><span class="n">cut_k_10</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w">
</span><span class="n">cut_k_5</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w">

</span><span class="c1">## How many clusters?</span><span class="w">
</span><span class="nf">length</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="n">cut_k_5</span><span class="p">))</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 5
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">length</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="n">cut_k_10</span><span class="p">))</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 10
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dunn</span><span class="p">(</span><span class="n">distance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">cut_k_5</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 0.8441528
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dunn</span><span class="p">(</span><span class="n">distance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">cut_k_10</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 0.7967132
</code></pre></div>    </div>
  </blockquote>
</blockquote>

<p>The figures below show in a more systematic way how changing the values of <code class="language-plaintext highlighter-rouge">k</code> and
<code class="language-plaintext highlighter-rouge">h</code> using <code class="language-plaintext highlighter-rouge">cutree()</code> affect the Dunn index.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">h_seq</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">70</span><span class="o">:</span><span class="m">10</span><span class="w">
</span><span class="n">h_dunn</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">h_seq</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="n">dunn</span><span class="p">(</span><span class="n">distance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">)))</span><span class="w">
</span><span class="n">k_seq</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w">
</span><span class="n">k_dunn</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">k_seq</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="n">dunn</span><span class="p">(</span><span class="n">distance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">)))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">h_seq</span><span class="p">,</span><span class="w"> </span><span class="n">h_dunn</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Height (h)"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Dunn index"</span><span class="p">)</span><span class="w">
</span><span class="n">grid</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-hclust-fig3-1.png" alt="Figure 3: Dunn index" width="432" />
<p class="caption">Figure 3: Dunn index</p>
</div>
<p>You can see that at low values of <code class="language-plaintext highlighter-rouge">h</code>, the Dunn index can be high. But this
is not very useful - cutting the given tree at a low <code class="language-plaintext highlighter-rouge">h</code> value like 15 leads to allmost all observations
ending up each in its own cluster. More relevant is the second maximum in the plot, around <code class="language-plaintext highlighter-rouge">h=55</code>.
Looking at the dendrogram, this corresponds to <code class="language-plaintext highlighter-rouge">k=4</code>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">k_seq</span><span class="p">,</span><span class="w"> </span><span class="n">k_dunn</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Number of clusters (k)"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Dunn index"</span><span class="p">)</span><span class="w">
</span><span class="n">grid</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-hclust-fig4-1.png" alt="Figure 4: Dunn index continued" width="432" />
<p class="caption">Figure 4: Dunn index continued</p>
</div>
<p>For the given range of <code class="language-plaintext highlighter-rouge">k</code> values explored, we obtain the highest Dunn index with <code class="language-plaintext highlighter-rouge">k=4</code>.
This is in agreement with the previous plot.</p>

<p>There have been criticisms of the use of the Dunn index in validating
clustering results, due to its high sensitivity to noise in the dataset.
An alternative is to use silhouette scores (see the k-means clustering episode).</p>

<p>As we said before (see previous episode), clustering is a non-trivial task.
It is important to think about the nature of your data and your expactations
rather than blindly using a some algorithm for clustering or cluster validation.</p>

<h1 id="further-reading">Further reading</h1>

<ul>
  <li>Dunn, J. C. (1974) Well-separated clusters and optimal fuzzy partitions. Journal of Cybernetics 4(1):95–104.</li>
  <li>Halkidi, M., Batistakis, Y. &amp; Vazirgiannis, M. (2001) On clustering validation techniques. Journal of Intelligent Information Systems 17(2/3):107-145.</li>
  <li>James, G., Witten, D., Hastie, T. &amp; Tibshirani, R. (2013) An Introduction to Statistical Learning with Applications in R. 
Section 10.3.2 (Hierarchical Clustering).</li>
  <li><a href="https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec">Understanding the concept of Hierarchical clustering Technique. towards data science blog</a>.</li>
</ul>






<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>Hierarchical clustering uses an algorithm to group similar data points into clusters. A dendrogram is used to plot relationships between clusters (using the <code class="language-plaintext highlighter-rouge">hclust()</code> function in R).</p>
</li>
    
    <li><p>Hierarchical clustering differs from k-means clustering as it does not require the user to specify expected number of clusters</p>
</li>
    
    <li><p>The distance (dissimilarity) matrix can be calculated in various ways, and different clustering algorithms (linkage methods) can affect the resulting dendrogram.</p>
</li>
    
    <li><p>The Dunn index can be used to validate clusters using the original dataset.</p>
</li>
    
  </ul>
</blockquote>

</article>


















  
  











<div class="row">
  <div class="col-xs-1">
    <h3 class="text-left">
      
      <a href="../06-k-means/index.html"><span class="glyphicon glyphicon-menu-left" aria-hidden="true"></span><span class="sr-only">previous episode</span></a>
      
    </h3>
  </div>
  <div class="col-xs-10">
    
  </div>
  <div class="col-xs-1">
    <h3 class="text-right">
      
      <a href="../"><span class="glyphicon glyphicon-menu-up" aria-hidden="true"></span><span class="sr-only">lesson home</span></a>
      
    </h3>
  </div>
</div>



      
      






<footer>
  <hr/>
  <div class="row">
    <div class="col-md-6 license" id="license-info" align="left">
	
        Licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a> 2024 by <a href="../CITATION">the authors</a>.
	
    </div>
    <div class="col-md-6 help-links" align="right">
	
	
	<a href="/edit//_episodes_rmd/07-hierarchical.Rmd" data-checker-ignore>Edit on GitHub</a>
	
	
	/
	<a href="/blob//CONTRIBUTING.md" data-checker-ignore>Contributing</a>
	/
	<a href="/">Source</a>
	/
	<a href="/blob//CITATION" data-checker-ignore>Cite</a>
	/
	<a href="mailto:alan.ocallaghan@outlook.com">Contact</a>
    </div>
  </div>
  <p class="text-muted text-right">
    <small><i>Using <a href="https://github.com/carpentries/carpentries-theme/">The Carpentries theme</a> &mdash; Site last built on: 2024-02-27 09:14:21 +0000.</i></small>
  </p>
</footer>

      
    </div>
    
<script src="../assets/js/jquery.min.js"></script>
<script src="../assets/js/bootstrap.min.js"></script>
<script src="../assets/js/lesson.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-37305346-2', 'auto');
  ga('send', 'pageview');
</script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        "HTML-CSS": { fonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  </body>
</html>
