






<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta http-equiv="last-modified" content="2024-02-27 09:14:21 +0000">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- meta "search-domain" used for google site search function google_search() -->
    <meta name="search-domain" value="">
    <link rel="stylesheet" type="text/css" href="../assets/css/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/lesson.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/syntax.css" />
     <link rel="stylesheet" type="text/css" href="../assets/css/fonts.css" />
    
    <link rel="license" href="#license-info" />

    



    <!-- Favicons for everyone -->
    <link rel="apple-touch-icon-precomposed" sizes="57x57" href="../assets/favicons/incubator/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="../assets/favicons/incubator/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="../assets/favicons/incubator/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="../assets/favicons/incubator/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon-precomposed" sizes="60x60" href="../assets/favicons/incubator/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon-precomposed" sizes="120x120" href="../assets/favicons/incubator/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon-precomposed" sizes="76x76" href="../assets/favicons/incubator/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../assets/favicons/incubator/apple-touch-icon-152x152.png" />
    <link rel="icon" type="image/png" href="../assets/favicons/incubator/favicon-196x196.png" sizes="196x196" />
    <link rel="icon" type="image/png" href="../assets/favicons/incubator/favicon-96x96.png" sizes="96x96" />
    <link rel="icon" type="image/png" href="../assets/favicons/incubator/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="../assets/favicons/incubator/favicon-16x16.png" sizes="16x16" />
    <link rel="icon" type="image/png" href="../assets/favicons/incubator/favicon-128.png" sizes="128x128" />
    <meta name="application-name" content="The Carpentries Incubator - High dimensional statistics with R"/>
    <meta name="msapplication-TileColor" content="#FFFFFF" />
    <meta name="msapplication-TileImage" content="../assets/favicons/incubator/mstile-144x144.png" />
    <meta name="msapplication-square70x70logo" content="../assets/favicons/incubator/mstile-70x70.png" />
    <meta name="msapplication-square150x150logo" content="../assets/favicons/incubator/mstile-150x150.png" />
    <meta name="msapplication-wide310x150logo" content="../assets/favicons/incubator/mstile-310x150.png" />
    <meta name="msapplication-square310x310logo" content="../assets/favicons/incubator/mstile-310x310.png" />


    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
	<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
	<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
	<![endif]-->
  <title>
  High dimensional statistics with R
  </title>

  </head>
  <body>
    


<div class="panel panel-default life-cycle">
  <div id="life-cycle" class="panel-body alpha">
    This lesson is in the early stages of development (Alpha version)
  </div>
</div>





    <div class="container">
      
















  
  










<nav class="navbar navbar-default">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

      
      
      <a href="../index.html" class="pull-left">
        <img class="navbar-logo" src="../assets/img/incubator-logo-blue.svg" alt="The Carpentries Incubator logo" />
      </a>
      

      
      <a class="navbar-brand" href="../index.html">Home</a>

    </div>
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">

	
        <li><a href="../CODE_OF_CONDUCT.html">Code of Conduct</a></li>

        
	
        <li><a href="../setup.html">Setup</a></li>

        
        
        <li class="dropdown">
          <a href="../" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Episodes <span class="caret"></span></a>
          <ul class="dropdown-menu">
            
            
            <li><a href="../01-introduction-to-high-dimensional-data/index.html">Introduction to high-dimensional data</a></li>
            
            
            <li><a href="../02-high-dimensional-regression/index.html">Regression with many outcomes</a></li>
            
            
            <li><a href="../03-regression-regularisation/index.html">Regularised regression</a></li>
            
            
            <li><a href="../04-principal-component-analysis/index.html">Principal component analysis</a></li>
            
            
            <li><a href="../05-factor-analysis/index.html">Factor analysis</a></li>
            
            
            <li><a href="../06-k-means/index.html">K-means</a></li>
            
            
            <li><a href="../07-hierarchical/index.html">Hierarchical clustering</a></li>
            
	    <li role="separator" class="divider"></li>
            <li><a href="../aio/index.html">All in one page (Beta)</a></li>
          </ul>
        </li>
        
	

	
	
        <li class="dropdown">
          <a href="../" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Extras <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="../reference.html">Reference</a></li>
            
            
            <li><a href="../about/index.html">About</a></li>
            
            
            <li><a href="../figures/index.html">Figures</a></li>
            
            
            <li><a href="../guide/index.html">Instructor Notes</a></li>
            
            
            <li><a href="../slides/index.html">Lecture slides</a></li>
            
          </ul>
        </li>
	

	
        <li><a href="../LICENSE.html">License</a></li>
	
	<li><a href="/edit//aio.md" data-checker-ignore>Improve this page <span class="glyphicon glyphicon-pencil" aria-hidden="true"></span></a></li>
	
      </ul>
      <form class="navbar-form navbar-right" role="search" id="search" onsubmit="google_search(); return false;">
        <div class="form-group">
          <input type="text" id="google-search" placeholder="Search..." aria-label="Google site search">
        </div>
      </form>
    </div>
  </div>
</nav>

      


<div class="alert alert-info text-center" role="alert">
  This lesson is part of
  <a href="https://github.com/carpentries-incubator/proposals/#the-carpentries-incubator" data-checker-ignore>
    The Carpentries Incubator</a>, a place to share and use each other's
  Carpentries-style lessons. <strong>This lesson has not been reviewed by and is
  not endorsed by The Carpentries</strong>.
</div>




      





<h1 class="maintitle"><a href="../index.html">High dimensional statistics with R</a></h1>



<article>

<h1 id="introduction-to-high-dimensional-data" class="maintitle">Introduction to high-dimensional data</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 20 min
      <br />
      <strong>Exercises:</strong> 20 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>What are high-dimensional data and what do these data look like in the biosciences?</p>
</li>
	
	<li><p>What are the challenges when analysing high-dimensional data?</p>
</li>
	
	<li><p>What statistical methods are suitable for analysing these data?</p>
</li>
	
	<li><p>How can Bioconductor be used to access high-dimensional data in the biosciences?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Explore examples of high-dimensional data in the biosciences.</p>
</li>
	
	<li><p>Appreciate challenges involved in analysing high-dimensional data.</p>
</li>
	
	<li><p>Explore different statistical methods used for analysing high-dimensional data.</p>
</li>
	
	<li><p>Work with example data created from biological studies.</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h1 id="what-are-high-dimensional-data">What are high-dimensional data?</h1>

<p><em>High-dimensional data</em> are defined as data in which the number of features (variables observed),
$p$, are close to or larger than the number of observations (or data points), $n$.
The opposite is <em>low-dimensional data</em> in which the number of observations,
$n$, far outnumbers the number of features, $p$. A related concept is <em>wide data</em>, which
refers to data with numerous features irrespective of the number of observations (similarly,
<em>tall data</em> is often used to denote data with a large number of observations).
Analyses of high-dimensional data require consideration of potential problems that
come from having more features than observations.</p>

<p>High-dimensional data have become more common in many scientific fields as new
automated data collection techniques have been developed. More and more datasets
have a large number of features and some have as many features as there are rows
in the dataset. Datasets in which $p \geq n$ are becoming more common. Such datasets
pose a challenge for data analysis as standard methods of analysis, such as linear
regression, are no longer appropriate.</p>

<p>High-dimensional datasets are common in the biological sciences. Data sets in subjects like
genomics and medical sciences are often tall (with large $n$) and wide
(with large $p$), and can be difficult to analyse or visualise using
standard statistical tools. An example of high-dimensional data in biological
sciences may include data collected from hospital patients recording symptoms,
blood test results, behaviours, and general health, resulting in datasets with
large numbers of features. Researchers often want to relate these features to
specific patient outcomes (e.g. survival, length of time spent in hospital).
An example of what high-dimensional data might look like in a biomedical study
is shown in the figure below.</p>

<div class="figure" style="text-align: center">
<img src="../fig/intro-table.png" alt="plot of chunk table-intro" />
<p class="caption">plot of chunk table-intro</p>
</div>

<blockquote class="challenge">
  <h2 id="challenge-1">Challenge 1</h2>

  <p>Descriptions of four research questions and their datasets are given below.
Which of these scenarios use high-dimensional data?</p>

  <ol>
    <li>Predicting patient blood pressure using: cholesterol level in blood, age,
and BMI measurements, collected from 100 patients.</li>
    <li>Predicting patient blood pressure using: cholesterol level in blood, age,
and BMI, as well as information on 200,000 single nucleotide polymorphisms
from 100 patients.</li>
    <li>Predicting the length of time patients spend in hospital with pneumonia infection
using: measurements on age, BMI, length of time with symptoms,
number of symptoms, and percentage of neutrophils in blood, using data
from 200 patients.</li>
    <li>Predicting probability of a patient’s cancer progressing using gene
expression data from 20,000 genes, as well as data associated with general patient health
(age, weight, BMI, blood pressure) and cancer growth (tumour size,
localised spread, blood test results).</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution">Solution</h2>

    <ol>
      <li>No. The number of observations (100 patients) is far greater than the number of features (3).</li>
      <li>Yes, this is an example of high-dimensional data. There are only 100 observations but 200,000+3 features.</li>
      <li>No. There are many more observations (200 patients) than features (5).</li>
      <li>Yes. There is only one observation of more than 20,000 features.</li>
    </ol>
  </blockquote>
</blockquote>

<p>Now that we have an idea of what high-dimensional data look like we can think
about the challenges we face in analysing them.</p>

<h1 id="challenges-in-dealing-with-high-dimensional-data">Challenges in dealing with high-dimensional data</h1>

<p>Most classical statistical methods are set up for use on low-dimensional data
(i.e. data where the number of observations $n$ is much larger than the number
of features $p$). This is because low-dimensional data were much more common in
the past when data collection was more difficult and time consuming. In recent
years advances in information technology have allowed large amounts of data to
be collected and stored with relative ease. This has allowed large numbers of
features to be collected, meaning that datasets in which $p$ matches or exceeds
$n$ are common (collecting observations is often more difficult or expensive
than collecting many features from a single observation).</p>

<p>Datasets with large numbers of features are difficult to visualise. When
exploring low-dimensional datasets, it is possible to plot the response variable
against each of the limited number of explanatory variables to get an idea which
of these are important predictors of the response. With high-dimensional data
the large number of explanatory variables makes doing this difficult. In some
high-dimensional datasets it can also be difficult to identify a single response
variable, making standard data exploration and analysis techniques less useful.</p>

<p>Let’s have a look at a simple dataset with lots of features to understand some
of the challenges we are facing when working with high-dimensional data.</p>

<blockquote class="challenge">
  <h2 id="challenge-2">Challenge 2</h2>

  <p>Load the <code class="language-plaintext highlighter-rouge">Prostate</code> dataset as follows:</p>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"here"</span><span class="p">)</span><span class="w">
</span><span class="n">Prostate</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readRDS</span><span class="p">(</span><span class="n">here</span><span class="p">(</span><span class="s2">"data/prostate.rds"</span><span class="p">))</span><span class="w">
</span></code></pre></div>  </div>

  <p>Although technically not a high-dimensional dataset, the <code class="language-plaintext highlighter-rouge">Prostate</code> data
will allow us explore the problems encountered when working with many features.</p>

  <p>Examine the dataset (in which each row represents a single patient) to:</p>

  <ol>
    <li>Determine how many observations ($n$) and features ($p$) are available (hint: see the <code class="language-plaintext highlighter-rouge">dim()</code> function).</li>
    <li>Examine what variables were measured (hint: see the <code class="language-plaintext highlighter-rouge">names()</code> and <code class="language-plaintext highlighter-rouge">head()</code> functions).</li>
    <li>Plot the relationship between the variables (hint: see the <code class="language-plaintext highlighter-rouge">pairs()</code> function).</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution-1">Solution</h2>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">dim</span><span class="p">(</span><span class="n">Prostate</span><span class="p">)</span><span class="w">   </span><span class="c1">#print the number of rows and columns</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">names</span><span class="p">(</span><span class="n">Prostate</span><span class="p">)</span><span class="w"> </span><span class="c1"># examine the variable names</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">Prostate</span><span class="p">)</span><span class="w">   </span><span class="c1">#print the first 6 rows</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">names</span><span class="p">(</span><span class="n">Prostate</span><span class="p">)</span><span class="w">  </span><span class="c1">#examine column names</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code> [1] "X"       "lcavol"  "lweight" "age"     "lbph"    "svi"     "lcp"    
 [8] "gleason" "pgg45"   "lpsa"   
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pairs</span><span class="p">(</span><span class="n">Prostate</span><span class="p">)</span><span class="w">  </span><span class="c1">#plot each pair of variables against each other</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-01-pairs-prostate-1.png" alt="plot of chunk pairs-prostate" width="432" />
<p class="caption">plot of chunk pairs-prostate</p>
</div>
    <p>The <code class="language-plaintext highlighter-rouge">pairs()</code> function plots relationships between each of the variables in
the <code class="language-plaintext highlighter-rouge">Prostate</code> dataset. This is possible for datasets with smaller numbers
of variables, but for datasets in which $p$ is larger it becomes difficult
(and time consuming) to visualise relationships between all variables in the
dataset. Even where visualisation is possible, fitting models to datasets
with many variables is difficult due to the potential for
overfitting and difficulties in identifying a response variable.</p>

  </blockquote>
</blockquote>

<blockquote class="callout">
  <h2 id="locating-data-with-r---the-here-package">Locating data with R - the <strong><code class="language-plaintext highlighter-rouge">here</code></strong> package</h2>

  <p>It is often desirable to access external datasets from inside R and to write 
code that does this reliably on different computers. While R has an inbulit 
function <code class="language-plaintext highlighter-rouge">setwd()</code> that can be used to denote where external datasets are 
stored, this usually requires the user to adjust the code to their specific 
system and folder structure. The <strong><code class="language-plaintext highlighter-rouge">here</code></strong> package is meant to be used in R 
projects. It allows users to specify the data location relative to the R 
project directory. This makes R code more portable and can contribute to 
improve the reproducibility of an analysis.</p>
</blockquote>

<p>Imagine we are carrying out least squares regression on a dataset with 25
observations. Fitting a best fit line through these data produces a plot shown
in the left-hand panel of the figure below.</p>

<p>However, imagine a situation in which the number of observations and features in a
dataset are almost equal. In that situation the effective number of observations
per features is low. The result of fitting a best fit line through
few observations can be seen in the right-hand panel below.</p>

<div class="figure" style="text-align: center">
<img src="../fig/intro-scatterplot.png" alt="plot of chunk intro-figure" />
<p class="caption">plot of chunk intro-figure</p>
</div>

<p>In the first situation, the least squares regression line does not fit the data
perfectly and there is some error around the regression line. But, when there are
only two observations the regression line will fit through the points exactly,
resulting in overfitting of the data. This suggests that carrying out least
squares regression on a dataset with few data points per feature would result
in difficulties in applying the resulting model to further datsets. This is a
common problem when using regression on high-dimensional datasets.</p>

<p>Another problem in carrying out regression on high-dimensional data is dealing
with correlations between explanatory variables. The large numbers of features
in these datasets makes high correlations between variables more likely.</p>

<blockquote class="challenge">
  <h2 id="challenge-3">Challenge 3</h2>

  <p>Use the <code class="language-plaintext highlighter-rouge">cor()</code> function to examine correlations between all variables in the 
<code class="language-plaintext highlighter-rouge">Prostate</code> dataset. Are some pairs of variables highly correlated (i.e. 
correlation coefficients &gt; 0.6)?</p>

  <p>Use the <code class="language-plaintext highlighter-rouge">lm()</code> function to fit univariate regression models to predict patient 
age using two variables that are highly correlated as predictors. Which of 
these variables are statistically significant predictors of age? Hint: the
<code class="language-plaintext highlighter-rouge">summary()</code> function can help here.</p>

  <p>Fit a multiple linear regression model predicting patient age using both
variables. What happened?</p>

  <blockquote class="solution">
    <h2 id="solution-2">Solution</h2>

    <p>Create a correlation matrix of all variables in the Prostate dataset</p>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cor</span><span class="p">(</span><span class="n">Prostate</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                X    lcavol      lweight       age         lbph         svi
X       1.0000000 0.7111363  0.350443662 0.1965557  0.167928486  0.56678035
lcavol  0.7111363 1.0000000  0.194128286 0.2249999  0.027349703  0.53884500
lweight 0.3504437 0.1941283  1.000000000 0.3075286  0.434934636  0.10877851
age     0.1965557 0.2249999  0.307528614 1.0000000  0.350185896  0.11765804
lbph    0.1679285 0.0273497  0.434934636 0.3501859  1.000000000 -0.08584324
svi     0.5667803 0.5388450  0.108778505 0.1176580 -0.085843238  1.00000000
lcp     0.5336960 0.6753105  0.100237795 0.1276678 -0.006999431  0.67311118
gleason 0.3936079 0.4324171 -0.001275658 0.2688916  0.077820447  0.32041222
pgg45   0.4497267 0.4336522  0.050846821 0.2761124  0.078460018  0.45764762
lpsa    0.9581149 0.7344603  0.354120390 0.1695928  0.179809410  0.56621822
                 lcp      gleason      pgg45      lpsa
X        0.533696039  0.393607939 0.44972672 0.9581149
lcavol   0.675310484  0.432417056 0.43365225 0.7344603
lweight  0.100237795 -0.001275658 0.05084682 0.3541204
age      0.127667752  0.268891599 0.27611245 0.1695928
lbph    -0.006999431  0.077820447 0.07846002 0.1798094
svi      0.673111185  0.320412221 0.45764762 0.5662182
lcp      1.000000000  0.514830063 0.63152825 0.5488132
gleason  0.514830063  1.000000000 0.75190451 0.3689868
pgg45    0.631528245  0.751904512 1.00000000 0.4223159
lpsa     0.548813169  0.368986803 0.42231586 1.0000000
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">round</span><span class="p">(</span><span class="n">cor</span><span class="p">(</span><span class="n">Prostate</span><span class="p">),</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="c1"># rounding helps to visualise the correlations</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>           X lcavol lweight  age  lbph   svi   lcp gleason pgg45 lpsa
X       1.00   0.71    0.35 0.20  0.17  0.57  0.53    0.39  0.45 0.96
lcavol  0.71   1.00    0.19 0.22  0.03  0.54  0.68    0.43  0.43 0.73
lweight 0.35   0.19    1.00 0.31  0.43  0.11  0.10    0.00  0.05 0.35
age     0.20   0.22    0.31 1.00  0.35  0.12  0.13    0.27  0.28 0.17
lbph    0.17   0.03    0.43 0.35  1.00 -0.09 -0.01    0.08  0.08 0.18
svi     0.57   0.54    0.11 0.12 -0.09  1.00  0.67    0.32  0.46 0.57
lcp     0.53   0.68    0.10 0.13 -0.01  0.67  1.00    0.51  0.63 0.55
gleason 0.39   0.43    0.00 0.27  0.08  0.32  0.51    1.00  0.75 0.37
pgg45   0.45   0.43    0.05 0.28  0.08  0.46  0.63    0.75  1.00 0.42
lpsa    0.96   0.73    0.35 0.17  0.18  0.57  0.55    0.37  0.42 1.00
</code></pre></div>    </div>

    <p>As seen above, some variables are highly correlated. In particular, the 
correlation between <code class="language-plaintext highlighter-rouge">gleason</code> and <code class="language-plaintext highlighter-rouge">pgg45</code> is equal to 0.75.</p>

    <p>Fitting univariate regression models to predict age using gleason and pgg45
as predictors.</p>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">age</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">gleason</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Prostate</span><span class="p">)</span><span class="w">
</span><span class="n">model2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">age</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">pgg45</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Prostate</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <p>Check which covariates have a significant efffect</p>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">summary</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Call:
lm(formula = age ~ gleason, data = Prostate)

Residuals:
    Min      1Q  Median      3Q     Max 
-20.780  -3.552   1.448   4.220  13.448 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   45.146      6.918   6.525 3.29e-09 ***
gleason        2.772      1.019   2.721  0.00774 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 7.209 on 95 degrees of freedom
Multiple R-squared:  0.0723,	Adjusted R-squared:  0.06254 
F-statistic: 7.404 on 1 and 95 DF,  p-value: 0.007741
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">summary</span><span class="p">(</span><span class="n">model2</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Call:
lm(formula = age ~ pgg45, data = Prostate)

Residuals:
     Min       1Q   Median       3Q      Max 
-21.0889  -3.4533   0.9111   4.4534  15.1822 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 62.08890    0.96758   64.17  &lt; 2e-16 ***
pgg45        0.07289    0.02603    2.80  0.00619 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 7.193 on 95 degrees of freedom
Multiple R-squared:  0.07624,	Adjusted R-squared:  0.06651 
F-statistic:  7.84 on 1 and 95 DF,  p-value: 0.006189
</code></pre></div>    </div>

    <p>Based on these results we conclude that both <code class="language-plaintext highlighter-rouge">gleason</code> and <code class="language-plaintext highlighter-rouge">pgg45</code> have a 
statistically significan univariate effect (also referred to as a marginal
effect) as predictors of age (5% significance level).</p>

    <p>Fitting a multivariate regression model using both both <code class="language-plaintext highlighter-rouge">gleason</code> and <code class="language-plaintext highlighter-rouge">pgg45</code> 
as predictors</p>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">age</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">gleason</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">pgg45</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Prostate</span><span class="p">)</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">model3</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Call:
lm(formula = age ~ gleason + pgg45, data = Prostate)

Residuals:
    Min      1Q  Median      3Q     Max 
-20.927  -3.677   1.323   4.323  14.420 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 52.95548    9.74316   5.435  4.3e-07 ***
gleason      1.45363    1.54299   0.942    0.349    
pgg45        0.04490    0.03951   1.137    0.259    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 7.198 on 94 degrees of freedom
Multiple R-squared:  0.08488,	Adjusted R-squared:  0.06541 
F-statistic: 4.359 on 2 and 94 DF,  p-value: 0.01547
</code></pre></div>    </div>

    <p>Although <code class="language-plaintext highlighter-rouge">gleason</code> and <code class="language-plaintext highlighter-rouge">pgg45</code> have statistically significant univariate effects,
this is no longer the case when both variables are simultaneously included
as covariates in a multivariate regression model.</p>
  </blockquote>
</blockquote>

<p>Including highly correlated variables such as <code class="language-plaintext highlighter-rouge">gleason</code> and <code class="language-plaintext highlighter-rouge">pgg45</code> 
simultaneously the same regression model can lead to problems 
in fitting a regression model and interpreting its output. To allow variables to 
be included in the same model despite high levels of correlation, we can use
dimensionality reduction methods to collapse multiple variables into a single
new variable (we will explore this dataset further in the dimensionality
reduction lesson). We can also use modifications to linear regression like
regularisation, which we will discuss in the lesson on high-dimensional
regression.</p>

<h1 id="what-statistical-methods-are-used-to-analyse-high-dimensional-data">What statistical methods are used to analyse high-dimensional data?</h1>

<p>As we found out in the above challenges, carrying out linear regression on
datasets with large numbers of features can be difficult due to: high levels of correlation
between variables; difficulty in identifying a clear response variable; and risk
of overfitting. These problems are common to the analysis of many high-dimensional datasets,
for example, those using genomics data with multiple genes, or species
composition data in an environment where the relative abundance of different species
within a community is of interest. For such datasets, other statistical methods
may be used to examine whether groups of observations show similar characteristics
and whether these groups may relate to other features in the data (e.g.
phenotype in genetics data).</p>

<p>In this course, we will cover four methods that help in dealing with high-dimensional data:
(1) regression with numerous outcome variables, (2) regularised regression, 
(3) dimensionality reduction, and (4) clustering. Here are some examples of when each of 
these approaches may be used:</p>

<p>(1) Regression with numerous outcomes refers to situations in which there are 
many variables of a similar kind (expression values for many genes, methylation 
levels for many sites in the genome) and when one is interested in assessing 
whether these variables are associated with a specific covariate of interest, 
such as experimental condition or age. In this case, multiple univariate 
regression models (one per each outcome, using the covariate of interest as 
predictor) could be fitted independently. In the context of high-dimensional 
molecular data, a typical example are <em>differential gene expression</em> analyses. 
We will explore this type of analysis in the <em>Regression with many outcomes</em> episode.</p>

<p>(2) Regularisation (also known as <em>regularised regression</em> or <em>penalised regression</em>) 
is typically used to fit regression models when there is a single outcome 
variable or interest but the number of potential predictors is large, e.g. 
there are more predictors than observations. Regularisation can help to prevent 
overfitting and may be used to identify a small subset of predictors that are
associated with the outcome of interest. For example, regularised regression has
been often used when building <em>epigenetic clocks</em>, where methylation values 
across several thousands of genomic sites are used to predict chronological age. 
We will explore this in more detail in the <em>Regularised regression</em> episode.</p>

<p>(3) Dimensionality reduction is commonly used on high-dimensional datasets for 
data exploration or as a preprocessing step prior to other downstream analyses. 
For instance, a low-dimensional visualisation of a gene expression dataset may
be used to inform <em>quality control</em> steps (e.g. are there any anomalous samples?). 
This course contains two episodes that explore dimensionality reduction
techniques: <em>Principal component analysis</em> and <em>Factor analysis</em>.</p>

<p>(4) Clustering methods can be used to identify potential grouping patterns 
within a dataset. A popular example is the <em>identification of distinct cell types</em>
through clustering cells with similar gene expression patterns. The <em>K-means</em>
episode will explore a specific method to perform clustering analysis.</p>

<blockquote class="callout">
  <h2 id="using-bioconductor-to-access-high-dimensional-data-in-the-biosciences">Using Bioconductor to access high-dimensional data in the biosciences</h2>

  <p>In this workshop, we will look at statistical methods that can be used to
visualise and analyse high-dimensional biological data using packages available
from Bioconductor, open source software for analysing high throughput genomic
data. Bioconductor contains useful packages and example datasets as shown on the
website <a href="https://www.bioconductor.org/">https://www.bioconductor.org/</a>.</p>

  <p>Bioconductor packages can be installed and used in <code class="language-plaintext highlighter-rouge">R</code> using the <strong><code class="language-plaintext highlighter-rouge">BiocManager</code></strong>
package. Let’s load the <strong><code class="language-plaintext highlighter-rouge">minfi</code></strong> package from Bioconductor (a package for
analysing Illumina Infinium DNA methylation arrays).</p>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"minfi"</span><span class="p">)</span><span class="w">
</span></code></pre></div>  </div>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">browseVignettes</span><span class="p">(</span><span class="s2">"minfi"</span><span class="p">)</span><span class="w">
</span></code></pre></div>  </div>

  <p>We can explore these packages by browsing the vignettes provided in
Bioconductor. Bioconductor has various packages that can be used to load and
examine datasets in <code class="language-plaintext highlighter-rouge">R</code> that have been made available in Bioconductor, usually
along with an associated paper or package.</p>

  <p>Next, we load the <code class="language-plaintext highlighter-rouge">methylation</code> dataset which represents data collected using
Illumina Infinium methylation arrays which are used to examine methylation
across the human genome. These data include information collected from the
assay as well as associated metadata from individuals from whom samples were
taken.</p>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"here"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"ComplexHeatmap"</span><span class="p">)</span><span class="w">

</span><span class="n">methylation</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readRDS</span><span class="p">(</span><span class="n">here</span><span class="p">(</span><span class="s2">"data/methylation.rds"</span><span class="p">))</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">colData</span><span class="p">(</span><span class="n">methylation</span><span class="p">))</span><span class="w">
</span></code></pre></div>  </div>

  <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DataFrame with 6 rows and 14 columns
                    Sample_Well Sample_Name    purity         Sex       Age
                    &lt;character&gt; &lt;character&gt; &lt;integer&gt; &lt;character&gt; &lt;integer&gt;
201868500150_R01C01         A07     PCA0612        94           M        39
201868500150_R03C01         C07   NKpan2510        95           M        49
201868500150_R05C01         E07      WB1148        95           M        20
201868500150_R07C01         G07       B0044        97           M        49
201868500150_R08C01         H07   NKpan1869        95           F        33
201868590193_R02C01         B03   NKpan1850        93           F        21
                    weight_kg  height_m       bmi    bmi_clas Ethnicity_wide
                    &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;character&gt;    &lt;character&gt;
201868500150_R01C01   88.4505    1.8542   25.7269  Overweight          Mixed
201868500150_R03C01   81.1930    1.6764   28.8911  Overweight  Indo-European
201868500150_R05C01   80.2858    1.7526   26.1381  Overweight  Indo-European
201868500150_R07C01   82.5538    1.7272   27.6727  Overweight  Indo-European
201868500150_R08C01   87.5433    1.7272   29.3452  Overweight  Indo-European
201868590193_R02C01   87.5433    1.6764   31.1507       Obese          Mixed
                       Ethnic_self      smoker       Array       Slide
                       &lt;character&gt; &lt;character&gt; &lt;character&gt;   &lt;numeric&gt;
201868500150_R01C01       Hispanic          No      R01C01 2.01869e+11
201868500150_R03C01      Caucasian          No      R03C01 2.01869e+11
201868500150_R05C01        Persian          No      R05C01 2.01869e+11
201868500150_R07C01      Caucasian          No      R07C01 2.01869e+11
201868500150_R08C01      Caucasian          No      R08C01 2.01869e+11
201868590193_R02C01 Finnish/Creole          No      R02C01 2.01869e+11
</code></pre></div>  </div>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">methyl_mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">assay</span><span class="p">(</span><span class="n">methylation</span><span class="p">))</span><span class="w">
</span><span class="c1">## calculate correlations between cells in matrix</span><span class="w">
</span><span class="n">cor_mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cor</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">)</span><span class="w">
</span></code></pre></div>  </div>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cor_mat</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">10</span><span class="p">]</span><span class="w"> </span><span class="c1"># print the top-left corner of the correlation matrix</span><span class="w">
</span></code></pre></div>  </div>

  <p>The <code class="language-plaintext highlighter-rouge">assay()</code> function creates a matrix-like object where rows represent probes
for genes and columns represent samples. We calculate correlations between
features in the <code class="language-plaintext highlighter-rouge">methylation</code> dataset and examine the first 100 cells of this
matrix. The size of the dataset makes it difficult to examine in full, a
common challenge in analysing high-dimensional genomics data.</p>
</blockquote>

<h1 id="further-reading">Further reading</h1>

<ul>
  <li>Buhlman, P. &amp; van de Geer, S. (2011) Statistics for High-Dimensional Data. Springer, London.</li>
  <li><a href="https://doi.org/10.1146/annurev-statistics-022513-115545">Buhlman, P., Kalisch, M. &amp; Meier, L. (2014) High-dimensional statistics with a view toward applications in biology. Annual Review of Statistics and Its Application</a>.</li>
  <li>Johnstone, I.M. &amp; Titterington, D.M. (2009) Statistical challenges of high-dimensional data. Philosophical Transactions of the Royal Society A 367:4237-4253.</li>
  <li><a href="https://www.bioconductor.org/packages/release/workflows/vignettes/methylationArrayAnalysis/inst/doc/methylationArrayAnalysis.html">Bioconductor ethylation array analysis vignette</a>.</li>
  <li>The <em>Introduction to Machine Learning with Python</em> course covers additional 
methods that could be used to analyse high-dimensional data. See 
<a href="https://carpentries-incubator.github.io/machine-learning-novice-python/">Introduction to machine learning</a>,
<a href="https://carpentries-incubator.github.io/machine-learning-trees-python/">Tree models</a> and
<a href="https://carpentries-incubator.github.io/machine-learning-neural-python/">Neural networks</a>. 
Some related (an important!) content is also available in 
<a href="https://carpentries-incubator.github.io/machine-learning-responsible-python/">Responsible machine learning</a>.</li>
</ul>

<h1 id="other-resources-suggested-by-former-students">Other resources suggested by former students</h1>

<ul>
  <li><a href="https://www.youtube.com/c/joshstarmer">Josh Starmer’s</a> youtube channel.</li>
</ul>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>High-dimensional data are data in which the number of features, $p$, are close to or larger than the number of observations, $n$.</p>
</li>
    
    <li><p>These data are becoming more common in the biological sciences due to increases in data storage capabilities and computing power.</p>
</li>
    
    <li><p>Standard statistical methods, such as linear regression, run into difficulties when analysing high-dimensional data.</p>
</li>
    
    <li><p>In this workshop, we will explore statistical methods used for analysing high-dimensional data using datasets available on Bioconductor.</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="regression-with-many-outcomes" class="maintitle">Regression with many outcomes</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 60 min
      <br />
      <strong>Exercises:</strong> 30 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>How can we apply linear regression in a high-dimensional setting?</p>
</li>
	
	<li><p>How can we benefit from the fact that we have many outcomes?</p>
</li>
	
	<li><p>How can we control for the fact that we do many tests?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Perform and critically analyse high dimensional regression.</p>
</li>
	
	<li><p>Understand methods for shrinkage of noise parameters in high-dimensional regression.</p>
</li>
	
	<li><p>Perform multiple testing adjustment.</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h1 id="dna-methylation-data">DNA methylation data</h1>

<p>For the following few episodes, we will be working with human DNA
methylation data from flow-sorted blood samples. DNA methylation assays
measure, for each of many sites in the genome, the proportion of DNA
that carries a methyl mark (a chemical modification that does not alter the 
DNA sequence). In this case, the methylation data come in
the form of a matrix of normalised methylation levels (M-values), where negative
values correspond to unmethylated DNA and positive values correspond to
methylated DNA. Along with this, we have a number of sample phenotypes
(eg, age in years, BMI).</p>

<p>Let’s read in the data for this episode:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"here"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"minfi"</span><span class="p">)</span><span class="w">
</span><span class="n">methylation</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readRDS</span><span class="p">(</span><span class="n">here</span><span class="p">(</span><span class="s2">"data/methylation.rds"</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p>Note: the code that we used to download these data from its source is available
<a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/blob/main/data/methylation.R">here</a></p>

<p>This <code class="language-plaintext highlighter-rouge">methylation</code> object is a <code class="language-plaintext highlighter-rouge">GenomicRatioSet</code>, a Bioconductor data
object derived from the <code class="language-plaintext highlighter-rouge">SummarizedExperiment</code> class. These
<code class="language-plaintext highlighter-rouge">SummarizedExperiment</code> objects contain <code class="language-plaintext highlighter-rouge">assay</code>s, in this case
normalised methylation levels, and optional sample-level <code class="language-plaintext highlighter-rouge">colData</code> and
feature-level <code class="language-plaintext highlighter-rouge">metadata</code>. These objects are very convenient to contain
all of the information about a dataset in a high-throughput context. If
you would like more detail on these objects it may be useful to consult
the <a href="https://www.bioconductor.org/packages/release/bioc/vignettes/SummarizedExperiment/inst/doc/SummarizedExperiment.html">vignettes on
Bioconductor</a>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">methylation</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class: GenomicRatioSet 
dim: 5000 37 
metadata(0):
assays(2): M CN
rownames(5000): cg00075967 cg00374717 ... cg08482167 cg13174700
rowData names(0):
colnames(37): 201868500150_R01C01 201868500150_R03C01 ...
  201870610111_R06C01 201870610111_R07C01
colData names(14): Sample_Well Sample_Name ... Array Slide
Annotation
  array: IlluminaHumanMethylationEPIC
  annotation: ilm10b4.hg19
Preprocessing
  Method: Raw (no normalization or bg correction)
  minfi version: 1.38.0
  Manifest version: 0.3.0
</code></pre></div></div>

<p>You can see in this output that this object has a <code class="language-plaintext highlighter-rouge">dim()</code> of
$5000 \times 37$, meaning it has
5000 features and 37 columns. To
extract the matrix of methylation M-values, we can use the
<code class="language-plaintext highlighter-rouge">assay()</code> function. One thing to bear in mind with these objects (and
data structures for computational biology in R generally) is that in the
matrix of methylation data, samples or observations are stored as
columns, while features (in this case, sites in the genome) are stored as rows.
This is in contrast to usual tabular data, where features or variables
are stored as columns and observations are stored as rows.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">methyl_mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">assay</span><span class="p">(</span><span class="n">methylation</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>The distribution of these M-values looks like this:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">hist</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">,</span><span class="w"> </span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"FD"</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"M-value"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-02-histx-1.png" alt="Histogram of M-values for all features. The distribution appears to be bimodal, with a large number of unmethylated features as well as many methylated features, and many intermediate features." width="432" />
<p class="caption">Methylation levels are generally bimodally distributed.</p>
</div>

<p>You can see that there are two peaks in this distribution, corresponding
to features which are largely unmethylated and methylated, respectively.</p>

<p>Similarly, we can examine the <code class="language-plaintext highlighter-rouge">colData()</code>, which represents the
sample-level metadata we have relating to these data. In this case, the
metadata, phenotypes, and groupings in the <code class="language-plaintext highlighter-rouge">colData</code> look like this for
the first 6 samples:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">knitr</span><span class="o">::</span><span class="n">kable</span><span class="p">(</span><span class="n">head</span><span class="p">(</span><span class="n">colData</span><span class="p">(</span><span class="n">methylation</span><span class="p">)),</span><span class="w"> </span><span class="n">row.names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Sample_Well</th>
      <th style="text-align: left">Sample_Name</th>
      <th style="text-align: right">purity</th>
      <th style="text-align: left">Sex</th>
      <th style="text-align: right">Age</th>
      <th style="text-align: right">weight_kg</th>
      <th style="text-align: right">height_m</th>
      <th style="text-align: right">bmi</th>
      <th style="text-align: left">bmi_clas</th>
      <th style="text-align: left">Ethnicity_wide</th>
      <th style="text-align: left">Ethnic_self</th>
      <th style="text-align: left">smoker</th>
      <th style="text-align: left">Array</th>
      <th style="text-align: right">Slide</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">A07</td>
      <td style="text-align: left">PCA0612</td>
      <td style="text-align: right">94</td>
      <td style="text-align: left">M</td>
      <td style="text-align: right">39</td>
      <td style="text-align: right">88.45051</td>
      <td style="text-align: right">1.8542</td>
      <td style="text-align: right">25.72688</td>
      <td style="text-align: left">Overweight</td>
      <td style="text-align: left">Mixed</td>
      <td style="text-align: left">Hispanic</td>
      <td style="text-align: left">No</td>
      <td style="text-align: left">R01C01</td>
      <td style="text-align: right">201868500150</td>
    </tr>
    <tr>
      <td style="text-align: left">C07</td>
      <td style="text-align: left">NKpan2510</td>
      <td style="text-align: right">95</td>
      <td style="text-align: left">M</td>
      <td style="text-align: right">49</td>
      <td style="text-align: right">81.19303</td>
      <td style="text-align: right">1.6764</td>
      <td style="text-align: right">28.89106</td>
      <td style="text-align: left">Overweight</td>
      <td style="text-align: left">Indo-European</td>
      <td style="text-align: left">Caucasian</td>
      <td style="text-align: left">No</td>
      <td style="text-align: left">R03C01</td>
      <td style="text-align: right">201868500150</td>
    </tr>
    <tr>
      <td style="text-align: left">E07</td>
      <td style="text-align: left">WB1148</td>
      <td style="text-align: right">95</td>
      <td style="text-align: left">M</td>
      <td style="text-align: right">20</td>
      <td style="text-align: right">80.28585</td>
      <td style="text-align: right">1.7526</td>
      <td style="text-align: right">26.13806</td>
      <td style="text-align: left">Overweight</td>
      <td style="text-align: left">Indo-European</td>
      <td style="text-align: left">Persian</td>
      <td style="text-align: left">No</td>
      <td style="text-align: left">R05C01</td>
      <td style="text-align: right">201868500150</td>
    </tr>
    <tr>
      <td style="text-align: left">G07</td>
      <td style="text-align: left">B0044</td>
      <td style="text-align: right">97</td>
      <td style="text-align: left">M</td>
      <td style="text-align: right">49</td>
      <td style="text-align: right">82.55381</td>
      <td style="text-align: right">1.7272</td>
      <td style="text-align: right">27.67272</td>
      <td style="text-align: left">Overweight</td>
      <td style="text-align: left">Indo-European</td>
      <td style="text-align: left">Caucasian</td>
      <td style="text-align: left">No</td>
      <td style="text-align: left">R07C01</td>
      <td style="text-align: right">201868500150</td>
    </tr>
    <tr>
      <td style="text-align: left">H07</td>
      <td style="text-align: left">NKpan1869</td>
      <td style="text-align: right">95</td>
      <td style="text-align: left">F</td>
      <td style="text-align: right">33</td>
      <td style="text-align: right">87.54333</td>
      <td style="text-align: right">1.7272</td>
      <td style="text-align: right">29.34525</td>
      <td style="text-align: left">Overweight</td>
      <td style="text-align: left">Indo-European</td>
      <td style="text-align: left">Caucasian</td>
      <td style="text-align: left">No</td>
      <td style="text-align: left">R08C01</td>
      <td style="text-align: right">201868500150</td>
    </tr>
    <tr>
      <td style="text-align: left">B03</td>
      <td style="text-align: left">NKpan1850</td>
      <td style="text-align: right">93</td>
      <td style="text-align: left">F</td>
      <td style="text-align: right">21</td>
      <td style="text-align: right">87.54333</td>
      <td style="text-align: right">1.6764</td>
      <td style="text-align: right">31.15070</td>
      <td style="text-align: left">Obese</td>
      <td style="text-align: left">Mixed</td>
      <td style="text-align: left">Finnish/Creole</td>
      <td style="text-align: left">No</td>
      <td style="text-align: left">R02C01</td>
      <td style="text-align: right">201868590193</td>
    </tr>
  </tbody>
</table>

<p>In this episode, we will focus on the association between age and
methylation. The following heatmap summarises age and methylation levels 
available in the Prostate dataset:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">age</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">methylation</span><span class="o">$</span><span class="n">Age</span><span class="w">

</span><span class="n">library</span><span class="p">(</span><span class="s2">"ComplexHeatmap"</span><span class="p">)</span><span class="w">
</span><span class="n">order</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">order</span><span class="p">(</span><span class="n">age</span><span class="p">)</span><span class="w">
</span><span class="n">age_ord</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">age</span><span class="p">[</span><span class="n">order</span><span class="p">]</span><span class="w">
</span><span class="n">methyl_mat_ord</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">methyl_mat</span><span class="p">[,</span><span class="w"> </span><span class="n">order</span><span class="p">]</span><span class="w">

</span><span class="n">Heatmap</span><span class="p">(</span><span class="n">methyl_mat_ord</span><span class="p">,</span><span class="w">
        </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"M-value"</span><span class="p">,</span><span class="w">
        </span><span class="n">cluster_columns</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w">
        </span><span class="n">show_row_names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w">
        </span><span class="n">show_column_names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w">
        </span><span class="n">row_title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Feature"</span><span class="p">,</span><span class="w">
        </span><span class="n">column_title</span><span class="w"> </span><span class="o">=</span><span class="w">  </span><span class="s2">"Sample"</span><span class="p">,</span><span class="w">
        </span><span class="n">top_annotation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">columnAnnotation</span><span class="p">(</span><span class="n">age</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">age_ord</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-02-heatmap-1.png" alt="Heatmap of methylation values across all features. Samples are ordered according to age." width="432" />
<p class="caption">Visualising the data as a heatmap, it's clear that there's too many models to fit 'by hand'.</p>
</div>
<p>Depending on the scientific question of interest, two types of high-dimensional 
problems could be explored in this context:</p>

<ol>
  <li>
    <p>To predict age using methylation levels as predictors. In this case, we would 
have a single outcome (age) which will be predicted using 5000 covariates 
(methylation levels across the genome).</p>
  </li>
  <li>
    <p>To predict methylation levels using age as a predictor. In this case, we 
would have 5000 outcomes (methylation levels across the genome) and a single 
covariate (age).</p>
  </li>
</ol>

<p>The examples in this episode will focus on the second type of problem, whilst 
the next episode will focus on the first.</p>

<blockquote class="challenge">
  <h2 id="challenge-1">Challenge 1</h2>

  <p>Why can we not just fit many linear regression models, one for each of the columns
in the <code class="language-plaintext highlighter-rouge">colData</code> above against each of the features in the matrix of
assays, and choose all of the significant results at a p-value of
0.05?</p>

  <blockquote class="solution">
    <h2 id="solution">Solution</h2>

    <p>There are a number of problems that this kind of approach presents.
For example: 1. Without a research question in mind when creating a
model, it’s not clear how we can interpret each model, and
rationalising the results after the fact can be dangerous; it’s easy
to make up a “story” that isn’t grounded in anything but the fact
that we have significant findings. 2. We may not have a representative
sample for each of these covariates. For example, we may have very
small sample sizes for some ethnicities, leading to spurious
findings. 3. If we perform 5000 tests for each of
14 variables, even if there were no true
associations in the data, we’d be likely to observe some strong
spurious associations that arise just from random noise.</p>

  </blockquote>
</blockquote>

<blockquote class="callout">
  <h2 id="measuring-dna-methylation">Measuring DNA Methylation</h2>

  <p>DNA methylation is an epigenetic modification of DNA. Generally, we
are interested in the proportion of methylation at many sites or
regions in the genome. DNA methylation microarrays, as we are using
here, measure DNA methylation using two-channel microarrays, where one
channel captures signal from methylated DNA and the other captures
unmethylated signal. These data can be summarised as “Beta values”
($\beta$ values), which is the ratio of the methylated signal to the
total signal (methylated plus unmethylated). The $\beta$ value for
site $i$ is calculated as</p>

\[\beta_i = \frac{
        m_i
    } {
        u_{i} + m_{i}
    }\]

  <p>where $m_i$ is the methylated signal for site $i$ and $u_i$ is the
unmethylated signal for site $i$. $\beta$ values take on a value in
the range $[0, 1]$, with 0 representing a completely unmethylated site
and 1 representing a completely methylated site.</p>

  <p>The M-values we use here are the $\log_2$ ratio of methylated versus
unmethylated signal:</p>

\[M_i = \log_2\left(\frac{m_i}{u_i}\right)\]

  <p>M-values are not bounded to an interval as Beta values are, and
therefore can be easier to work with in statistical models.</p>
</blockquote>

<h1 id="regression-with-many-outcomes">Regression with many outcomes</h1>

<p>In high-throughput studies, it is common to have one or more phenotypes
or groupings that we want to relate to features of interest (eg, gene
expression, DNA methylation levels). In general, we want to identify
differences in the features of interest that are related to a phenotype
or grouping of our samples. Identifying features of interest that vary
along with phenotypes or groupings can allow us to understand how
phenotypes arise or manifest. Analysis of this type is sometimes referred 
to using the term <em>differential analysis</em>.</p>

<p>For example, we might want to identify genes that are expressed at a
higher level in mutant mice relative to wild-type mice to understand the
effect of a mutation on cellular phenotypes. Alternatively, we might
have samples from a set of patients, and wish to identify epigenetic
features that are different in young patients relative to old patients,
to help us understand how ageing manifests.</p>

<p>Using linear regression, it is possible to identify differences like
these. However, high-dimensional data like the ones we’re working with
require some special considerations. A primary consideration, as we saw
above, is that there are far too many features to fit each one-by-one as
we might do when analysing low-dimensional datasets (for example using
<code class="language-plaintext highlighter-rouge">lm</code> on each feature and checking the linear model assumptions). A
secondary consideration is that statistical approaches may behave
slightly differently in very high-dimensional data, compared to
low-dimensional data. A third consideration is the speed at which we can
actually compute statistics for data this large – methods optimised for
low-dimensional data may be very slow when applied to high-dimensional
data.</p>

<p>Ideally when performing regression, we want to identify cases like this,
where there is a clear association, and we probably “don’t need”
statistics:</p>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-02-example1-1.png" alt="An example of a strong linear association between a continuous phenotype (age) on the x-axis and a feature of interest (DNA methylation at a given locus) on the y-axis. A strong linear relationship with a positive slope exists between the two." width="432" />
<p class="caption">A scatter plot of age and a feature of interest.</p>
</div>

<p>or equivalently for a discrete covariate:</p>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-02-example2-1.png" alt="An example of a strong linear association between a discrete phenotype (group) on the x-axis and a feature of interest (DNA methylation at a given locus) on the y-axis. The two groups clearly differ with respect to DNA methylation." width="432" />
<p class="caption">A scatter plot of a grouping and a feature of interest.</p>
</div>

<p>However, often due to small differences and small sample sizes, the
problem is more difficult:</p>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-02-example3-1.png" alt="An example of a strong linear association between a discrete phenotype (group) on the x-axis and a feature of interest (DNA methylation at a given locus) on the y-axis. The two groups seem to differ with respect to DNA methylation, but the relationship is weak." width="432" />
<p class="caption">A scatter plot of a grouping and a feature of interest.</p>
</div>

<p>And, of course, we often have an awful lot of features and need to
prioritise a subset of them! We need a rigorous way to prioritise genes
for further analysis.</p>

<h1 id="fitting-a-linear-model">Fitting a linear model</h1>

<p>So, in the data we have read in, we have a matrix of methylation values
$X$ and a vector of ages, $y$. One way to model this is to see if we can
use age to predict the expected (average) methylation value for sample
$j$ at a given locus $i$, which we can write as $X_{ij}$. We can write
that model as:</p>

<p>[\mathbf{E}(X_{ij}) = \beta_0 + \beta_1 \text{Age}_j]</p>

<p>where $\text{Age}_j$ is the age of sample $j$. In this model, $\beta_1$
represents the unit change in mean methylation level for each unit
(year) change in age. For a specific CpG, we can fit this model and get more 
information from the model object. For illustration purposes, here we 
arbitrarily select the first CpG in the <code class="language-plaintext highlighter-rouge">methyl_mat</code> matrix (the one on its first row).</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">age</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">methylation</span><span class="o">$</span><span class="n">Age</span><span class="w">
</span><span class="c1"># methyl_mat[1, ] indicates that the 1st CpG will be used as outcome variable</span><span class="w">
</span><span class="n">lm_age_methyl1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">[</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">age</span><span class="p">)</span><span class="w">
</span><span class="n">lm_age_methyl1</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Call:
lm(formula = methyl_mat[1, ] ~ age)

Coefficients:
(Intercept)          age  
   0.902334     0.008911  
</code></pre></div></div>

<p>We now have estimates for the expected methylation level when age equals
0 (the intercept) and the change in methylation level for a unit change
in age (the slope). We could plot this linear model:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">methyl_mat</span><span class="p">[</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="p">],</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Age"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Methylation level"</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">16</span><span class="p">)</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="n">lm_age_methyl1</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-02-plot-lm-methyl1-1.png" alt="An example of the relationship between age (x-axis) and methylation levels (y-axis) for an arbitrarily selected CpG. In this case, the y-axis shows methylation levels for the first CpG in our data. The black line shows the fitted regression line (based on the intercept and slope estimates shown above). For this feature, we can see that there is no strong relationship between methylation and age." width="432" />
<p class="caption">A scatter plot of age versus the methylation level for an arbitrarily selected CpG side (the one stored as the first column of methyl_mat). Each dot represents an individual. The black line represents the estimated linear model.</p>
</div>

<p>For this feature, we can see that there is no strong relationship
between methylation and age. We could try to repeat this for every
feature in our dataset; however, we have a lot of features! We need an
approach that allows us to assess associations between all of these
features and our outcome while addressing the three considerations we
outlined previously. Before we introduce this approach, let’s go into
detail about how we generally check whether the results of a linear
model are statistically significant.</p>

<h1 id="hypothesis-testing-in-linear-regression">Hypothesis testing in linear regression</h1>

<p>Using the linear model we defined above, we can ask questions based on the 
estimated value for the regression coefficients. For example, do individuals
with different age have different methylation values for a given CpG? We usually 
do this via <em>hypothesis testing</em>. This framework compares the results that we 
observed (here, estimated linear model coefficients) to the results you would 
expect under a <em>null hypothesis</em> associated to our question. In the example above, 
a suitable null hypothesis would test whether the regression coefficient associated
to age ($\beta_1$) is equal to zero or not. If $\beta_1$ is equal to zero,
the linear model indicates that there is no linear relationship between age
and the methylation level for the CpG  (remember: as its name suggests, linear 
regression can only be used to model linear relationships between predictors and 
outcomes!). In other words, the answer to our question would be: no!</p>

<p>The output of a linear model typically returns the results associated 
with the null hypothesis described above (this may not always be the most realistic 
or useful null hypothesis, but it is the one we have by default!). To be 
more specific, the test compares our observed results with a set of 
hypothetical counter-examples of what we would expect to observe if we repeated 
the same experiment and analysis over and over again under the null hypothesis.</p>

<p>For this linear model, we can use <code class="language-plaintext highlighter-rouge">tidy()</code> from the <strong><code class="language-plaintext highlighter-rouge">broom</code></strong> package to 
extract detailed information about the coefficients and the associated 
hypothesis tests in this model:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"broom"</span><span class="p">)</span><span class="w">
</span><span class="n">tidy</span><span class="p">(</span><span class="n">lm_age_methyl1</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code># A tibble: 2 × 5
  term        estimate std.error statistic p.value
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
1 (Intercept)  0.902      0.344      2.62   0.0129
2 age          0.00891    0.0100     0.888  0.381 
</code></pre></div></div>

<p>The standard errors (<code class="language-plaintext highlighter-rouge">std.error</code>) represent the statistical uncertainty in our
regression coefficient estimates (often referred to as <em>effect size</em>). The test 
statistics and p-values represent measures of how (un)likely it would be to observe 
results like this under the “null hypothesis”.</p>

<blockquote class="challenge">
  <h2 id="challenge-2">Challenge 2</h2>

  <p>In the model we fitted, the estimate for the intercept is 0.902 and its associated 
p-value is 0.0129. What does this mean?</p>

  <blockquote class="solution">
    <h2 id="solution-1">Solution</h2>

    <p>The first coefficient in a linear model like this is the intercept, which measures 
the mean of the outcome (in this case, the methylation value for the first CpG)
when age is zero. In this case, the intercept estimate is 0.902. However, this is 
not a particularly noteworthy finding as we do not have any observations with age
zero (nor even any with age &lt; 20!).</p>

    <p>The reported p-value is associated to the following null hypothesis:
the intercept ($\beta_0$ above) is equal to zero. Using the usual
significance threshold of 0.05, we reject the null hypothesis as
the p-value is smaller than 0.05. However, it is not really interesting
if this intercept is zero or not, since we probably do not care what the
methylation level is when age is zero. In fact, this question does not
even make much sense! In this example, we are more interested
in the regression coefficient associated to age, as that can tell us 
whether there is a linear relationship between age and methylation for the CpG.</p>

  </blockquote>
</blockquote>

<h1 id="fitting-a-lot-of-linear-models">Fitting a lot of linear models</h1>

<p>In the linear model above, we are generally interested in the second regression
coefficient (often referred to as <em>slope</em>) which measures the linear relationship
between age and methylation levels. For the first CpG, here is its estimate:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">coef_age_methyl1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tidy</span><span class="p">(</span><span class="n">lm_age_methyl1</span><span class="p">)[</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">
</span><span class="n">coef_age_methyl1</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code># A tibble: 1 × 5
  term  estimate std.error statistic p.value
  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
1 age    0.00891    0.0100     0.888   0.381
</code></pre></div></div>

<p>In this case, the p-value is equal to 0.381 and therefore we cannot reject the null
hypothesis: there is no statistical evidence to suggest that the regression 
coefficient associated to age is not equal to zero.</p>

<p>Now, we could do this for every feature (CpG) in the dataset and rank the
results based on their test statistic or associated p-value. However, fitting
models in this way to 5000 features is not very computationally 
efficient, and it would also be laborious to do programmatically. There are ways 
to get around this, but first let us talk about what exactly we are doing when 
we look at significance tests in this context.</p>

<h1 id="how-does-hypothesis-testing-for-a-linear-model-work">How does hypothesis testing for a linear model work?</h1>

<p>In order to decide whether a result would be unlikely under the null
hypothesis, we must calculate a test statistic. For coefficient $k$ in a
linear model (in our case, it would be the slope), the test statistic is
a t-statistic given by:</p>

<p>[t_{k} = \frac{\hat{\beta}<em>{k}}{SE\left(\hat{\beta}</em>{k}\right)}]</p>

<p>$SE\left(\hat{\beta}_{k}\right)$ measures the uncertainty we have in our
effect size estimate. Knowing what distribution these t-statistics
follow under the null hypothesis allows us to determine how unlikely it
would be for us to observe what we have under those circumstances, if we
repeated the experiment and analysis over and over again. To
demonstrate, we can compute the t-statistics “by hand” (advanced content).</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">table_age_methyl1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tidy</span><span class="p">(</span><span class="n">lm_age_methyl1</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>We can see that the t-statistic is just the ratio between the coefficient estimate
and the standard error:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tvals</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">table_age_methyl1</span><span class="o">$</span><span class="n">estimate</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">table_age_methyl1</span><span class="o">$</span><span class="n">std.error</span><span class="w">
</span><span class="n">all.equal</span><span class="p">(</span><span class="n">tvals</span><span class="p">,</span><span class="w"> </span><span class="n">table_age_methyl1</span><span class="o">$</span><span class="n">statistic</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] TRUE
</code></pre></div></div>

<p>Calculating the p-values is a bit more tricky. Specifically, it is the
proportion of the distribution of the test statistic under the null
hypothesis that is <em>as extreme or more extreme</em> than the observed value
of the test statistic. This is easy to observe visually, by plotting the
theoretical distribution of the test statistic under the null hypothesis 
(see next call-out box for more details about it):</p>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-02-tdist-1.png" alt="Density plot of a t-distribution showing the observed test statistics (here, t-statistics). The p-values, visualised here with shaded regions, represent the portion of the null distribution that is as extreme or more extreme as the observed test statistics, which are shown as dashed lines." width="432" />
<p class="caption">The p-value for a regression coefficient represents how often it'd be observed under the null.</p>
</div>

<p>The red-ish shaded region represents the portion of the distribution of
the test statistic under the null hypothesis that is equal or greater to
the value we observe for the intercept term. As our null hypothesis 
relates to a 2-tailed test (as the null hypothesis states that the regression
coefficient is equal to zero, we would reject it if the regression
coefficient is substantially larger <strong>or</strong> smaller than zero), the p-value for
the test is twice the value of the shaded region. In this case, the shaded region 
is small relative to the total area of the null distribution; therefore, the
p-value is small ($p=0.013$). The blue-ish shaded region represents the same measure for the slope term; 
this is larger, relative to the total area of the distribution, therefore the 
p-value is larger than the one for the intercept term 
($p=0.381$). The
the p-value is a function of the test statistic: the ratio between the effect size 
we’re estimating and the uncertainty we have in that effect. A large effect with large
uncertainty may not lead to a small p-value, and a small effect with
small uncertainty may lead to a small p-value.</p>

<blockquote class="callout">
  <h2 id="calculating-p-values-from-a-linear-model">Calculating p-values from a linear model</h2>

  <p>Manually calculating the p-value for a linear model is a little bit
more complex than calculating the t-statistic. The intuition posted
above is definitely sufficient for most cases, but for completeness,
here is how we do it:</p>

  <p>Since the statistic in a linear model is a t-statistic, it follows a
student t distribution under the null hypothesis, with degrees of
freedom (a parameter of the student t-distribution) given by the
number of observations minus the number of coefficients fitted, in
this case
$37 - 2 = 35$.
We want to know what portion of the distribution function of the test
statistic is as extreme as, or more  extreme than, the value we observed.
The function<code class="language-plaintext highlighter-rouge">pt()</code>(similar to<code class="language-plaintext highlighter-rouge">pnorm()</code>, etc) can give us this information.</p>

  <p>Since we’re not sure if the coefficient will be larger or smaller than
zero, we want to do a 2-tailed test. Therefore we take the absolute
value of the t-statistic, and look at the upper rather than lower
tail. In the figure above the shaded areas are only looking at “half” of the
t-distribution (which is symmetric around zero), therefore we multiply the 
shaded area by 2 in order to calculate the p-value.</p>

  <p>Combining all of this gives us:</p>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pvals</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">pt</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">tvals</span><span class="p">),</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lm_age_methyl1</span><span class="o">$</span><span class="n">df</span><span class="p">,</span><span class="w"> </span><span class="n">lower.tail</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span><span class="n">all.equal</span><span class="p">(</span><span class="n">table_age_methyl1</span><span class="o">$</span><span class="n">p.value</span><span class="p">,</span><span class="w"> </span><span class="n">pvals</span><span class="p">)</span><span class="w">
</span></code></pre></div>  </div>

  <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] TRUE
</code></pre></div>  </div>
</blockquote>

<h1 id="sharing-information-across-outcome-variables">Sharing information across outcome variables</h1>

<p>Now that we understand how hypothesis tests work in the 
linear model framework, we are going to introduce an idea that allows us to
take advantage of the fact that we carry out many tests at once on
structured data. We can leverage this fact to <em>share information</em>
between model parameters. The insight that we use to perform
<em>information pooling</em> or sharing is derived from our knowledge about the
structure of the data. For example, in a high-throughput experiment like
a DNA methylation assay, we know that all of the features were measured
simultaneously, using the same technique. This means that generally, we
expect the base-level variability for each feature to be broadly
similar.</p>

<p>This can enable us to get a better estimate of the uncertainty of model
parameters than we could get if we consider each feature in isolation.
So, to share information between features allows us to get more robust
estimators. Remember that the t-statistic for coefficient $\beta_k$ in a
linear model is the ratio between the coefficient estimate and its standard
error:</p>

<p>[t_{k} = \frac{\hat{\beta}<em>{k}}{SE\left(\hat{\beta}</em>{k}\right)}]</p>

<p>It is clear that large effect sizes will likely lead to small p-values,
as long as the standard error for the coefficent is not large. However,
the standard error is affected by the amount of noise, as we saw
earlier. If we have a small number of observations, it is common for the
noise for some features to be extremely small simply by chance. This, in turn,
causes small p-values for these features, which may give us unwarranted
confidence in the level of certainty we have in the results (false positives).</p>

<p>There are many statistical methods in genomics that use this type of
approach to get better estimates by pooling information between features
that were measured simultaneously using the same techniques. Here we
will focus on the package <strong><code class="language-plaintext highlighter-rouge">limma</code></strong>, which is an established software
package used to fit linear models, originally for the gene expression
micro-arrays that were common in the 2000s, but which is still in use in
RNAseq experiments, among others. The authors of <strong><code class="language-plaintext highlighter-rouge">limma</code></strong> made some
assumptions about the distributions that these follow, and pool
information across genes to get a better estimate of the uncertainty in
effect size estimates. It uses the idea that noise levels should be
similar between features to <em>moderate</em> the estimates of the test
statistic by shrinking the estimates of standard errors towards a common
value. This results in a <em>moderated t-statistic</em>.</p>

<p>The process of running a model in <strong><code class="language-plaintext highlighter-rouge">limma</code></strong> is somewhat different to what you
may have seen when running linear models. Here, we define a <em>model matrix</em> or 
<em>design matrix</em>, which is a way of representing the
coefficients that should be fit in each linear model. These are used in
similar ways in many different modelling libraries.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"limma"</span><span class="p">)</span><span class="w">
</span><span class="n">design_age</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">model.matrix</span><span class="p">(</span><span class="o">~</span><span class="n">age</span><span class="p">)</span><span class="w">
</span><span class="nf">dim</span><span class="p">(</span><span class="n">design_age</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 37  2
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">head</span><span class="p">(</span><span class="n">design_age</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  (Intercept) age
1           1  39
2           1  49
3           1  20
4           1  49
5           1  33
6           1  21
</code></pre></div></div>

<blockquote class="callout">
  <h2 id="what-is-a-model-matrix">What is a model matrix?</h2>
  <p>When R fits a regression model, it chooses a vector of regression coefficients 
that minimises the differences between outcome values and those values 
predicted by using the covariates (or predictor variables). But how do we get 
from a set of predictors and regression coefficients to predicted values? This 
is done via matrix multipliciation. The matrix of predictors is (matrix) 
multiplied by the vector of coefficients. That matrix is called the 
<strong>model matrix</strong> (or design matrix). It has one row for each observation and 
one column for each predictor plus (by default) one aditional column of ones 
(the intercept column). Many R libraries (but not <strong><code class="language-plaintext highlighter-rouge">limma</code></strong> ) contruct the 
model matrix behind the scenes. Usually, it can be extracted from a model fit 
using the function <code class="language-plaintext highlighter-rouge">model.matrix()</code>. Here is an example:</p>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span><span class="p">(</span><span class="n">cars</span><span class="p">)</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">cars</span><span class="p">)</span><span class="w">
</span></code></pre></div>  </div>

  <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  speed dist
1     4    2
2     4   10
3     7    4
4     7   22
5     8   16
6     9   10
</code></pre></div>  </div>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mod1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">dist</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">speed</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">cars</span><span class="p">)</span><span class="w"> </span><span class="c1"># fit regression model using speed as a predictor</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">model.matrix</span><span class="p">(</span><span class="n">mod1</span><span class="p">))</span><span class="w"> </span><span class="c1"># the model matrix contains two columns: intercept and speed</span><span class="w">
</span></code></pre></div>  </div>

  <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  (Intercept) speed
1           1     4
2           1     4
3           1     7
4           1     7
5           1     8
6           1     9
</code></pre></div>  </div>
</blockquote>

<p>As you can see, the design matrix has the same number of rows as our
methylation data has samples. It also has two columns - one for the
intercept (similar to the linear model we fit above) and one for age.
This happens “under the hood” when fitting a linear model with <code class="language-plaintext highlighter-rouge">lm()</code>, but
here we have to specify it directly. The <a href="https://www.bioconductor.org/packages/release/bioc/vignettes/limma/inst/doc/usersguide.pdf">limma user
manual</a>
has more detail on how to make design matrices for different types of
experimental design, but here we are going to stick with this simple two-variable case.</p>

<p>We then pass our matrix of methylation values into <code class="language-plaintext highlighter-rouge">lmFit()</code>, specifying
the design matrix. Internally, this function runs <code class="language-plaintext highlighter-rouge">lm()</code> on each row of
the data in an efficient way. The function <code class="language-plaintext highlighter-rouge">eBayes()</code>, when applied to the
output of <code class="language-plaintext highlighter-rouge">lmFit()</code>, performs the pooled estimation of standard errors
that results in the moderated t-statistics and resulting p-values.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fit_age</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lmFit</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">,</span><span class="w"> </span><span class="n">design</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">design_age</span><span class="p">)</span><span class="w">
</span><span class="n">fit_age</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">eBayes</span><span class="p">(</span><span class="n">fit_age</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>To obtain the results of the linear models, we can use the <code class="language-plaintext highlighter-rouge">topTable()</code>
function. By default, this returns results for the first coefficient in
the model. As we saw above when using <code class="language-plaintext highlighter-rouge">lm()</code>, and when we defined
<code class="language-plaintext highlighter-rouge">design_age</code> above, the first coefficient relates to the intercept term,
which we are not particularly interested in here; therefore we specify
<code class="language-plaintext highlighter-rouge">coef = 2</code>. Further, <code class="language-plaintext highlighter-rouge">topTable()</code> by default only returns the top 10
results. To see all of the results in the data, we specify
<code class="language-plaintext highlighter-rouge">number = nrow(fit_age)</code> to ensure that it returns a row for every row
of the input matrix.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">toptab_age</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">topTable</span><span class="p">(</span><span class="n">fit_age</span><span class="p">,</span><span class="w"> </span><span class="n">coef</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nrow</span><span class="p">(</span><span class="n">fit_age</span><span class="p">))</span><span class="w">
</span><span class="n">orderEffSize</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rev</span><span class="p">(</span><span class="n">order</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">toptab_age</span><span class="o">$</span><span class="n">logFC</span><span class="p">)))</span><span class="w"> </span><span class="c1"># order by effect size (absolute log-fold change)</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">toptab_age</span><span class="p">[</span><span class="n">orderEffSize</span><span class="p">,</span><span class="w"> </span><span class="p">])</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                 logFC    AveExpr         t    P.Value adj.P.Val         B
cg22160073 -0.07615967  0.2261869 -2.200534 0.03410063 0.2563957 -5.536033
cg02371766 -0.07480442  1.6744282 -2.032526 0.04933710 0.3004092 -5.861179
cg18633711 -0.07221177 -0.1668962 -2.254569 0.03017881 0.2459313 -5.427228
cg01267675 -0.06393861  1.2496114 -2.127641 0.04010694 0.2758387 -5.679584
cg07334644 -0.05880317  0.9591176 -2.297448 0.02735916 0.2339981 -5.339467
cg01387455 -0.05873510  0.5872700 -2.051339 0.04737637 0.2964936 -5.825782
</code></pre></div></div>

<p>The output of <code class="language-plaintext highlighter-rouge">topTable</code> includes the coefficient, here termed a log
fold change <code class="language-plaintext highlighter-rouge">logFC</code>, the average level (<code class="language-plaintext highlighter-rouge">aveExpr</code>), the t-statistic <code class="language-plaintext highlighter-rouge">t</code>,
the p-value (<code class="language-plaintext highlighter-rouge">P.Value</code>), and the <em>adjusted</em> p-value (<code class="language-plaintext highlighter-rouge">adj.P.Val</code>). We’ll
cover what an adjusted p-value is very shortly. The table also includes
<code class="language-plaintext highlighter-rouge">B</code>, which represents the log-odds that a feature is signficantly
different, which we won’t cover here, but which will generally be a 1-1
transformation of the p-value. The coefficient estimates here are termed
<code class="language-plaintext highlighter-rouge">logFC</code> for legacy reasons relating to how microarray experiments were
traditionally performed. There are more details on this topic in many
places, for example <a href="https://kasperdanielhansen.github.io/genbioconductor/html/limma.html">this tutorial by Kasper D.
Hansen</a></p>

<p>Now we have estimates of effect sizes and p-values for the association
between methylation level at each locus and age for our 37 samples. It’s
useful to create a plot of effect size estimates (model coefficients)
against p-values for each of these linear models, to visualise the
magnitude of effects and the statistical significance of each. These
plots are often called “volcano plots”, because they resemble an
eruption.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">toptab_age</span><span class="o">$</span><span class="n">logFC</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">log10</span><span class="p">(</span><span class="n">toptab_age</span><span class="o">$</span><span class="n">P.Value</span><span class="p">),</span><span class="w">
    </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Effect size"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bquote</span><span class="p">(</span><span class="o">-</span><span class="n">log</span><span class="p">[</span><span class="m">10</span><span class="p">](</span><span class="n">p</span><span class="o">-</span><span class="n">value</span><span class="p">)),</span><span class="w">
    </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">19</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-02-limmavolc1-1.png" alt="A plot of -log10(p) against effect size estimates for a regression of age against methylation using limma." width="432" />
<p class="caption">Plotting p-values against effect sizes using limma; the results are similar to a standard linear model.</p>
</div>

<p>In this figure, every point represents a feature of interest. The x-axis
represents the effect size observed for that feature in a linear model,
while the y-axis is the $-\log_{10}(\text{p-value})$, where larger
values indicate increasing statistical evidence of a non-zero effect
size. A positive effect size represents increasing methylation with
increasing age, and a negative effect size represents decreasing
methylation with increasing age. Points higher on the x-axis represent
features for which we think the results we observed would be very
unlikely under the null hypothesis.</p>

<p>Since we want to identify features that have different methylation levels
in different age groups, in an ideal case there would be clear
separation between “null” and “non-null” features. However, usually we
observe results as we do here: there is a continuum of effect sizes and
p-values, with no clear separation between these two classes of
features. While statistical methods exist to derive insights from
continuous measures like these, it is often convenient to obtain a list
of features which we are confident have non-zero effect sizes. This is
made more difficult by the number of tests we perform.</p>

<blockquote class="challenge">
  <h2 id="challenge-3">Challenge 3</h2>

  <p>The effect size estimates are very small, and yet many of the p-values
are well below a usual significance level of p &lt; 0.05. Why is this?</p>

  <blockquote class="solution">
    <h2 id="solution-2">Solution</h2>

    <p>Because age has a much larger range than methylation levels, the
unit change in methylation level even for a strong relationship is
very small!</p>

    <p>As we mentioned, the p-value is a function of both the effect size
estimate and the uncertainty (standard error) of that estimate.
Because the uncertainty in our estimates is much smaller than the
estimates themselves, the p-values are also small.</p>

    <p>If we predicted age using methylation level, it is likely we would see
much larger coefficients, though broadly similar p-values!</p>

  </blockquote>
</blockquote>

<p>It is worthwhile considering what exactly the effect of the <em>moderation</em>
or information sharing that <strong><code class="language-plaintext highlighter-rouge">limma</code></strong> performs has on our results. To do
this, let us compare the effect sizes estimates and p-values from the two
approaches.</p>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-02-plot-limma-lm-effect-1.png" alt="plot of chunk plot-limma-lm-effect" width="432" />
<p class="caption">plot of chunk plot-limma-lm-effect</p>
</div>

<p>These are exactly identical! This is because <strong><code class="language-plaintext highlighter-rouge">limma</code></strong> does not perform
any sharing of information when estimating effect sizes. This is in
contrast to similar packages that apply shrinkage to the effect size
estimates, like <strong><code class="language-plaintext highlighter-rouge">DESeq2</code></strong>. These often use information sharing to shrink
or moderate the effect size estimates, in the case of <strong><code class="language-plaintext highlighter-rouge">DESeq2</code></strong> by again
sharing information between features about sample-to-sample variability.
In contrast, let us look at the p-values from <strong><code class="language-plaintext highlighter-rouge">limma</code></strong> and R’s built-in <code class="language-plaintext highlighter-rouge">lm()</code> function:</p>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-02-plot-limma-lm-pval-1.png" alt="plot of chunk plot-limma-lm-pval" width="432" />
<p class="caption">plot of chunk plot-limma-lm-pval</p>
</div>

<p>we can see that for the vast majority of features, the results are
broadly similar. There seems to be a minor general tendency for <strong><code class="language-plaintext highlighter-rouge">limma</code></strong>
to produce smaller p-values, but for several features, the p-values from
limma are considerably larger than the p-values from <code class="language-plaintext highlighter-rouge">lm()</code>. This is
because the information sharing tends to shrink large standard error
estimates downwards and small estimates upwards. When the degree of
statistical significance is due to an abnormally small standard error
rather than a large effect, this effect results in this prominent
reduction in statistical significance, which has been shown to perform
well in case studies. The degree of shrinkage generally depends on the
amount of pooled information and the strength of the evidence
independent of pooling. For example, with very few samples and many
features, information sharing has a larger effect, because there are a
lot of genes that can be used to provide pooled estimates, and the
evidence from the data that this is weighed against is relatively
sparse. In contrast, when there are many samples and few features, there
is not much opportunity to generate pooled estimates, and the evidence
of the data can easily outweigh the pooling.</p>

<p>Shrinkage methods like these ones can be complex to implement and
understand, but it is useful to develop an intuition about why these approaches may be more
precise and sensitive than the naive approach of fitting a model to each
feature separately.</p>

<blockquote class="challenge">
  <h2 id="challenge-4">Challenge 4</h2>

  <ol>
    <li>Try to run the same kind of linear model with smoking status as
covariate instead of age, and making a volcano plot. <em>Note:
smoking status is stored as</em> <code class="language-plaintext highlighter-rouge">methylation$smoker</code>.</li>
    <li>We saw in the example in the lesson that this information sharing
can lead to larger p-values. Why might this be preferable?</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution-3">Solution</h2>

    <ol>
      <li>
        <p>The following code runs the same type of model with smoking
status:</p>

        <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">design_smoke</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">model.matrix</span><span class="p">(</span><span class="o">~</span><span class="n">methylation</span><span class="o">$</span><span class="n">smoker</span><span class="p">)</span><span class="w">
</span><span class="n">fit_smoke</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lmFit</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">,</span><span class="w"> </span><span class="n">design</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">design_smoke</span><span class="p">)</span><span class="w">
</span><span class="n">fit_smoke</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">eBayes</span><span class="p">(</span><span class="n">fit_smoke</span><span class="p">)</span><span class="w">
</span><span class="n">toptab_smoke</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">topTable</span><span class="p">(</span><span class="n">fit_smoke</span><span class="p">,</span><span class="w"> </span><span class="n">coef</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nrow</span><span class="p">(</span><span class="n">fit_smoke</span><span class="p">))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">toptab_smoke</span><span class="o">$</span><span class="n">logFC</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">log10</span><span class="p">(</span><span class="n">toptab_smoke</span><span class="o">$</span><span class="n">P.Value</span><span class="p">),</span><span class="w">
    </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Effect size"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bquote</span><span class="p">(</span><span class="o">-</span><span class="n">log</span><span class="p">[</span><span class="m">10</span><span class="p">](</span><span class="n">p</span><span class="p">)),</span><span class="w">
    </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">19</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div>        </div>

        <div class="figure" style="text-align: center">
<img src="../fig/rmd-02-limmavolc2-1.png" alt="A plot of -log10(p) against effect size estimates for a regression of smoking status against methylation using limma." width="432" />
<p class="caption">A plot of significance against effect size for a regression of smoking against methylation.</p>
</div>
      </li>
      <li>
        <p>Being a bit more conservative when identifying features can help
to avoid false discoveries. Furthermore, when rejecting the null
hypothesis is based more on a small standard error resulting
from abnormally low levels of variability for a given feature,
we might want to be a bit more conservative in our expectations.</p>
      </li>
    </ol>
  </blockquote>
</blockquote>

<blockquote class="callout">
  <h2 id="shrinkage">Shrinkage</h2>

  <p>Shrinkage is an intuitive term for an effect of information sharing,
and is something observed in a broad range of statistical models.
Often, shrinkage is induced by a <em>multilevel</em> modelling approach or by
<em>Bayesian</em> methods.</p>

  <p>The general idea is that these models incorporate information about
the structure of the data into account when fitting the parameters. We
can share information between features because of our knowledge about
the data structure; this generally requires careful consideration
about how the data were generated and the relationships within.</p>

  <p>An example people often use is estimating the effect of attendance on
grades in several schools. We can assume that this effect is similar
in different schools (but maybe not identical), so we can <em>share
information</em> about the effect size between schools and shrink our
estimates towards a common value.</p>

  <p>For example in <strong><code class="language-plaintext highlighter-rouge">DESeq2</code></strong>, the authors used the observation that genes
with similar expression counts in RNAseq data have similar
<em>dispersion</em>, and a better estimate of these dispersion parameters
makes estimates of fold changes much more stable. Similarly, in
<strong><code class="language-plaintext highlighter-rouge">limma</code></strong> the authors made the assumption that in the absence of
biological effects, we can often expect the technical variation in the
measurement of the expression of each of the genes to be broadly
similar. Again, better estimates of variability allow us to prioritise
genes in a more reliable way.</p>

  <p>There are many good resources to learn about this type of approach,
including:</p>

  <ul>
    <li><a href="https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/">a blog post by TJ
Mahr</a></li>
    <li><a href="https://gumroad.com/l/empirical-bayes">a book by David Robinson</a></li>
    <li><a href="http://www.stat.columbia.edu/~gelman/arm/">a (relatively technical) book by Gelman and
Hill</a></li>
  </ul>
</blockquote>

<h1 id="the-problem-of-multiple-tests">The problem of multiple tests</h1>

<p>With such a large number of features, it would be useful to decide which
features are “interesting” or “significant” for further study. However,
if we were to apply a normal significance threshold of 0.05, it would be likely
we end up with a lot of false positives. This is because a p-value
threshold like this represents a $\frac{1}{20}$ chance that we observe
results as extreme or more extreme under the null hypothesis (that there
is no assocation between age and methylation level). If we carry out many more
than 20 such tests, we can expect to see situations where, despite the null
hypothesis being true, we observe observe signifiant p-values due to random chance. To
demonstrate this, it is useful to see what happens if we permute (scramble) the age values and
run the same test again:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">age_perm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">age</span><span class="p">[</span><span class="n">sample</span><span class="p">(</span><span class="n">ncol</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">),</span><span class="w"> </span><span class="n">ncol</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">))]</span><span class="w">
</span><span class="n">design_age_perm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">model.matrix</span><span class="p">(</span><span class="o">~</span><span class="n">age_perm</span><span class="p">)</span><span class="w">

</span><span class="n">fit_age_perm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lmFit</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">,</span><span class="w"> </span><span class="n">design</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">design_age_perm</span><span class="p">)</span><span class="w">
</span><span class="n">fit_age_perm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">eBayes</span><span class="p">(</span><span class="n">fit_age_perm</span><span class="p">)</span><span class="w">
</span><span class="n">toptab_age_perm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">topTable</span><span class="p">(</span><span class="n">fit_age_perm</span><span class="p">,</span><span class="w"> </span><span class="n">coef</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nrow</span><span class="p">(</span><span class="n">fit_age_perm</span><span class="p">))</span><span class="w">

</span><span class="n">plot</span><span class="p">(</span><span class="n">toptab_age_perm</span><span class="o">$</span><span class="n">logFC</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">log10</span><span class="p">(</span><span class="n">toptab_age_perm</span><span class="o">$</span><span class="n">P.Value</span><span class="p">),</span><span class="w">
    </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Effect size"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bquote</span><span class="p">(</span><span class="o">-</span><span class="n">log</span><span class="p">[</span><span class="m">10</span><span class="p">](</span><span class="n">p</span><span class="p">)),</span><span class="w">
    </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">19</span><span class="w">
</span><span class="p">)</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">log10</span><span class="p">(</span><span class="m">0.05</span><span class="p">),</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"red"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-02-volcplotfake-1.png" alt="Plot of -log10(p) against effect size estimates for a regression of a made-up feature against methylation level for each feature in the data. A dashed line represents a 0.05 significance level." width="432" />
<p class="caption">Plotting p-values against effect sizes for a randomised outcome shows we still observe 'significant' results.</p>
</div>

<p>Since we have generated a random sequence of ages, we have no reason to
suspect that there is a true association between methylation levels and
this sequence of random numbers. However, you can see that the p-value
for many features is still lower than a traditional significance level
of $p=0.05$. In fact, here 235
features are significant at p &lt; 0.05. If we were to use this fixed
threshold in a real experiment, it is likely that we would identify many
features as associated with age, when the results we are observing are
simply due to chance.</p>

<blockquote class="challenge">
  <h2 id="challenge-5">Challenge 5</h2>

  <ol>
    <li>If we run 5000 tests under the null hypothesis,
how many of them (on average) will be statistically significant at
a threshold of $p &lt; 0.05$?</li>
    <li>Why would we want to be conservative in labelling features as
significantly different? By conservative, we mean to err towards
labelling true differences as “not significant” rather than vice
versa.</li>
    <li>How could we account for a varying number of tests to ensure
“significant” changes are truly different?</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution-4">Solution</h2>

    <ol>
      <li>By default we expect
$5000 \times 0.05 = 250$
features to be statistically significant under the null
hypothesis, because p-values should always be uniformly
distributed under the null hypothesis.</li>
      <li>Features that we label as “significantly different” will often
be reported in manuscripts. We may also spend time and money
investigating them further, computationally or in the lab.
Therefore, spurious results have a real cost for ourselves and
for others.</li>
      <li>One approach to controlling for the number of tests is to divide
our significance threshold by the number of tests performed.
This is termed “Bonferroni correction” and we’ll discuss this
further now.</li>
    </ol>
  </blockquote>
</blockquote>

<h1 id="adjusting-for-multiple-tests">Adjusting for multiple tests</h1>

<p>When performing many statistical tests to categorise features, we are
effectively classifying features as “non-significant” or “significant”, that latter meaning those for
which we reject the null hypothesis. We also
generally hope that there is a subset of features for which the null
hypothesis is truly false, as well as many for which the null truly does
hold. We hope that for all features for which the null hypothesis is
true, we accept it, and for all features for which the null hypothesis
is not true, we reject it. As we showed in the example with permuted
age, with a large number of tests it is inevitable that we will get some of
these wrong.</p>

<p>We can think of these features as being “truly different” or “not truly
different”<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. Using this idea, we can see that each categorisation we
make falls into four categories:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">Label as different</th>
      <th style="text-align: right">Label as not different</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">Truly different</td>
      <td style="text-align: right">True positive</td>
      <td style="text-align: right">False negative</td>
    </tr>
    <tr>
      <td style="text-align: right">Truly not different</td>
      <td style="text-align: right">False positive</td>
      <td style="text-align: right">True negative</td>
    </tr>
  </tbody>
</table>

<p>If the null hypothesis was true for every feature, then as we perform
more and more tests we’d tend to correctly categorise most results as
negative. However, since p-values are uniformly distributed under the
null, at a significance level of 5%, 5% of all results will be
“significant” even though we would expect to see these results, given
the null hypothesis is true, simply by chance. These would fall under
the label “false positives” in the table above, and are also termed
“false discoveries.”</p>

<p>There are two common ways of controlling these false discoveries. The
first is to say, when we’re doing $n$ tests, that we want to have the
same certainty of making one false discovery with $n$ tests as we have
if we’re only doing one test. This is “Bonferroni” correction,<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> which
divides the significance level by the number of tests performed, $n$.
Equivalently, we can use the non-transformed p-value threshold but
multiply our p-values by the number of tests. This is often very
conservative, especially with a lot of features!</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p_raw</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">toptab_age</span><span class="o">$</span><span class="n">P.Value</span><span class="w">
</span><span class="n">p_fwer</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">p.adjust</span><span class="p">(</span><span class="n">p_raw</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"bonferroni"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"ggplot2"</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">aes</span><span class="p">(</span><span class="n">p_raw</span><span class="p">,</span><span class="w"> </span><span class="n">p_fwer</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_point</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">scale_x_log10</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">scale_y_log10</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_abline</span><span class="p">(</span><span class="n">slope</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">linetype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_hline</span><span class="p">(</span><span class="n">yintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.05</span><span class="p">,</span><span class="w"> </span><span class="n">linetype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"red"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_vline</span><span class="p">(</span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.05</span><span class="p">,</span><span class="w"> </span><span class="n">linetype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"red"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Raw p-value"</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Bonferroni p-value"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-02-p-fwer-1.png" alt="Plot of Bonferroni-adjusted p-values (y) against unadjusted p-values (x). A dashed black line represents the identity (where x=y), while dashed red lines represent 0.05 significance thresholds." width="432" />
<p class="caption">Bonferroni correction often produces very large p-values, especially with low sample sizes.</p>
</div>

<p>You can see that the p-values are exactly one for the vast majority of
tests we performed! This is not ideal sometimes, because unfortunately
we usually don’t have very large sample sizes in health sciences.</p>

<p>The second main way of controlling for multiple tests is to control the
<em>false discovery rate</em>.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> This is the proportion of false positives,
or false discoveries, we’d expect to get each time if we repeated the
experiment over and over.</p>

<ol>
  <li>Rank the p-values</li>
  <li>Assign each a rank (1 is smallest)</li>
  <li>Calculate the critical value \(q = \left(\frac{i}{m}\right)Q\), where $i$ is rank, $m$ is the number of tests, and $Q$ is the
false discovery rate we want to target.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup></li>
  <li>Find the largest p-value less than the critical value. All smaller
than this are significant.</li>
</ol>

<table>
  <thead>
    <tr>
      <th style="text-align: left">FWER</th>
      <th style="text-align: left">FDR</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">+ Controls probability of identifying a false positive</td>
      <td style="text-align: left">+ Controls rate of false discoveries</td>
    </tr>
    <tr>
      <td style="text-align: left">+ Strict error rate control</td>
      <td style="text-align: left">+ Allows error control with less stringency</td>
    </tr>
    <tr>
      <td style="text-align: left">- Very conservative</td>
      <td style="text-align: left">- Does not control probability of making errors</td>
    </tr>
    <tr>
      <td style="text-align: left">- Requires larger statistical power</td>
      <td style="text-align: left">- May result in false discoveries</td>
    </tr>
  </tbody>
</table>

<blockquote class="challenge">
  <h2 id="challenge-6">Challenge 6</h2>

  <ol>
    <li>At a significance level of 0.05, with 100 tests performed, what is
the Bonferroni significance threshold?</li>
    <li>In a gene expression experiment, after FDR correction we observe
500 significant genes. What proportion of these genes are truly
different?</li>
    <li>Try running FDR correction on the <code class="language-plaintext highlighter-rouge">p_raw</code> vector. <em>Hint: check
<code class="language-plaintext highlighter-rouge">help("p.adjust")</code> to see what the method is called</em>.<br />
Compare these values to the raw p-values and the Bonferroni
p-values.</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution-5">Solution</h2>

    <ol>
      <li>
        <p>The Bonferroni threshold for this significance threshold is \(\frac{0.05}{100} = 0.0005\)</p>
      </li>
      <li>
        <p>Trick question! We can’t say what proportion of these genes are
truly different. However, if we repeated this experiment and
statistical test over and over, on average 5% of the results
from each run would be false discoveries.</p>
      </li>
      <li>
        <p>The following code runs FDR correction and compares it to
non-corrected values and to Bonferroni:</p>

        <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p_fdr</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">p.adjust</span><span class="p">(</span><span class="n">p_raw</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"BH"</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">aes</span><span class="p">(</span><span class="n">p_raw</span><span class="p">,</span><span class="w"> </span><span class="n">p_fdr</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_point</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">scale_x_log10</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">scale_y_log10</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_abline</span><span class="p">(</span><span class="n">slope</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">linetype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_hline</span><span class="p">(</span><span class="n">yintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.05</span><span class="p">,</span><span class="w"> </span><span class="n">linetype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"red"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_vline</span><span class="p">(</span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.05</span><span class="p">,</span><span class="w"> </span><span class="n">linetype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"red"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Raw p-value"</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Benjamini-Hochberg p-value"</span><span class="p">)</span><span class="w">
</span></code></pre></div>        </div>

        <div class="figure" style="text-align: center">
<img src="../fig/rmd-02-p-fdr-1.png" alt="Plot of Benjamini-Hochberg-adjusted p-values (y) against unadjusted p-values (x). A dashed black line represents the identity (where x=y), while dashed red lines represent 0.05 significance thresholds." width="432" />
<p class="caption">Benjamini-Hochberg correction is less conservative than Bonferroni</p>
</div>

        <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ggplot</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">aes</span><span class="p">(</span><span class="n">p_fdr</span><span class="p">,</span><span class="w"> </span><span class="n">p_fwer</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_point</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">scale_x_log10</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">scale_y_log10</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_abline</span><span class="p">(</span><span class="n">slope</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">linetype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_hline</span><span class="p">(</span><span class="n">yintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.05</span><span class="p">,</span><span class="w"> </span><span class="n">linetype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"red"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_vline</span><span class="p">(</span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.05</span><span class="p">,</span><span class="w"> </span><span class="n">linetype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"red"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Benjamini-Hochberg p-value"</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Bonferroni p-value"</span><span class="p">)</span><span class="w">
</span></code></pre></div>        </div>

        <div class="figure" style="text-align: center">
<img src="../fig/rmd-02-plot-fdr-fwer-1.png" alt="Plot of Benjamini-Hochberg-adjusted p-values (y) against Bonferroni-adjusted p-values (x). A dashed black line represents the identity (where x=y), while dashed red lines represent 0.05 significance thresholds." width="432" />
<p class="caption">plot of chunk plot-fdr-fwer</p>
</div>
      </li>
    </ol>

  </blockquote>
</blockquote>

<blockquote class="callout">
  <h2 id="feature-selection">Feature selection</h2>

  <p>In this episode, we have focussed on regression in a setting where there are more
features than observations. This approach is relevant if we are interested in the
association of each feature with some outcome or if we want to screen for features
that have a strong association with an outcome. If, however, we are interested in
predicting an outcome or if we want to know which features explain the variation
in the outcome, we may want to restrict ourselves to a subset of relevant features.
One way of doing this is called <em>regularisation</em>, and this is the topic of the next episode.
An alternative is called <em>feature selection</em>. This is covered in the subsequent (optional) episode.</p>
</blockquote>

<h2 id="further-reading">Further reading</h2>

<ul>
  <li><a href="https://kasperdanielhansen.github.io/genbioconductor/html/limma.html"><strong><code class="language-plaintext highlighter-rouge">limma</code></strong> tutorial by Kasper D.
Hansen</a></li>
  <li><a href="https://www.bioconductor.org/packages/release/bioc/vignettes/limma/inst/doc/usersguide.pdf"><strong><code class="language-plaintext highlighter-rouge">limma</code></strong> user
manual</a>.</li>
  <li><a href="https://bioconductor.org/packages/release/bioc/vignettes/variancePartition/inst/doc/dream.html">The <strong><code class="language-plaintext highlighter-rouge">VariancePartition</code></strong> package</a> has similar functionality to <strong><code class="language-plaintext highlighter-rouge">limma</code></strong> but allows the inclusion of random effects.</li>
</ul>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>“True difference” is a hard category to rigidly define. As we’ve
seen, with a lot of data, we can detect tiny differences, and with
little data, we can’t detect large differences. However, both can be
argued to be “true”. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Bonferroni correction is also termed “family-wise” error rate
control. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>This is often called “Benjamini-Hochberg” adjustment. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>People often perform extra controls on FDR-adjusted p-values,
ensuring that ranks don’t change and the critical value is never
smaller than the original p-value. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>Performing linear regression in a high-dimensional setting requires us to perform hypothesis testing in a way that low-dimensional regression may not.</p>
</li>
    
    <li><p>Sharing information between features can increase power and reduce false positives.</p>
</li>
    
    <li><p>When running a lot of null hypothesis tests for high-dimensional data, multiple testing correction allows retain power and avoid making costly false discoveries.</p>
</li>
    
    <li><p>Multiple testing methods can be more conservative or more liberal, depending on our goals.</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="regularised-regression" class="maintitle">Regularised regression</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 60 min
      <br />
      <strong>Exercises:</strong> 20 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>What is regularisation?</p>
</li>
	
	<li><p>How does regularisation work?</p>
</li>
	
	<li><p>How can we select the level of regularisation for a model?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Understand the benefits of regularised models.</p>
</li>
	
	<li><p>Understand how different types of regularisation work.</p>
</li>
	
	<li><p>Apply and critically analyse regularised regression models.</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h1 id="introduction">Introduction</h1>

<p>This episode is about <strong>regularisation</strong>, also called <strong>regularised regression</strong> 
or <strong>penalised regression</strong>. This approach can be used for prediction and for 
feature selection and it is particularly useful when dealing with high-dimensional data.</p>

<p>One reason that we need special statistical tools for high-dimensional data is
that standard linear models cannot handle high-dimensional data sets – one cannot fit
a linear model where there are more features (predictor variables) than there are observations
(data points). In the previous lesson we dealt with this problem by fitting individual
models for each feature and sharing information among these models. Now we will
take a look at an alternative approach called regularisation. Regularisation can be used to
stabilise coefficient estimates (and thus to fit models with more features than observations)
and even to select a subset of relevant features.</p>

<p>First, let us check out what happens if we try to fit a linear model to high-dimensional
data! We start by reading in the data from the last lesson:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"here"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"minfi"</span><span class="p">)</span><span class="w">
</span><span class="n">methylation</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readRDS</span><span class="p">(</span><span class="n">here</span><span class="p">(</span><span class="s2">"data/methylation.rds"</span><span class="p">))</span><span class="w">

</span><span class="c1">## here, we transpose the matrix to have features as rows and samples as columns</span><span class="w">
</span><span class="n">methyl_mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">assay</span><span class="p">(</span><span class="n">methylation</span><span class="p">))</span><span class="w">
</span><span class="n">age</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">methylation</span><span class="o">$</span><span class="n">Age</span><span class="w">
</span></code></pre></div></div>

<p>Then, we try to fit a model with outcome age and all 5,000 features in this
dataset as predictors (average methylation levels, M-values, across different
sites in the genome).</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># by using methyl_mat in the formula below, R will run a multivariate regression</span><span class="w">
</span><span class="c1"># model in which each of the columns in methyl_mat is used as a predictor. </span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">age</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">methyl_mat</span><span class="p">)</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Call:
lm(formula = age ~ methyl_mat)

Residuals:
ALL 37 residuals are 0: no residual degrees of freedom!

Coefficients: (4964 not defined because of singularities)
                          Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)               2640.474        NaN     NaN      NaN
methyl_matcg00075967      -108.216        NaN     NaN      NaN
methyl_matcg00374717      -139.637        NaN     NaN      NaN
methyl_matcg00864867        33.102        NaN     NaN      NaN
methyl_matcg00945507        72.250        NaN     NaN      NaN
 [ reached getOption("max.print") -- omitted 4996 rows ]

Residual standard error: NaN on 0 degrees of freedom
Multiple R-squared:      1,	Adjusted R-squared:    NaN 
F-statistic:   NaN on 36 and 0 DF,  p-value: NA
</code></pre></div></div>

<p>You can see that we’re able to get some effect size estimates, but they seem very 
high! The summary also says that we were unable to estimate
effect sizes for 4,964 features
because of “singularities”. What this means is that R couldn’t find a way to
perform the calculations necessary due to the fact that we have more features
than observations.</p>

<blockquote class="callout">
  <h2 id="singularities">Singularities</h2>

  <p>The message that <code class="language-plaintext highlighter-rouge">lm</code> produced is not necessarily the most intuitive. What
are “singularities”, and why are they an issue? A singular matrix 
is one that cannot be
<a href="https://en.wikipedia.org/wiki/Invertible_matrix">inverted</a>.
The inverse of an $n \times n$ square matrix $A$ is the matrix $B$ for which
$AB = BA = I_n$, where $I_n$ is the $n \times n$ identity matrix.</p>

  <p>Why is the inverse important? Well, to find the
coefficients of a linear model of a matrix of predictor features $X$ and an
outcome vector $y$, we may perform the calculation</p>

\[(X^TX)^{-1}X^Ty\]

  <p>You can see that, if we’re unable to find the inverse of the matrix $X^TX$,
then we’ll be unable to find the regression coefficients.</p>

  <p>Why might this be the case?
Well, when the <a href="https://en.wikipedia.org/wiki/Determinant">determinant</a>
of the matrix is zero, we are unable to find its inverse.</p>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xtx</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">methyl_mat</span><span class="w">
</span><span class="n">det</span><span class="p">(</span><span class="n">xtx</span><span class="p">)</span><span class="w">
</span></code></pre></div>  </div>

  <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 0
</code></pre></div>  </div>
</blockquote>

<blockquote class="callout">
  <h2 id="correlated-features--common-in-high-dimensional-data">Correlated features – common in high-dimensional data</h2>

  <p>So, we can’t fit a standard linear model to high-dimensional data. But there
is another issue. In high-dimensional datasets, there
are often multiple features that contain redundant information (correlated features).</p>

  <p>We have seen in the first episode that correlated features can make it hard 
(or impossible) to correctly infer parameters. If we visualise the level of 
correlation between sites in the methylation dataset, we can see that many 
of the features essentially represent the same information - there are many 
off-diagonal cells, which are deep red or blue. For example, the following
heatmap visualises the correlations for the first 500 features in the 
<code class="language-plaintext highlighter-rouge">methylation</code> dataset (we selected 500 features only as it can be hard to
visualise patterns when there are too many features!).</p>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"ComplexHeatmap"</span><span class="p">)</span><span class="w">
</span><span class="n">small</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">methyl_mat</span><span class="p">[,</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">500</span><span class="p">]</span><span class="w">
</span><span class="n">cor_mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cor</span><span class="p">(</span><span class="n">small</span><span class="p">)</span><span class="w">
</span><span class="n">Heatmap</span><span class="p">(</span><span class="n">cor_mat</span><span class="p">,</span><span class="w">
    </span><span class="n">column_title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Feature-feature correlation in methylation data"</span><span class="p">,</span><span class="w">
    </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Pearson correlation"</span><span class="p">,</span><span class="w">
    </span><span class="n">show_row_dend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">show_column_dend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w">
    </span><span class="n">show_row_names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">show_column_names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div>  </div>

  <div class="figure" style="text-align: center">
<img src="../fig/rmd-03-corr-mat-meth-1.png" alt="Alt" width="432" />
<p class="caption">Cap</p>
</div>

  <p>Correlation between features can be problematic for technical reasons. If it is 
very severe, it may even make it impossible to fit a model! This is in addition to
the fact that with more features than observations, we can’t even estimate
the model properly. Regularisation can help us to deal with correlated features.</p>
</blockquote>

<blockquote class="challenge">
  <h2 id="challenge-1">Challenge 1</h2>

  <p>Discuss in  groups:</p>

  <ol>
    <li>Why would we observe correlated features in high-dimensional biological
data?</li>
    <li>Why might correlated features be a problem when fitting linear models?</li>
    <li>What issue might correlated features present when selecting features to include in a model one at a time?</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution">Solution</h2>

    <ol>
      <li>Many of the features in biological data represent very similar 
information biologically. For example, sets of genes that form complexes
are often expressed in very similar quantities. Similarly, methylation
levels at nearby sites are often very highly correlated.</li>
      <li>Correlated features can make inference unstable or even impossible
mathematically.</li>
      <li>When we are selecting features one at a time we want to pick the most predictive feature each time. 
When a lot of features are very similar but encode
slightly different information, which of the correlated features we 
select to include can have a huge impact on the later stages of model selection!</li>
    </ol>

  </blockquote>
</blockquote>

<h1 id="coefficient-estimates-of-a-linear-model">Coefficient estimates of a linear model</h1>

<p>When we fit a linear model, we’re finding the line through our data that 
minimises the sum of the squared residuals. We can think of that as finding
the slope and intercept that minimises the square of the length of the dashed
lines. In this case, the red line in the left panel is the line that
accomplishes this objective, and the red dot in the right panel is the point 
that represents this line in terms of its slope and intercept among many 
different possible models, where the background colour represents how well
different combinations of slope and intercept accomplish this objective.</p>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-03-regplot-1.png" alt="For each observation, the left panel shows the residuals with respect to the optimal line (the one that minimises the sum of square errors). These are calculated as the difference between the value predicted by the line and the observed outcome. Right panel shows the sum of squared residuals across all possible linear regression models (as defined by different values of the regression coefficients)." width="720" />
<p class="caption">Illustrative example demonstrated how regression coefficients are inferred under a linear model framework.</p>
</div>

<p>Mathematically, we can write the sum of squared residuals as</p>

<p>[\sum_{i=1}^N ( y_i-x’_i\beta)^2]</p>

<p>where $\beta$ is a vector of (unknown) covariate effects which we want to learn
by fitting a regression model: the $j$-th element of $\beta$, which we denote as 
$\beta_j$ quantifies the effect of the $j$-th covariate. For each individual 
$i$, $x_i$ is a vector of $j$ covariate values and $y_i$ is the true observed value for 
the outcome. The notation $x’_i\beta$ indicates matrix multiplication. In this case, the 
result is equivalent to multiplying each element of $x_i$ by its corresponding element in 
$\beta$ and then calculating the sum across all of those values. The result of this 
product (often denoted by $\hat{y}_i$) is the predicted value of the outcome generated
by the model. As such, $y_i-x’_i\beta$ can be interpreted as the prediction error, also 
referred to as model residual. To quantify the total error across all individuals, we sum 
the square residuals $( y_i-x’_i\beta)^2$ across all the individuals in our data.</p>

<p>Finding the value of $\beta$ that minimises
the sum above is the line of best fit through our data when considering 
this goal of minimising the sum of squared error. However, it is not the only 
possible line we could use! For example, we might want to err on the side of
caution when estimating effect sizes (coefficients). That is, we might want to 
avoid estimating very large effect sizes. This can help us to create <em>generalisable</em>
models. This is important when models that are fitted (trained) on one dataset
and then used to predict outcomes from a new dataset. Restricting parameter
estimates is particularly important when analysing high-dimensional data.</p>

<blockquote class="challenge">
  <h2 id="challenge-2">Challenge 2</h2>

  <p>Discuss in groups:</p>

  <ol>
    <li>What are we minimising when we fit a linear model?</li>
    <li>Why are we minimising this objective? What assumptions are we making
about our data when we do so?</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution-1">Solution</h2>

    <ol>
      <li>When we fit a linear model we are minimising the squared error.
In fact, the standard linear model estimator is often known as
“ordinary least squares”. The “ordinary” really means “original” here,
to distinguish between this method, which dates back to ~1800, and some
more “recent” (think 1940s…) methods.</li>
      <li>Least squares assumes that, when we account for the change in the mean of
the outcome based on changes in the income, the data are normally
distributed. That is, the <em>residuals</em> of the model, or the error 
left over after we account for any linear relationships in the data,
are normally distributed, and have a fixed variance.</li>
    </ol>

  </blockquote>
</blockquote>

<h1 id="model-selection-using-training-and-test-sets">Model selection using training and test sets</h1>

<p>Sets of models are often compared using statistics such as adjusted $R^2$, AIC or BIC.
These show us how well the model is learning the data used in fitting that same model <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.
However, these statistics do not really tell us how well the model will generalise to new data.
This is an important thing to  consider – if our model doesn’t generalise to new data,
then there’s a chance that it’s just picking up on a technical or batch effect
in our data, or simply some noise that happens to fit the outcome we’re modelling.
This is especially important when our goal is prediction – it’s not much good
if we can only predict well for samples where the outcome is already known,
after all!</p>

<p>To get an idea of how well our model generalises, we can split the data into
two - a “training” and a “test” set. We use the “training” data to fit the model,
and then see its performance on the “test” data.</p>

<div class="figure" style="text-align: center">
<img src="../fig/validation.png" alt="Schematic representation of how a dataset can be divided into a training (the portion of the data used to fit a model) and a test set (the portion of the data used to assess external generalisability)." width="500px" />
<p class="caption">Schematic representation of how a dataset can be divided into a training and a test set.</p>
</div>

<p>One thing that often happens in this context is that large 
coefficient values minimise the training error, but they don’t minimise the 
test error on unseen data. First, we’ll go through an example of what exactly 
this means.</p>

<p>For the next few challenges, we’ll work with a set of features
known to be associated with age from a paper by Horvath et al.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Horvath et al
use methylation markers alone to predict the biological age of an individual.
This is useful in studying age-related disease amongst many other things.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">coef_horvath</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readRDS</span><span class="p">(</span><span class="n">here</span><span class="o">::</span><span class="n">here</span><span class="p">(</span><span class="s2">"data/coefHorvath.rds"</span><span class="p">))</span><span class="w">
</span><span class="n">methylation</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readRDS</span><span class="p">(</span><span class="n">here</span><span class="o">::</span><span class="n">here</span><span class="p">(</span><span class="s2">"data/methylation.rds"</span><span class="p">))</span><span class="w">

</span><span class="n">library</span><span class="p">(</span><span class="s2">"SummarizedExperiment"</span><span class="p">)</span><span class="w">
</span><span class="n">age</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">methylation</span><span class="o">$</span><span class="n">Age</span><span class="w">
</span><span class="n">methyl_mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">assay</span><span class="p">(</span><span class="n">methylation</span><span class="p">))</span><span class="w">

</span><span class="n">coef_horvath</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">coef_horvath</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">
</span><span class="n">features</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">coef_horvath</span><span class="o">$</span><span class="n">CpGmarker</span><span class="w">
</span><span class="n">horvath_mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">methyl_mat</span><span class="p">[,</span><span class="w"> </span><span class="n">features</span><span class="p">]</span><span class="w">

</span><span class="c1">## Generate an index to split the data</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">train_ind</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">),</span><span class="w"> </span><span class="m">25</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<blockquote class="challenge">
  <h2 id="challenge-3">Challenge 3</h2>

  <ol>
    <li>Split the methylation data matrix and the age vector
into training and test sets.</li>
    <li>Fit a model on the training data matrix and training age 
vector.</li>
    <li>Check the mean squared error on this model.</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution-2">Solution</h2>

    <ol>
      <li>
        <p>Splitting the data involves using our index to split up the matrix and
the age vector into two each. We can use a negative subscript to create
the test data.</p>

        <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">horvath_mat</span><span class="p">[</span><span class="n">train_ind</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">
</span><span class="n">train_age</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">age</span><span class="p">[</span><span class="n">train_ind</span><span class="p">]</span><span class="w">
</span><span class="n">test_mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">horvath_mat</span><span class="p">[</span><span class="o">-</span><span class="n">train_ind</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">
</span><span class="n">test_age</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">age</span><span class="p">[</span><span class="o">-</span><span class="n">train_ind</span><span class="p">]</span><span class="w">
</span></code></pre></div>        </div>
        <p>The solution to this exercise is important because the generated objects 
(<code class="language-plaintext highlighter-rouge">train_mat</code>, <code class="language-plaintext highlighter-rouge">train_age</code>, <code class="language-plaintext highlighter-rouge">test_mat</code> and <code class="language-plaintext highlighter-rouge">test_age</code>) will be used later in 
this episode. Please make sure that you use the same object names.</p>
      </li>
      <li>
        <p>To</p>

        <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># as.data.frame() converts train_mat into a data.frame</span><span class="w">
</span><span class="n">fit_horvath</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">train_age</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">as.data.frame</span><span class="p">(</span><span class="n">train_mat</span><span class="p">))</span><span class="w">
</span></code></pre></div>        </div>
      </li>
    </ol>

    <p>Using the <code class="language-plaintext highlighter-rouge">.</code> syntax above together with a <code class="language-plaintext highlighter-rouge">data</code> argument will lead to
the same result as usign <code class="language-plaintext highlighter-rouge">train_age ~ tran_mat</code>: R will fit a multivariate 
regression model in which each of the colums in <code class="language-plaintext highlighter-rouge">train_mat</code> is used as 
a predictor. We opted to use the <code class="language-plaintext highlighter-rouge">.</code> syntax because it will help us to 
obtain model predictions using the <code class="language-plaintext highlighter-rouge">predict()</code> function.</p>

    <ol>
      <li>
        <p>The mean squared error of the model is the mean of the square of the
residuals. This seems very low here – on average we’re only off by 
about a year!</p>

        <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mean</span><span class="p">(</span><span class="n">residuals</span><span class="p">(</span><span class="n">fit_horvath</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
</span></code></pre></div>        </div>

        <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 1.319628
</code></pre></div>        </div>
      </li>
    </ol>

  </blockquote>
</blockquote>

<p>Having trained this model, now we can check how well it does in predicting age
from new dataset (the test data).
Here we use the mean of the squared difference between our predictions and the
true ages for the test data, or “mean squared error” (MSE). Unfortunately, it
seems like this is a lot higher than the error on the training data!</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mse</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">true</span><span class="p">,</span><span class="w"> </span><span class="n">prediction</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">mean</span><span class="p">((</span><span class="n">true</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">prediction</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="n">pred_lm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">fit_horvath</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">as.data.frame</span><span class="p">(</span><span class="n">test_mat</span><span class="p">))</span><span class="w">
</span><span class="n">err_lm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mse</span><span class="p">(</span><span class="n">test_age</span><span class="p">,</span><span class="w"> </span><span class="n">pred_lm</span><span class="p">)</span><span class="w">
</span><span class="n">err_lm</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 223.3571
</code></pre></div></div>

<p>Further, if we plot true age against predicted age for the samples in the test
set, we can see how well we’re really doing - ideally these would line up
exactly!</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">test_age</span><span class="p">,</span><span class="w"> </span><span class="n">pred_lm</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">19</span><span class="p">)</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="n">coef</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="o">:</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-03-test-plot-lm-1.png" alt="A scatter plot of observed age versus predicted age for individuals in the test set. Each dot represents one individual. Dashed line is used as a reference to indicate how perfect predictions would look (observed = predicted). In this case we observe high prediction error in the test set." width="432" />
<p class="caption">A scatter plot of observed age versus predicted age for individuals in the test set. Each dot represents one individual. Dashed line is used as a reference to indicate how perfect predictions would look (observed = predicted).</p>
</div>

<p>This figure shows the predicted ages obtained from a linear model fit plotted 
against the true ages, which we kept in the test dataset. If the prediction were 
good, the dots should follow a line. Regularisation can help us to make the 
model more generalisable, improving predictions for the test dataset (or any 
other dataset that is not used when fitting our model).</p>

<h1 id="using-regularisation-to-impove-generalisability">Using regularisation to impove generalisability</h1>

<p>As stated above, restricting model parameter estimates can improve a model’s
generalisability. This can be done with <em>regularisation</em>. The idea to add another
condition to the problem we’re solving with linear regression. This condition
controls the total size of the  coefficients that come out. 
For example, we might say that the point representing the slope and intercept
must fall within a certain distance of the origin, $(0, 0)$. Note that we are 
still trying to solve for the line that minimises the square of the residuals; 
we are just adding this extra constraint to our solution.</p>

<p>For the 2-parameter model (slope and intercept), we could
visualise this constraint as a circle with a given radius. We 
want to find the “best” solution (in terms of minimising the 
residuals) that also falls within a circle of a given radius 
(in this case, 2).</p>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-03-ridgeplot-1.png" alt="For each observation, the left panel shows the residuals with respect to the optimal lines obtained with and without regularisation. Right panel shows the sum of squared residuals across all possible linear regression models. Regularisation moves the line away from the optimal (in terms of minimising the sum of squared residuals). " width="720" />
<p class="caption">Illustrative example demonstrated how regression coefficients are inferred under a linear model framework, with (blue line) and without (red line) regularisation. A ridge penalty is used in this example</p>
</div>

<p>There are multiple ways to define the distance that our solution must fall in,
though. The one we’ve plotted above controls the squared sum of the 
coefficients, $\beta$.
This is also sometimes called the $L^2$ norm. This is defined as</p>

<p>[\left\lVert \beta\right\lVert_2 = \sqrt{\sum_{j=1}^p \beta_j^2}]</p>

<p>To control this, we specify that the solution for the equation above
also has to have an $L^2$ norm smaller than a certain amount. Or, equivalently,
we try to minimise a function that includes our $L^2$ norm scaled by a 
factor that is usually written $\lambda$.</p>

<p>[\sum_{i=1}^N \biggl( y_i - x’_i\beta\biggr)^2  + \lambda \left\lVert \beta \right\lVert_2 ^2]</p>

<p>Another way of thinking about this is that when finding the best model, we’re
weighing up a balance of the ordinary least squares objective and a “penalty”
term that punished models with large coefficients. The balance between the
penalty and the ordinary least squares objective is controlled by $\lambda$ - 
when $\lambda$ is large, we care a lot about the size of the coefficients.
When it’s small, we don’t really care a lot. When it’s zero, we’re back to
just using ordinary least squares. This type of regularisation is called <em>ridge regression</em>.</p>

<h1 id="why-would-we-want-to-restrict-our-model">Why would we want to restrict our model?</h1>

<p>It may seem an odd thing to do: to restrict the possible values of our model
parameters! Why would we want to do this? Firstly, as discussed earlier, our 
model estimates can be very unstable or even difficult to calculate when we have 
many correlated features. Secondly, this type of approach can make our model more 
generalisable to new data. To show this, we’ll fit a model using the same set
of 20 features (stored as <code class="language-plaintext highlighter-rouge">features</code>) selected earlier in this episode (these
are a subset of the features identified by Horvarth et al), using both 
regularised and ordinary least squares.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"glmnet"</span><span class="p">)</span><span class="w">

</span><span class="c1">## glmnet() performs scaling by default, supply un-scaled data:</span><span class="w">
</span><span class="n">horvath_mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">methyl_mat</span><span class="p">[,</span><span class="w"> </span><span class="n">features</span><span class="p">]</span><span class="w"> </span><span class="c1"># select the first 20 sites as before</span><span class="w">
</span><span class="n">train_mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">horvath_mat</span><span class="p">[</span><span class="n">train_ind</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w"> </span><span class="c1"># use the same individuals as selected before</span><span class="w">
</span><span class="n">test_mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">horvath_mat</span><span class="p">[</span><span class="o">-</span><span class="n">train_ind</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">



</span><span class="n">ridge_fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glmnet</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train_mat</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train_age</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">ridge_fit</span><span class="p">,</span><span class="w"> </span><span class="n">xvar</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"lambda"</span><span class="p">)</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-03-plot-ridge-1.png" alt="Alt" width="432" />
<p class="caption">Cap</p>
</div>

<p>This plot shows how the estimated coefficients for each CpG site change
as we increase the penalty, $\lambda$. That is,
as we decrease the size of the region that solutions can fall into, the values
of the coefficients that we get back tend to decrease. In this case,
coefficients trend towards zero but generally don’t reach it until the penalty
gets very large. We can see that initially, some parameter estimates are really,
really large, and these tend to shrink fairly rapidly.</p>

<p>We can also notice that some parameters “flip signs”; that is, they start off
positive and become negative as lambda grows. This is a sign of collinearity,
or correlated predictors. As we reduce the importance of one feature, we can 
“make up for” the loss in accuracy from that one feature by adding a bit of
weight to another feature that represents similar information.</p>

<p>Since we split the data into test and training data, we can prove that ridge
regression gives us a better prediction in this case:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pred_ridge</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">ridge_fit</span><span class="p">,</span><span class="w"> </span><span class="n">newx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">test_mat</span><span class="p">)</span><span class="w">
</span><span class="n">err_ridge</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">apply</span><span class="p">(</span><span class="n">pred_ridge</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">col</span><span class="p">)</span><span class="w"> </span><span class="n">mse</span><span class="p">(</span><span class="n">test_age</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="p">))</span><span class="w">
</span><span class="nf">min</span><span class="p">(</span><span class="n">err_ridge</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 46.76802
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">err_lm</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 223.3571
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">which_min_err</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">which.min</span><span class="p">(</span><span class="n">err_ridge</span><span class="p">)</span><span class="w">
</span><span class="n">min_err_ridge</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">err_ridge</span><span class="p">)</span><span class="w">
</span><span class="n">pred_min_ridge</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pred_ridge</span><span class="p">[,</span><span class="w"> </span><span class="n">which_min_err</span><span class="p">]</span><span class="w">
</span></code></pre></div></div>

<p>We can see where on the continuum of lambdas we’ve picked a model by plotting
the coefficient paths again. In this case, we’ve picked a model with fairly
modest shrinkage.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chosen_lambda</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ridge_fit</span><span class="o">$</span><span class="n">lambda</span><span class="p">[</span><span class="n">which.min</span><span class="p">(</span><span class="n">err_ridge</span><span class="p">)]</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">ridge_fit</span><span class="p">,</span><span class="w"> </span><span class="n">xvar</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"lambda"</span><span class="p">)</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="n">chosen_lambda</span><span class="p">),</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-03-chooselambda-1.png" alt="Alt" width="432" />
<p class="caption">Cap</p>
</div>

<blockquote class="challenge">
  <h2 id="challenge-4">Challenge 4</h2>

  <ol>
    <li>Which performs better, ridge or OLS?</li>
    <li>Plot predicted ages for each method against the true ages.
How do the predictions look for both methods? Why might ridge be 
performing better?</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution-3">Solution</h2>

    <ol>
      <li>
        <p>Ridge regression performs significantly better on unseen data, despite
being “worse” on the training data.</p>

        <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">min_err_ridge</span><span class="w">
</span></code></pre></div>        </div>

        <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 46.76802
</code></pre></div>        </div>

        <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">err_lm</span><span class="w">
</span></code></pre></div>        </div>

        <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 223.3571
</code></pre></div>        </div>
      </li>
      <li>
        <p>The ridge ones are much less spread out with far fewer extreme predictions.</p>

        <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">all</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">pred_lm</span><span class="p">,</span><span class="w"> </span><span class="n">test_age</span><span class="p">,</span><span class="w"> </span><span class="n">pred_min_ridge</span><span class="p">)</span><span class="w">
</span><span class="n">lims</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">range</span><span class="p">(</span><span class="n">all</span><span class="p">)</span><span class="w">
</span><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">test_age</span><span class="p">,</span><span class="w"> </span><span class="n">pred_lm</span><span class="p">,</span><span class="w">
    </span><span class="n">xlim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lims</span><span class="p">,</span><span class="w"> </span><span class="n">ylim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lims</span><span class="p">,</span><span class="w">
    </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">19</span><span class="w">
</span><span class="p">)</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="n">coef</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="o">:</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">test_age</span><span class="p">,</span><span class="w"> </span><span class="n">pred_min_ridge</span><span class="p">,</span><span class="w">
    </span><span class="n">xlim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lims</span><span class="p">,</span><span class="w"> </span><span class="n">ylim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lims</span><span class="p">,</span><span class="w">
    </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">19</span><span class="w">
</span><span class="p">)</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="n">coef</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="o">:</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">)</span><span class="w">
</span></code></pre></div>        </div>

        <div class="figure" style="text-align: center">
<img src="../fig/rmd-03-plot-ridge-prediction-1.png" alt="Alt" width="720" />
<p class="caption">Cap</p>
</div>
      </li>
    </ol>
  </blockquote>
</blockquote>

<h1 id="lasso-regression">LASSO regression</h1>

<p><em>LASSO</em> is another type of regularisation. In this case we use the $L^1$ norm,
or the sum of the absolute values of the coefficients.</p>

<table>
  <tbody>
    <tr>
      <td>[\left\lVert \beta \right\lVert_1 = \sum_{j=1}^p</td>
      <td>\beta_j</td>
      <td>]</td>
    </tr>
  </tbody>
</table>

<p>This tends to produce sparse models; that is to say, it tends to remove features
from the model that aren’t necessary to produce accurate predictions. This
is because the region we’re restricting the coefficients to has sharp edges.
So, when we increase the penalty (reduce the norm), it’s more likely that
the best solution that falls in this region will be at the corner of this
diagonal (i.e., one or more coefficient is exactly zero).</p>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-03-shrink-lasso-1.png" alt="For each observation, the left panel shows the residuals with respect to the optimal lines obtained with and without regularisation. Right panel shows the sum of squared residuals across all possible linear regression models. Regularisation moves the line away from the optimal (in terms of minimising the sum of squared residuals)" width="720" />
<p class="caption">Illustrative example demonstrated how regression coefficients are inferred under a linear model framework, with (blue line) and without (red line) regularisation. A LASSO penalty is used in this example.</p>
</div>

<blockquote class="challenge">
  <h2 id="challenge-5">Challenge 5</h2>

  <ol>
    <li>Use <code class="language-plaintext highlighter-rouge">glmnet</code> to fit a LASSO model (hint: set <code class="language-plaintext highlighter-rouge">alpha = 1</code>).</li>
    <li>Plot the model object. Remember that for ridge regression,
we set <code class="language-plaintext highlighter-rouge">xvar = "lambda"</code>. What if you don’t set this? What’s the
relationship between the two plots?</li>
    <li>How do the coefficient paths differ to the ridge case?</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution-4">Solution</h2>

    <ol>
      <li>
        <p>Fitting a LASSO model is very similar to a ridge model, we just need
to change the <code class="language-plaintext highlighter-rouge">alpha</code> setting.</p>

        <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fit_lasso</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glmnet</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">methyl_mat</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span></code></pre></div>        </div>
      </li>
      <li>When <code class="language-plaintext highlighter-rouge">xvar = "lambda"</code>, the x-axis represents increasing model sparsity
from left to right, because increasing lambda increases the penalty added
to the coefficients. When we instead plot the L1 norm on the x-axis,
increasing L1 norm means that we are allowing our
coefficients to take on increasingly large values.
        <div class="figure" style="text-align: center">
<img src="../fig/rmd-03-plotlas-1.png" alt="plot of chunk plotlas" width="720" />
<p class="caption">plot of chunk plotlas</p>
</div>
      </li>
      <li>The paths tend to go to exactly zero much more when sparsity increases when we use a LASSO model. 
In the ridge case, the paths tend towards zero but less commonly reach exactly zero.</li>
    </ol>

  </blockquote>
</blockquote>

<h1 id="cross-validation-to-find-the-best-value-of-lambda">Cross-validation to find the best value of $\lambda$</h1>

<p>There are various methods to select the “best”
value for $\lambda$. One is to split
the data into $K$ chunks. We then use $K-1$ of
these as the training set, and the remaining $1$ chunk
as the test set. We can repeat this until we’ve rotated through all $K$ chunks,
giving us a good estimate of how well each of the lambda values work in our
data. This is called cross-validation, and doing this repeated test/train split
gives us a better estimate of how generalisable our model is. Cross-validation
is a really deep topic that we’re not going to cover in more detail today, though!</p>

<div class="figure" style="text-align: center">
<img src="../fig/cross_validation.png" alt="The data is divided into $K$ chunks. For each cross-validation iteration, one data chunk is used as the test set. The remaining $K-1$ chunks are combined into a training set." />
<p class="caption">Schematic representiation of a $K$-fold cross-validation procedure.</p>
</div>

<p>We can use this new idea to choose a lambda value, by finding the lambda
that minimises the error across each of the test and training splits.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lasso</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.glmnet</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">[,</span><span class="w"> </span><span class="m">-1</span><span class="p">],</span><span class="w"> </span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">lasso</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-03-lasso-cv-1.png" alt="Alt" width="432" />
<p class="caption">Cross-validated mean squared error for different values of lambda under a LASSO penalty.</p>
</div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">coefl</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">coef</span><span class="p">(</span><span class="n">lasso</span><span class="p">,</span><span class="w"> </span><span class="n">lasso</span><span class="o">$</span><span class="n">lambda.min</span><span class="p">)</span><span class="w">
</span><span class="n">selected_coefs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.matrix</span><span class="p">(</span><span class="n">coefl</span><span class="p">)[</span><span class="n">which</span><span class="p">(</span><span class="n">coefl</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="m">0</span><span class="p">),</span><span class="w"> </span><span class="m">1</span><span class="p">]</span><span class="w">

</span><span class="c1">## load the horvath signature to compare features</span><span class="w">
</span><span class="n">coef_horvath</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readRDS</span><span class="p">(</span><span class="n">here</span><span class="o">::</span><span class="n">here</span><span class="p">(</span><span class="s2">"data/coefHorvath.rds"</span><span class="p">))</span><span class="w">
</span><span class="c1">## We select some of the same features! Hooray</span><span class="w">
</span><span class="n">intersect</span><span class="p">(</span><span class="nf">names</span><span class="p">(</span><span class="n">selected_coefs</span><span class="p">),</span><span class="w"> </span><span class="n">coef_horvath</span><span class="o">$</span><span class="n">CpGmarker</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] "cg02388150" "cg06493994" "cg22449114" "cg22736354" "cg03330058"
[6] "cg09809672" "cg11299964" "cg19761273" "cg26162695"
</code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-03-heatmap-lasso-1.png" alt="Overall, we observe either increasing or decreasing methylation patterns as a function of age." width="432" />
<p class="caption">Heatmap showing methylation values for the selected CpG and how the vary with age.</p>
</div>

<h1 id="blending-ridge-regression-and-the-lasso---elastic-nets">Blending ridge regression and the LASSO - elastic nets</h1>

<p>So far, we’ve used ridge regression, where <code class="language-plaintext highlighter-rouge">alpha = 0</code>, and LASSO regression,
where <code class="language-plaintext highlighter-rouge">alpha = 1</code>. What if <code class="language-plaintext highlighter-rouge">alpha</code> is set to a value between zero and one?
Well, this actually lets us blend the properties of ridge and LASSO
regression. This allows us to have the nice properties of the LASSO, where
uninformative variables are dropped automatically, and the nice properties
of ridge regression, where our coefficient estimates are a bit more
conservative, and as a result our predictions are a bit better.</p>

<p>Formally, the objective function of elastic net regression is to optimise the
following function:</p>

<p>[\left(\sum_{i=1}^N y_i - x’_i\beta\right)
        + \lambda \left(
            \alpha \left\lVert \beta \right\lVert_1 +
            (1-\alpha)  \left\lVert \beta \right\lVert_2
        \right)]</p>

<p>You can see that if <code class="language-plaintext highlighter-rouge">alpha = 1</code>, then the L1 norm term is multiplied by one,
and the L2 norm is multiplied by zero. This means we have pure LASSO regression.
Conversely, if <code class="language-plaintext highlighter-rouge">alpha = 0</code>, the L2 norm term is multiplied by one, and the L1
norm is multiplied by zero, meaning we have pure ridge regression. Anything
in between gives us something in between.</p>

<p>The contour plots we looked at previously to visualise what this penalty looks
like for different values of <code class="language-plaintext highlighter-rouge">alpha</code>.</p>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-03-elastic-contour-1.png" alt="For lower values of alpha, the penalty resembles ridge regression. For higher values of alpha, the penalty resembles LASSO regression." width="1152" />
<p class="caption">For an elastic net, the panels show the effect of the regularisation across different values of alpha</p>
</div>

<blockquote class="challenge">
  <h2 id="challenge-6">Challenge 6</h2>

  <ol>
    <li>Fit an elastic net model (hint: alpha = 0.5) without cross-validation and plot the model
object.</li>
    <li>Fit an elastic net model with cross-validation and plot the error. Compare
with LASSO.</li>
    <li>Select the lambda within one standard error of 
the minimum cross-validation error (hint: <code class="language-plaintext highlighter-rouge">lambda.1se</code>). Compare the
coefficients with the LASSO model.</li>
    <li>Discuss: how could we pick an <code class="language-plaintext highlighter-rouge">alpha</code> in the range (0, 1)? Could we justify
choosing one <em>a priori</em>?</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution-5">Solution</h2>
    <ol>
      <li>
        <p>Fitting an elastic net model is just like fitting a LASSO model.
You can see that coefficients tend to go exactly to zero,
but the paths are a bit less
extreme than with pure LASSO; similar to ridge.</p>

        <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">elastic</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glmnet</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">[,</span><span class="w"> </span><span class="m">-1</span><span class="p">],</span><span class="w"> </span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">elastic</span><span class="p">)</span><span class="w">
</span></code></pre></div>        </div>

        <div class="figure" style="text-align: center">
<img src="../fig/rmd-03-elastic-1.png" alt="plot of chunk elastic" width="432" />
<p class="caption">plot of chunk elastic</p>
</div>
      </li>
      <li>
        <p>The process of model selection is similar for elastic net models as for
LASSO models.</p>

        <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">elastic_cv</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.glmnet</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">[,</span><span class="w"> </span><span class="m">-1</span><span class="p">],</span><span class="w"> </span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">elastic_cv</span><span class="p">)</span><span class="w">
</span></code></pre></div>        </div>

        <div class="figure" style="text-align: center">
<img src="../fig/rmd-03-elastic-cv-1.png" alt="Alt" width="432" />
<p class="caption">Elastic</p>
</div>
      </li>
      <li>
        <p>You can see that the coefficients from these two methods are broadly
similar, but the elastic net coefficients are a bit more conservative.
Further, more coefficients are exactly zero in the LASSO model.</p>

        <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">coefe</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">coef</span><span class="p">(</span><span class="n">elastic_cv</span><span class="p">,</span><span class="w"> </span><span class="n">elastic_cv</span><span class="o">$</span><span class="n">lambda.1se</span><span class="p">)</span><span class="w">
</span><span class="nf">sum</span><span class="p">(</span><span class="n">coefe</span><span class="p">[,</span><span class="w"> </span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
</span></code></pre></div>        </div>

        <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 4973
</code></pre></div>        </div>

        <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">sum</span><span class="p">(</span><span class="n">coefl</span><span class="p">[,</span><span class="w"> </span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
</span></code></pre></div>        </div>

        <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 4955
</code></pre></div>        </div>

        <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="w">
    </span><span class="n">coefl</span><span class="p">[,</span><span class="w"> </span><span class="m">1</span><span class="p">],</span><span class="w"> </span><span class="n">coefe</span><span class="p">[,</span><span class="w"> </span><span class="m">1</span><span class="p">],</span><span class="w">
    </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">16</span><span class="p">,</span><span class="w">
    </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"LASSO coefficients"</span><span class="p">,</span><span class="w">
    </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Elastic net coefficients"</span><span class="w">
</span><span class="p">)</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="m">0</span><span class="o">:</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">)</span><span class="w">
</span></code></pre></div>        </div>

        <div class="figure" style="text-align: center">
<img src="../fig/rmd-03-elastic-plot-1.png" alt="Alt" width="432" />
<p class="caption">LASSO-Elastic</p>
</div>
      </li>
      <li>
        <p>You could pick an arbitrary value of <code class="language-plaintext highlighter-rouge">alpha</code>, because arguably pure ridge
regression or pure LASSO regression are also arbitrary model choices.
To be rigorous and to get the best-performing model and the best 
inference about predictors, it’s usually best to find the best
combination of <code class="language-plaintext highlighter-rouge">alpha</code> and <code class="language-plaintext highlighter-rouge">lambda</code> using a grid search approach
in cross-validation. However, this can be very computationally demanding.</p>
      </li>
    </ol>
  </blockquote>
</blockquote>

<blockquote class="callout">
  <h2 id="the-bias-variance-tradeoff">The bias-variance tradeoff</h2>

  <p>When we make predictions in statistics, there are two sources of error
that primarily influence the (in)accuracy of our predictions. these are <em>bias</em>
and <em>variance</em>.</p>

  <p>The total expected error in our predictions is given by the following
equation:</p>

\[E(y - \hat{y}) = \text{Bias}^2 + \text{Variance} + \sigma^2\]

  <p>Here, $\sigma^2$ represents the irreducible error, that we can never overcome.
Bias results from erroneous assumptions in the model used for predictions.
Fundamentally, bias means that our model is mis-specified in some way,
and fails to capture some components of the data-generating process 
(which is true of all models). If we have failed to account for a confounding
factor that leads to very inaccurate predictions in a subgroup of our
population, then our model has high bias.</p>

  <p>Variance results from sensitivity to particular properties of the input data.
For example, if a tiny change to the input data would result in a huge change
to our predictions, then our model has high variance.</p>

  <p>Linear regression is an unbiased model under certain conditions.
In fact, the <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">Gauss-Markov theorem</a>
shows that under the right conditions, OLS is the best possible type of
unbiased linear model.</p>

  <p>Introducing penalties to means that our model is no longer unbiased, meaning
that the coefficients estimated from our data will systematically deviate
from the ground truth. Why would we do this? As we saw, the total error is
a function of bias and variance. By accepting a small amount of bias, it’s
possible to achieve huge reductions in the total expected error.</p>

  <p>This bias-variance tradeoff is also why people often favour elastic net
regression over pure LASSO regression.</p>

</blockquote>

<blockquote class="callout">
  <h2 id="other-types-of-outcomes">Other types of outcomes</h2>

  <p>You may have noticed that <code class="language-plaintext highlighter-rouge">glmnet</code> is written as <code class="language-plaintext highlighter-rouge">glm</code>, not <code class="language-plaintext highlighter-rouge">lm</code>.
This means we can actually model a variety of different outcomes
using this regularisation approach. For example, we can model binary
variables using logistic regression, as shown below. The type of outcome
can be specified using the <code class="language-plaintext highlighter-rouge">family</code> argument, which specifies the family
of the outcome variable.</p>

  <p>In fact, <code class="language-plaintext highlighter-rouge">glmnet</code> is somewhat cheeky as it also allows you to model
survival using Cox proportional hazards models, which aren’t GLMs, strictly
speaking.</p>

  <p>For example, in the current dataset we can model smoking status as a binary
variable in logistic regression by setting <code class="language-plaintext highlighter-rouge">family = "binomial"</code>.</p>

  <p>The <a href="https://glmnet.stanford.edu/articles/glmnet.html">package documentation</a>
explains this in more detail.</p>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">smoking</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">as.numeric</span><span class="p">(</span><span class="n">factor</span><span class="p">(</span><span class="n">methylation</span><span class="o">$</span><span class="n">smoker</span><span class="p">))</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="c1"># binary outcome</span><span class="w">
</span><span class="n">table</span><span class="p">(</span><span class="n">smoking</span><span class="p">)</span><span class="w">
</span></code></pre></div>  </div>

  <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>smoking
 0  1 
30  7 
</code></pre></div>  </div>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.glmnet</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">methyl_mat</span><span class="p">,</span><span class="w"> </span><span class="n">nfolds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">smoking</span><span class="p">,</span><span class="w"> </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"binomial"</span><span class="p">)</span><span class="w">
</span></code></pre></div>  </div>

  <div class="language-plaintext warning highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
</code></pre></div>  </div>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">coef</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">coef</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fit</span><span class="o">$</span><span class="n">lambda.min</span><span class="p">)</span><span class="w">
</span><span class="n">coef</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.matrix</span><span class="p">(</span><span class="n">coef</span><span class="p">)</span><span class="w">
</span><span class="n">coef</span><span class="p">[</span><span class="n">which</span><span class="p">(</span><span class="n">coef</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="m">0</span><span class="p">),</span><span class="w"> </span><span class="m">1</span><span class="p">]</span><span class="w">
</span></code></pre></div>  </div>

  <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] -1.455287
</code></pre></div>  </div>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div>  </div>

  <div class="figure" style="text-align: center">
<img src="../fig/rmd-03-binomial-1.png" alt="Alt" width="432" />
<p class="caption">Title</p>
</div>
  <p>In this case, the results aren’t very interesting! We select an intercept-only
model. However, as highlighted by the warnings above, we should not trust this
result too much as the data was too small to obtain reliable results! We only 
included it here to provide the code that <em>could</em> be used to perform penalised
regression for binary outcomes (i.e. penalised logistic regression).</p>
</blockquote>

<blockquote class="callout">
  <h2 id="tidymodels">tidymodels</h2>

  <p>A lot of the packages for fitting predictive models like regularised
regression have different user interfaces. To do predictive modelling, it’s
important to consider things like choosing a good performance metric and 
how to run normalisation. It’s also useful to compare different 
model “engines”.</p>

  <p>To this end, the <strong><code class="language-plaintext highlighter-rouge">tidymodels</code></strong> R framework exists. We’re not doing a course on 
advanced topics in predictive modelling so we are not covering this framework 
in detail. However, the code below would be useful to perform repeated 
cross-validation. More information about <strong><code class="language-plaintext highlighter-rouge">tidymodels</code></strong>, including installation 
instructions, can be found <a href="https://www.rdocumentation.org/packages/tidymodels/versions/1.1.1">here</a>.</p>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"tidymodels"</span><span class="p">)</span><span class="w">
</span><span class="n">all_data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.data.frame</span><span class="p">(</span><span class="n">cbind</span><span class="p">(</span><span class="n">age</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">methyl_mat</span><span class="p">))</span><span class="w">
</span><span class="n">split_data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">initial_split</span><span class="p">(</span><span class="n">all_data</span><span class="p">)</span><span class="w">

</span><span class="n">norm_recipe</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">recipe</span><span class="p">(</span><span class="n">training</span><span class="p">(</span><span class="n">split_data</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
    </span><span class="c1">## everything other than age is a predictor</span><span class="w">
    </span><span class="n">update_role</span><span class="p">(</span><span class="n">everything</span><span class="p">(),</span><span class="w"> </span><span class="n">new_role</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"predictor"</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
    </span><span class="n">update_role</span><span class="p">(</span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">new_role</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"outcome"</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
    </span><span class="c1">## center and scale all the predictors</span><span class="w">
    </span><span class="n">step_center</span><span class="p">(</span><span class="n">all_predictors</span><span class="p">())</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
    </span><span class="n">step_scale</span><span class="p">(</span><span class="n">all_predictors</span><span class="p">())</span><span class="w"> 

</span><span class="c1">## set the "engine" to be a linear model with tunable alpha and lambda</span><span class="w">
</span><span class="n">glmnet_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">linear_reg</span><span class="p">(</span><span class="n">penalty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tune</span><span class="p">(),</span><span class="w"> </span><span class="n">mixture</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tune</span><span class="p">())</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> 
    </span><span class="n">set_engine</span><span class="p">(</span><span class="s2">"glmnet"</span><span class="p">)</span><span class="w">

</span><span class="c1">## define a workflow, with normalisation recipe into glmnet engine</span><span class="w">
</span><span class="n">workflow</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">workflow</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
    </span><span class="n">add_recipe</span><span class="p">(</span><span class="n">norm_recipe</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
    </span><span class="n">add_model</span><span class="p">(</span><span class="n">glmnet_model</span><span class="p">)</span><span class="w">

</span><span class="c1">## 5-fold cross-validation repeated 5 times</span><span class="w">
</span><span class="n">folds</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vfold_cv</span><span class="p">(</span><span class="n">training</span><span class="p">(</span><span class="n">split_data</span><span class="p">),</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">repeats</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w">

</span><span class="c1">## define a grid of lambda and alpha parameters to search</span><span class="w">
</span><span class="n">glmn_set</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">parameters</span><span class="p">(</span><span class="w">
    </span><span class="n">penalty</span><span class="p">(</span><span class="n">range</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">-5</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">trans</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">log10_trans</span><span class="p">()),</span><span class="w">
    </span><span class="n">mixture</span><span class="p">()</span><span class="w">
</span><span class="p">)</span><span class="w">
</span><span class="n">glmn_grid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">grid_regular</span><span class="p">(</span><span class="n">glmn_set</span><span class="p">)</span><span class="w">
</span><span class="n">ctrl</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">control_grid</span><span class="p">(</span><span class="n">save_pred</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">verbose</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">

</span><span class="c1">## use the metric "rmse" (root mean squared error) to grid search for the</span><span class="w">
</span><span class="c1">## best model</span><span class="w">
</span><span class="n">results</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">workflow</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
    </span><span class="n">tune_grid</span><span class="p">(</span><span class="w">
        </span><span class="n">resamples</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">folds</span><span class="p">,</span><span class="w">
        </span><span class="n">metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric_set</span><span class="p">(</span><span class="n">rmse</span><span class="p">),</span><span class="w">
        </span><span class="n">control</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctrl</span><span class="w">
    </span><span class="p">)</span><span class="w">
</span><span class="c1">## select the best model based on RMSE</span><span class="w">
</span><span class="n">best_mod</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">results</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">select_best</span><span class="p">(</span><span class="s2">"rmse"</span><span class="p">)</span><span class="w">
</span><span class="n">best_mod</span><span class="w">
</span><span class="c1">## finalise the workflow and fit it with all of the training data</span><span class="w">
</span><span class="n">final_workflow</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">finalize_workflow</span><span class="p">(</span><span class="n">workflow</span><span class="p">,</span><span class="w"> </span><span class="n">best_mod</span><span class="p">)</span><span class="w">
</span><span class="n">final_workflow</span><span class="w">
</span><span class="n">final_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">final_workflow</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
    </span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">training</span><span class="p">(</span><span class="n">split_data</span><span class="p">))</span><span class="w">

</span><span class="c1">## plot predicted age against true age for test data</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="w">
    </span><span class="n">testing</span><span class="p">(</span><span class="n">split_data</span><span class="p">)</span><span class="o">$</span><span class="n">age</span><span class="p">,</span><span class="w">
    </span><span class="n">predict</span><span class="p">(</span><span class="n">final_model</span><span class="p">,</span><span class="w"> </span><span class="n">new_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">testing</span><span class="p">(</span><span class="n">split_data</span><span class="p">))</span><span class="o">$</span><span class="n">.pred</span><span class="p">,</span><span class="w">
    </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"True age"</span><span class="p">,</span><span class="w">
    </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Predicted age"</span><span class="p">,</span><span class="w">
    </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">16</span><span class="p">,</span><span class="w">
    </span><span class="n">log</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"xy"</span><span class="w">
</span><span class="p">)</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="m">0</span><span class="o">:</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">)</span><span class="w">
</span></code></pre></div>  </div>
</blockquote>

<h1 id="further-reading">Further reading</h1>

<ul>
  <li><a href="https://www.statlearning.com/">An introduction to statistical learning</a>.</li>
  <li><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">Elements of statistical learning</a>.</li>
  <li><a href="https://glmnet.stanford.edu/articles/glmnet.html">glmnet vignette</a>.</li>
  <li><a href="https://www.tidymodels.org/">tidymodels</a>.</li>
</ul>

<h1 id="footnotes">Footnotes</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Model selection including $R^2$, AIC and BIC are covered in the additional feature selection for regression episode of this course. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0014821">Epigenetic Predictor of Age, Bocklandt et al. (2011)</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>Regularisation is a way to fit a model, get better estimates of effect sizes, and perform variable selection simultaneously.</p>
</li>
    
    <li><p>Test and training splits, or cross-validation, are a useful way to select models or hyperparameters.</p>
</li>
    
    <li><p>Regularisation can give us a more predictive set of variables, and by restricting the magnitude of coefficients, can give us a better (and more stable) estimate of our outcome.</p>
</li>
    
    <li><p>Regularisation is often <em>very</em> fast! Compared to other methods for variable selection, it is very efficient. This makes it easier to practice rigorous variable selection.</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="principal-component-analysis" class="maintitle">Principal component analysis</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 90 min
      <br />
      <strong>Exercises:</strong> 30 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>What is principal component analysis (PCA) and when can it be used?</p>
</li>
	
	<li><p>How can we perform a PCA in R?</p>
</li>
	
	<li><p>How many principal components are needed to explain a significant amount of variation in the data?</p>
</li>
	
	<li><p>How to interpret the output of PCA using loadings and principal components?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Identify situations where PCA can be used to answer research questions using high-dimensional data.</p>
</li>
	
	<li><p>Perform a PCA on high-dimensional data.</p>
</li>
	
	<li><p>Select the appropriate number of principal components.</p>
</li>
	
	<li><p>Interpret the output of PCA.</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h1 id="introduction">Introduction</h1>

<p>Imagine a dataset which contains many variables ($p$), close to the total number
of rows in the dataset ($n$). Some of these variables are highly correlated and
several form groups which you might expect to represent the same overall effect.
Such datasets are challenging to analyse for several reasons, with the main
problem being how to reduce dimensionality in the dataset while retaining the
important features.</p>

<p>In this episode we will explore <em>principal component analysis</em> (PCA) as a
popular method of analysing high-dimensional data. PCA is an unsupervised
statistical method which allows large datasets of correlated variables to be
summarised into smaller numbers of uncorrelated principal components that
explain most of the variability in the original dataset. This is useful,
for example, during initial data exploration as it allows correlations among
data points to be observed and principal components to be calculated for
inclusion in further analysis (e.g. linear regression). An example of PCA might
be reducing several variables representing aspects of patient health
(blood pressure, heart rate, respiratory rate) into a single feature.</p>

<h1 id="advantages-and-disadvantages-of-pca">Advantages and disadvantages of PCA</h1>

<p>Advantages:</p>
<ul>
  <li>It is a relatively easy to use and popular method.</li>
  <li>Various software/packages are available to run a PCA.</li>
  <li>The calculations used in a PCA are easy to understand for statisticians and
non-statisticians alike.</li>
</ul>

<p>Disadvantages:</p>
<ul>
  <li>It assumes that variables in a dataset are correlated.</li>
  <li>It is sensitive to the scale at which input variables are measured.
If input variables are measured at different scales, the variables
with large variance relative to the scale of measurement will have
greater impact on the principal components relative to variables with smaller
variance. In many cases, this is not desirable.</li>
  <li>It is not robust against outliers, meaning that very large or small data
points can have a large effect on the output of the PCA.</li>
  <li>PCA assumes a linear relationship between variables which is not always a
realistic assumption.</li>
  <li>It can be difficult to interpret the meaning of the principal components,
especially when including them in further analyses (e.g. inclusion in a linear
regression).</li>
</ul>

<blockquote class="callout">
  <h2 id="supervised-vs-unsupervised-learning">Supervised vs unsupervised learning</h2>
  <p>Most statistical problems fall into one of two categories: supervised or
unsupervised learning. 
Examples of supervised learning problems include linear regression and include
analyses in which each observation has both at least one independent variable
($x$) as well as a dependent variable ($y$). In supervised learning problems
the aim is to predict the value of the response given future observations or
to understand the relationship between the dependent variable and the
predictors. In unsupervised learning for each observation there is no
dependent variable ($y$), but only 
a series of independent variables. In this situation there is no need for
prediction, as there is no dependent variable to predict (hence the analysis
can be thought as being unsupervised by the dependent variable). Instead
statistical analysis can be used to understand relationships between the
independent variables or between observations themselves. Unsupervised
learning problems often occur when analysing high-dimensional datasets in
which there is no obvious dependent variable to be
predicted, but the analyst would like to understand more about patterns
between groups of observations or reduce dimensionality so that a supervised
learning process may be used.</p>
</blockquote>

<blockquote class="challenge">
  <h2 id="challenge-1">Challenge 1</h2>

  <p>Descriptions of three datasets and research questions are given below. For
which of these might PCA be considered a useful tool for analysing data so
that the research questions may be addressed?</p>

  <ol>
    <li>An epidemiologist has data collected from different patients admitted to
hospital with infectious respiratory disease. They would like to determine
whether length of stay in hospital differs in patients with different
respiratory diseases.</li>
    <li>An online retailer has collected data on user interactions with its online
app and has information on the number of times each user interacted with
the app, what products they viewed per interaction, and the type and cost
of these products. The retailer would like to use this information to
predict whether or not a user will be interested in a new product.</li>
    <li>A scientist has assayed gene expression levels in 1000 cancer patients and
has data from probes targeting different genes in tumour samples from
patients. She would like to create new variables representing relative
abundance of different groups of genes to i) find out if genes form
subgroups based on biological function and ii) use these new variables
in a linear regression examining how gene expression varies with disease
severity.</li>
    <li>All of the above.</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution">Solution</h2>

    <p>In the first case, a regression model would be more suitable; perhaps a
survival model.
In the second, again a regression model, likely linear or logistic, would
be more suitable.
In the third example, PCA can help to identify modules of correlated
features that explain a large amount of variation within the data.</p>

    <p>Therefore the answer here is 3.</p>
  </blockquote>
</blockquote>

<h1 id="what-is-a-principal-component">What is a principal component?</h1>

<p>The first principal component is the direction of the data along which the
observations vary the most. The second principal component is the direction of
the data along which the observations show the next highest amount of variation.
For example, Figure 1 shows biodiversity index versus percentage area left
fallow for 50 farms in southern England. The red line represents the first
principal component direction of the data, which is the direction along which
there is greatest variability in the data. Projecting points onto this line
(i.e. by finding the location on the line closest to the point) would give a
vector of points with the greatest possible variance. The next highest amount
of variability in the data is represented by the line perpendicular to first
regression line which represents the second principal component (green line).</p>

<p>The second principal component is a linear combination of the variables that
is uncorrelated with the first principal component. There are as many principal
components as there are variables in your dataset, but as we’ll see, some are
more useful at explaining your data than others. By definition, the first
principal component explains more variation than other principal components.</p>

<div class="figure" style="text-align: center">
<img src="../fig/bio_index_vs_percentage_fallow.png" alt="Alt" />
<p class="caption">Cap</p>
</div>

<p>The animation below illustrates how principal components are calculated from
data. You can imagine that the black line is a rod and each red dashed line is
a spring. The energy of each spring is proportional to its squared length. The
direction of the first principal component is the one that minimises the total
energy of all of the springs. In the animation below, the springs pull the rod,
finding the direction of the first principal component when they reach
equilibrium. We then use the length of the springs from the rod as the first
principal component.
This is explained in more detail on <a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues">this Q&amp;A website</a>.</p>

<div class="figure" style="text-align: center">
<img src="../fig/pendulum.gif" alt="Alt" />
<p class="caption">Cap</p>
</div>

<p>The first principal component’s scores ($Z_1$) are calculated using the equation:</p>

<p>[Z_1 = a_{11}X_1 + a_{21}X_2 +….+a_{p1}X_p]</p>

<p>$X_1…X_p$ represents variables in the original dataset and $a_{11}…a_{p1}$
represent principal component loadings, which can be thought of as the degree to
which each variable contributes to the calculation of the principal component.
We will come back to principal component scores and loadings further below.</p>

<h1 id="how-do-we-perform-a-pca">How do we perform a PCA?</h1>

<h2 id="a-prostate-cancer-dataset">A prostate cancer dataset</h2>

<p>The <code class="language-plaintext highlighter-rouge">prostate</code> dataset represents data from 97
men who have prostate cancer. The data come from a study which examined the
correlation between the level of prostate specific antigen and a number of
clinical measures in men who were about to receive a radical prostatectomy.
The data have 97 rows and 9 columns.</p>

<p>Columns include:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">lcavol</code> (log-transformed cancer volume),</li>
  <li><code class="language-plaintext highlighter-rouge">lweight</code> (log-transformed prostate weight),</li>
  <li><code class="language-plaintext highlighter-rouge">lbph</code> (log-transformed amount of benign prostate enlargement),</li>
  <li><code class="language-plaintext highlighter-rouge">svi</code> (seminal vesicle invasion),</li>
  <li><code class="language-plaintext highlighter-rouge">lcp</code> (log-transformed capsular penetration; amount of spread of cancer in
 outer walls of prostate),</li>
  <li><code class="language-plaintext highlighter-rouge">gleason</code> (Gleason score; grade of cancer cells),</li>
  <li><code class="language-plaintext highlighter-rouge">pgg45</code> (percentage Gleason scores 4 or 5),</li>
  <li><code class="language-plaintext highlighter-rouge">lpsa</code> (log-tranformed prostate specific antigen; level of PSA in blood).</li>
  <li><code class="language-plaintext highlighter-rouge">age</code> (patient age in years).</li>
</ul>

<p>Here we will calculate principal component scores for each of the rows in this
dataset, using five principal components (one for each variable included in the
PCA). We will include five clinical variables in our PCA, each of the continuous
variables in the prostate dataset, so that we can create fewer variables
representing clinical markers of cancer progression. Standard PCAs are carried
out using continuous variables only.</p>

<p>First, we will examine the <code class="language-plaintext highlighter-rouge">prostate</code> dataset (originally part of the
<strong><code class="language-plaintext highlighter-rouge">lasso2</code></strong> package):</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prostate</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readRDS</span><span class="p">(</span><span class="n">here</span><span class="p">(</span><span class="s2">"data/prostate.rds"</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">head</span><span class="p">(</span><span class="n">prostate</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  X     lcavol  lweight age      lbph svi       lcp gleason pgg45       lpsa
1 1 -0.5798185 2.769459  50 -1.386294   0 -1.386294       6     0 -0.4307829
2 2 -0.9942523 3.319626  58 -1.386294   0 -1.386294       6     0 -0.1625189
3 3 -0.5108256 2.691243  74 -1.386294   0 -1.386294       7    20 -0.1625189
4 4 -1.2039728 3.282789  58 -1.386294   0 -1.386294       6     0 -0.1625189
5 5  0.7514161 3.432373  62 -1.386294   0 -1.386294       6     0  0.3715636
6 6 -1.0498221 3.228826  50 -1.386294   0 -1.386294       6     0  0.7654678
</code></pre></div></div>

<p>Note that each row of the dataset represents a single patient.</p>

<p>We will create a subset of the data including only the clinical variables we
want to use in the PCA.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pros2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">prostate</span><span class="p">[,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"lcavol"</span><span class="p">,</span><span class="w"> </span><span class="s2">"lweight"</span><span class="p">,</span><span class="w"> </span><span class="s2">"lbph"</span><span class="p">,</span><span class="w"> </span><span class="s2">"lcp"</span><span class="p">,</span><span class="w"> </span><span class="s2">"lpsa"</span><span class="p">)]</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">pros2</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      lcavol  lweight      lbph       lcp       lpsa
1 -0.5798185 2.769459 -1.386294 -1.386294 -0.4307829
2 -0.9942523 3.319626 -1.386294 -1.386294 -0.1625189
3 -0.5108256 2.691243 -1.386294 -1.386294 -0.1625189
4 -1.2039728 3.282789 -1.386294 -1.386294 -0.1625189
5  0.7514161 3.432373 -1.386294 -1.386294  0.3715636
6 -1.0498221 3.228826 -1.386294 -1.386294  0.7654678
</code></pre></div></div>

<h2 id="do-we-need-to-standardise-the-data">Do we need to standardise the data?</h2>

<p>Now we compare the variances between variables in the dataset.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">apply</span><span class="p">(</span><span class="n">pros2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">var</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  lcavol  lweight     lbph      lcp     lpsa 
1.389157 0.246642 2.104840 1.955102 1.332476 
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="n">hist</span><span class="p">(</span><span class="n">pros2</span><span class="o">$</span><span class="n">lweight</span><span class="p">,</span><span class="w"> </span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"FD"</span><span class="p">)</span><span class="w">
</span><span class="n">hist</span><span class="p">(</span><span class="n">pros2</span><span class="o">$</span><span class="n">lbph</span><span class="p">,</span><span class="w"> </span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"FD"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-05-var-hist-1.png" alt="Alt" width="432" />
<p class="caption">Alt</p>
</div>

<p>Note that variance is greatest for <code class="language-plaintext highlighter-rouge">lbph</code> and lowest for <code class="language-plaintext highlighter-rouge">lweight</code>. It is clear
from this output that we need to scale each of these variables before including
them in a PCA analysis to ensure that differences in variances between variables
do not drive the calculation of principal components. In this example we
standardise all five variables to have a mean of 0 and a standard
deviation of 1.</p>

<blockquote class="challenge">
  <h2 id="challenge-2">Challenge 2</h2>

  <p>Why might it be necessary to standardise variables before performing a PCA?<br />
Can you think of datasets where it might not be necessary to standardise
variables?
Discuss.</p>

  <ol>
    <li>To make the results of the PCA interesting.</li>
    <li>If you want to ensure that variables with different ranges of values
contribute equally to analysis.</li>
    <li>To allow the feature matrix to be calculated faster, especially in cases
where there are a lot of input variables.</li>
    <li>To allow both continuous and categorical variables to be included in the PCA.</li>
    <li>All of the above.</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution-1">Solution</h2>

    <p>2.
Scaling the data isn’t guaranteed to make the results more interesting.
It also won’t affect how quickly the output will be calculated, whether
continuous and categorical variables are present or not.</p>

    <p>It is done to ensure that all features have equal weighting in the resulting
PCs.</p>

    <p>You may not want to standardise datasets which contain continuous variables
all measured on the same scale (e.g. gene expression data or RNA sequencing
data). In this case, variables with very little sample-to-sample variability
may represent only random noise, and standardising the data would give
these extra weight in the PCA.</p>

  </blockquote>
</blockquote>

<p>Next we will carry out a PCA using the <code class="language-plaintext highlighter-rouge">prcomp()</code> function in base R. The input
data (<code class="language-plaintext highlighter-rouge">pros2</code>) is in the form of a matrix. Note that the <code class="language-plaintext highlighter-rouge">scale = TRUE</code> argument
is used to standardise the variables to have a mean 0 and standard deviation of
1.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca.pros</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">prcomp</span><span class="p">(</span><span class="n">pros2</span><span class="p">,</span><span class="w"> </span><span class="n">scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">center</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="n">pca.pros</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Standard deviations (1, .., p=5):
[1] 1.5648756 1.1684678 0.7452990 0.6362941 0.4748755

Rotation (n x k) = (5 x 5):
              PC1         PC2         PC3         PC4         PC5
lcavol  0.5616465 -0.23664270  0.01486043  0.22708502 -0.75945046
lweight 0.2985223  0.60174151 -0.66320198 -0.32126853 -0.07577123
lbph    0.1681278  0.69638466  0.69313753  0.04517286 -0.06558369
lcp     0.4962203 -0.31092357  0.26309227 -0.72394666  0.25253840
lpsa    0.5665123 -0.01680231 -0.10141557  0.56487128  0.59111493
</code></pre></div></div>

<h1 id="how-many-principal-components-do-we-need">How many principal components do we need?</h1>

<p>We have calculated one principal component for each variable in the original
dataset. How do we choose how many of these are necessary to represent the true
variation in the data, without having extra components that are unnecessary?</p>

<p>Let’s look at the relative importance of each component using <code class="language-plaintext highlighter-rouge">summary</code>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">summary</span><span class="p">(</span><span class="n">pca.pros</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Importance of components:
                          PC1    PC2    PC3     PC4    PC5
Standard deviation     1.5649 1.1685 0.7453 0.63629 0.4749
Proportion of Variance 0.4898 0.2731 0.1111 0.08097 0.0451
Cumulative Proportion  0.4898 0.7628 0.8739 0.95490 1.0000
</code></pre></div></div>

<p>This returns the proportion of variance in the data explained by each of the
(p = 5) principal components. In this example, PC1 explains approximately
49% of variance in the data, PC2 27% of variance,
PC3 a further 11%, PC4 approximately 8% and PC5
around 5%.</p>

<p>Let us visualise this. A plot of the amount of variance accounted for by each PC
is also called a scree plot. Note that the amount of variance accounted for by a principal
component is also called eigenvalue and thus the y-axis in scree plots if often
labelled “eigenvalue”.</p>

<p>Often, scree plots show a characteristic pattern where initially, the variance drops
rapidly with each additional principal component. But then there is an “elbow” after which the
variance decreases more slowly. The total variance explained up to the elbow point is sometimes
interpreted as structural variance that is relevant and should be retained versus noise
which may be discarded after the elbow.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># calculate variance explained</span><span class="w">
</span><span class="n">varExp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="n">pca.pros</span><span class="o">$</span><span class="n">sdev</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">pca.pros</span><span class="o">$</span><span class="n">sdev</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">100</span><span class="w">
</span><span class="c1"># calculate percentage variance explained using output from the PCA</span><span class="w">
</span><span class="n">varDF</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">Dimensions</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="nf">length</span><span class="p">(</span><span class="n">varExp</span><span class="p">),</span><span class="w"> </span><span class="n">varExp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">varExp</span><span class="p">)</span><span class="w">
</span><span class="c1"># create new dataframe with five rows, one for each principal component</span><span class="w">
</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">varDF</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-05-vardf-plot-1.png" alt="Alt" width="432" />
<p class="caption">Alt</p>
</div>

<p>The screeplot shows that the first principal component explains most of the
variance in the data (&gt;50%) and each subsequent principal component explains
less and less of the total variance. The first two principal components
explain &gt;70% of variance in the data. But what do these two principal
components mean?</p>

<h2 id="what-are-loadings-and-principal-component-scores">What are loadings and principal component scores?</h2>

<p>Most PCA functions will produce two main output matrices: the
<em>principal component scores</em> and the <em>loadings</em>. The matrix of principal component scores
has as many rows as there were observations in the input matrix. These
scores are what is usually visualised or used for down-stream analyses.
The matrix of loadings (also called rotation matrix) has as many rows as there
are features in the original data. It contains information about how the
(usually centered and scaled) original data relate to the PC scores.</p>

<p>When calling a PCA object generated with <code class="language-plaintext highlighter-rouge">prcomp()</code>, the loadings are printed by default:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca.pros</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Standard deviations (1, .., p=5):
[1] 1.5648756 1.1684678 0.7452990 0.6362941 0.4748755

Rotation (n x k) = (5 x 5):
              PC1         PC2         PC3         PC4         PC5
lcavol  0.5616465 -0.23664270  0.01486043  0.22708502 -0.75945046
lweight 0.2985223  0.60174151 -0.66320198 -0.32126853 -0.07577123
lbph    0.1681278  0.69638466  0.69313753  0.04517286 -0.06558369
lcp     0.4962203 -0.31092357  0.26309227 -0.72394666  0.25253840
lpsa    0.5665123 -0.01680231 -0.10141557  0.56487128  0.59111493
</code></pre></div></div>

<p>The principal component scores are obtained by carrying out matrix multiplication of the
(usually centered and scaled) original data times the loadings. The following
callout demonstrates this.</p>

<blockquote class="callout">
  <h2 id="computing-a-pca-by-hand">Computing a PCA “by hand”</h2>
  <p>The rotation matrix obtained in a PCA is identical to the eigenvectors
of the covariance matrix of the data. Multiplying these with the (centered and scaled)
data yields the PC scores:</p>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pros2.scaled</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">scale</span><span class="p">(</span><span class="n">pros2</span><span class="p">)</span><span class="w"> </span><span class="c1"># centre and scale the Prostate data</span><span class="w">
</span><span class="n">pros2.cov</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cov</span><span class="p">(</span><span class="n">pros2.scaled</span><span class="p">)</span><span class="w">   </span><span class="c1">#generate covariance matrix</span><span class="w">
</span><span class="n">pros2.cov</span><span class="w">
</span></code></pre></div>  </div>

  <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>           lcavol   lweight         lbph          lcp      lpsa
lcavol  1.0000000 0.1941283  0.027349703  0.675310484 0.7344603
lweight 0.1941283 1.0000000  0.434934636  0.100237795 0.3541204
lbph    0.0273497 0.4349346  1.000000000 -0.006999431 0.1798094
lcp     0.6753105 0.1002378 -0.006999431  1.000000000 0.5488132
lpsa    0.7344603 0.3541204  0.179809410  0.548813169 1.0000000
</code></pre></div>  </div>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pros2.eigen</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">eigen</span><span class="p">(</span><span class="n">pros2.cov</span><span class="p">)</span><span class="w"> </span><span class="c1"># preform eigen decomposition</span><span class="w">
</span><span class="n">pros2.eigen</span><span class="w"> </span><span class="c1"># The slot $vectors = rotation of the PCA</span><span class="w">
</span></code></pre></div>  </div>

  <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>eigen() decomposition
$values
[1] 2.4488355 1.3653171 0.5554705 0.4048702 0.2255067

$vectors
           [,1]        [,2]        [,3]        [,4]        [,5]
[1,] -0.5616465  0.23664270  0.01486043  0.22708502  0.75945046
[2,] -0.2985223 -0.60174151 -0.66320198 -0.32126853  0.07577123
[3,] -0.1681278 -0.69638466  0.69313753  0.04517286  0.06558369
[4,] -0.4962203  0.31092357  0.26309227 -0.72394666 -0.25253840
[5,] -0.5665123  0.01680231 -0.10141557  0.56487128 -0.59111493
</code></pre></div>  </div>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate PC scores by by hand, using matrix multiplication</span><span class="w">
</span><span class="n">my.pros2.pcs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pros2.scaled</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">pros2.eigen</span><span class="o">$</span><span class="n">vectors</span><span class="w">
</span><span class="c1"># compare results</span><span class="w">
</span><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">pca.pros</span><span class="o">$</span><span class="n">x</span><span class="p">[,</span><span class="m">1</span><span class="o">:</span><span class="m">2</span><span class="p">],</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"prcomp()"</span><span class="p">)</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="n">h</span><span class="o">=</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="o">=</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">my.pros2.pcs</span><span class="p">[,</span><span class="m">1</span><span class="o">:</span><span class="m">2</span><span class="p">],</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"\"By hand\""</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="o">=</span><span class="s2">"PC1"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="o">=</span><span class="s2">"PC2"</span><span class="p">)</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="n">h</span><span class="o">=</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="o">=</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w">
</span></code></pre></div>  </div>

  <div class="figure" style="text-align: center">
<img src="../fig/rmd-05-pca-by-hand-1.png" alt="plot of chunk pca-by-hand" width="432" />
<p class="caption">plot of chunk pca-by-hand</p>
</div>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">1</span><span class="p">))</span><span class="w">
</span><span class="c1"># Note that the axis orientations may be swapped but the relative positions of the dots should be the same in both plots.</span><span class="w">
</span></code></pre></div>  </div>
</blockquote>

<p>One way to visualise how principal components relate to the original variables
is by creating a biplot. Biplots usually show two principal components plotted
against each other. Observations are sometimes labelled with numbers. The
contribution of each original variable to the principal components displayed
is then shown by arrows (generated from those two columns of the rotation matrix that
correspond to the principal components shown). NB, there are several biplot
implementations in different R libraries. It is thus a good idea to specify
the desired package when calling <code class="language-plaintext highlighter-rouge">biplot()</code>. A biplot of the first two principal
components can be generated as follows:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stats</span><span class="o">::</span><span class="n">biplot</span><span class="p">(</span><span class="n">pca.pros</span><span class="p">,</span><span class="w"> </span><span class="n">xlim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">-0.3</span><span class="p">,</span><span class="w"> </span><span class="m">0.3</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-05-stats-biplot-1.png" alt="Alt" width="432" />
<p class="caption">Alt</p>
</div>

<p>This biplot shows the position of each patient on a 2-dimensional plot where
loadings can be observed via the red arrows associated with each of
the variables. The variables <code class="language-plaintext highlighter-rouge">lpsa</code>, <code class="language-plaintext highlighter-rouge">lcavol</code> and <code class="language-plaintext highlighter-rouge">lcp</code> are associated with
positive values on PC1 while positive values on PC2 are associated with the
variables <code class="language-plaintext highlighter-rouge">lbph</code> and <code class="language-plaintext highlighter-rouge">lweight</code>. The length of the arrows indicates how much
each variable contributes to the calculation of each principal component.</p>

<p>The left and bottom axes show normalised principal component scores. The axes
on the top and right of the plot are used to interpret the loadings, where
loadings are scaled by the standard deviation of the principal components
(<code class="language-plaintext highlighter-rouge">pca.pros$sdev</code>) times the square root the number of observations.</p>

<p>Finally, you need to know that PC scores and rotations may have different slot names, 
depending on the PCA implementation you use. Here are some examples:</p>

<table>
  <thead>
    <tr>
      <th>library::command()</th>
      <th>PC scores</th>
      <th>Loadings</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>stats::prcomp()</td>
      <td>$x</td>
      <td>$rotation</td>
    </tr>
    <tr>
      <td>stats::princomp()</td>
      <td>$scores</td>
      <td>$loadings</td>
    </tr>
    <tr>
      <td>PCAtools::pca()</td>
      <td>$rotated</td>
      <td>$loadings</td>
    </tr>
  </tbody>
</table>

<h1 id="using-pca-to-analyse-gene-expression-data">Using PCA to analyse gene expression data</h1>

<p>In this section you will carry out your own PCA using the Bioconductor package <strong><code class="language-plaintext highlighter-rouge">PCAtools</code></strong> 
applied to gene expression data to explore the topics covered above. 
<strong><code class="language-plaintext highlighter-rouge">PCAtools</code></strong> provides functions that can be used to explore data via PCA and
produce useful figures and analysis tools. The package is made for the somewhat unusual
Bioconductor style of data tables (observations in columns, features in rows). When
using Bioconductor data sets and <strong><code class="language-plaintext highlighter-rouge">PCAtools</code></strong>, it is thus not necessary to transpose the data.</p>

<h2 id="a-gene-expression-dataset-of-cancer-patients">A gene expression dataset of cancer patients</h2>

<p>The dataset we will be analysing in this lesson includes two subsets of data:</p>
<ul>
  <li>a matrix of gene expression data showing microarray results for different
probes used to examine gene expression profiles in 91 different breast
cancer patient samples.</li>
  <li>metadata associated with the gene expression results detailing information
from patients from whom samples were taken.</li>
</ul>

<p>Let’s load the <strong><code class="language-plaintext highlighter-rouge">PCAtools</code></strong> package and the data.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"PCAtools"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>We will first load the microarray breast cancer gene expression data and
associated metadata, downloaded from the
<a href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2990">Gene Expression Omnibus</a>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"SummarizedExperiment"</span><span class="p">)</span><span class="w">
</span><span class="n">cancer</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readRDS</span><span class="p">(</span><span class="n">here</span><span class="o">::</span><span class="n">here</span><span class="p">(</span><span class="s2">"data/cancer_expression.rds"</span><span class="p">))</span><span class="w">
</span><span class="n">mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">assay</span><span class="p">(</span><span class="n">cancer</span><span class="p">)</span><span class="w">
</span><span class="n">metadata</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">colData</span><span class="p">(</span><span class="n">cancer</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">View</span><span class="p">(</span><span class="n">mat</span><span class="p">)</span><span class="w">
</span><span class="c1">#nrow=22215 probes</span><span class="w">
</span><span class="c1">#ncol=91 samples</span><span class="w">
</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">View</span><span class="p">(</span><span class="n">metadata</span><span class="p">)</span><span class="w">
</span><span class="c1">#nrow=91</span><span class="w">
</span><span class="c1">#ncol=8</span><span class="w">
</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">all</span><span class="p">(</span><span class="n">colnames</span><span class="p">(</span><span class="n">mat</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">rownames</span><span class="p">(</span><span class="n">metadata</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] TRUE
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Check that column names and row names match</span><span class="w">
</span><span class="c1">#If they do should return TRUE</span><span class="w">
</span></code></pre></div></div>

<p>The ‘mat’ variable contains a matrix of gene expression profiles for each sample.
Rows represent gene expression measurements and columns represent samples. The
‘metadata’ variable contains the metadata associated with the gene expression
data including the name of the study from which data originate, the age of the
patient from which the sample was taken, whether or not an oestrogen receptor
was involved in their cancer and the grade and size of the cancer for each
sample (represented by rows).</p>

<p>Microarray data are difficult to analyse for several reasons. Firstly, 
they are typically high-dimensional and therefore are subject to the same
difficulties associated with analysing high dimensional data outlined above
(i.e. <em>p</em>&gt;<em>n</em>, large numbers of rows, multiple possible response variables,
curse of dimensionality). Secondly, formulating a research question using
microarray data can be difficult, especially if not much is known a priori
about which genes code for particular phenotypes of interest. Finally,
exploratory analysis, which can be used to help formulate research questions
and display relationships, is difficult using microarray data due to the number
of potentially interesting response variables (i.e. expression data from probes
targeting different genes).</p>

<p>If researchers hypothesise that groups of genes (e.g. biological pathways) may
be associated with different phenotypic characteristics of cancers (e.g.
histologic grade, tumour size), using statistical methods that reduce the
number of columns in the microarray matrix to a smaller number of dimensions
representing groups of genes would help visualise the data and address
research questions regarding the effect different groups of genes have on
disease progression.</p>

<p>Using the <strong><code class="language-plaintext highlighter-rouge">PCAtools</code></strong> we will apply a PCA to the cancer
gene expression data, plot the amount of variation in the data explained by
each principal component and plot the most important principal components
against each other as well as understanding what each principal component
represents.</p>

<blockquote class="challenge">
  <h2 id="challenge-3">Challenge 3</h2>

  <p>Apply a PCA to the cancer gene expression data using the <code class="language-plaintext highlighter-rouge">pca()</code> function from
<strong><code class="language-plaintext highlighter-rouge">PCAtools</code></strong>. You can use the help files in PCAtools to find out about the <code class="language-plaintext highlighter-rouge">pca()</code>
function (type <code class="language-plaintext highlighter-rouge">help("pca")</code> or <code class="language-plaintext highlighter-rouge">?pca</code> in R).</p>

  <p>Let us assume we only care about the principal components accounting for the top
80% of the variance in the dataset. Use the <code class="language-plaintext highlighter-rouge">removeVar</code> argument in <code class="language-plaintext highlighter-rouge">pca()</code> to remove
the PCs accounting for the bottom 20%.</p>

  <p>As in the example using prostate data above, examine the first 5 rows and
columns of rotated data and loadings from your PCA.</p>

  <blockquote class="solution">
    <h2 id="solution-2">Solution</h2>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pc</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pca</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span><span class="w"> </span><span class="n">metadata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metadata</span><span class="p">)</span><span class="w">
</span><span class="c1">#Many PCs explain a very small amount of the total variance in the data</span><span class="w">
</span><span class="c1">#Remove the lower 20% of PCs with lower variance</span><span class="w">
</span><span class="n">pc</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pca</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span><span class="w"> </span><span class="n">metadata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metadata</span><span class="p">,</span><span class="w"> </span><span class="n">removeVar</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.2</span><span class="p">)</span><span class="w">
</span><span class="c1">#Explore other arguments provided in pca</span><span class="w">
</span><span class="n">pc</span><span class="o">$</span><span class="n">rotated</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">5</span><span class="p">]</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>               PC1        PC2        PC3        PC4        PC5
GSM65752 -29.79105  43.866788   3.255903 -40.663138 15.3427597
GSM65753 -37.33911 -15.244788  -4.948201  -6.182795  9.4725870
GSM65755 -29.41462   7.846858 -22.880525 -16.149669 22.3821009
GSM65757 -33.35286   1.343573 -22.579568   2.200329 15.0082786
GSM65758 -40.51897  -8.491125   5.288498  14.007364  0.8739772
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pc</span><span class="o">$</span><span class="n">loadings</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">5</span><span class="p">]</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                    PC1          PC2          PC3        PC4          PC5
206378_at -0.0024680993 -0.053253543 -0.004068209 0.04068635  0.015078376
205916_at -0.0051557973  0.001315022 -0.009836545 0.03992371  0.038552048
206799_at  0.0005684075 -0.050657061 -0.009515725 0.02610233  0.006208078
205242_at  0.0130742288  0.028876408  0.007655420 0.04449641 -0.001061205
206509_at  0.0019031245 -0.054698479 -0.004667356 0.01566468  0.001306807
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">which.max</span><span class="p">(</span><span class="n">pc</span><span class="o">$</span><span class="n">loadings</span><span class="p">[,</span><span class="w"> </span><span class="m">1</span><span class="p">])</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 49
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pc</span><span class="o">$</span><span class="n">loadings</span><span class="p">[</span><span class="m">49</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                   PC1          PC2         PC3          PC4          PC5
215281_x_at 0.03752947 -0.007369379 0.006243377 -0.008242589 -0.004783206
                   PC6          PC7         PC8         PC9          PC10
215281_x_at 0.01194012 -0.002822407 -0.01216792 0.001137451 -0.0009056616
                   PC11          PC12       PC13         PC14          PC15
215281_x_at -0.00196034 -0.0001676705 0.00699201 -0.002897995 -0.0005044658
                     PC16        PC17         PC18        PC19         PC20
215281_x_at -0.0004547916 0.002277035 -0.006199078 0.002708574 -0.006217326
                  PC21        PC22        PC23        PC24        PC25
215281_x_at 0.00516745 0.007625912 0.003434534 0.005460017 0.001477415
                   PC26         PC27          PC28         PC29       PC30
215281_x_at 0.002350428 0.0007183107 -0.0006195515 0.0006349803 0.00413627
                    PC31        PC32         PC33         PC34         PC35
215281_x_at 0.0001322301 0.003182956 -0.002123462 -0.001042769 -0.001729869
                    PC36        PC37        PC38          PC39        PC40
215281_x_at -0.006556369 0.005766949 0.002537993 -0.0002846248 -0.00018195
                     PC41        PC42         PC43          PC44         PC45
215281_x_at -0.0007970789 0.003888626 -0.008210075 -0.0009570174 0.0007998935
                     PC46         PC47        PC48        PC49         PC50
215281_x_at -0.0006931441 -0.005717836 0.005189649 0.002591188 0.0007810259
                   PC51        PC52         PC53         PC54        PC55
215281_x_at 0.006610815 0.005371134 -0.001704796 -0.002286475 0.001365417
                   PC56         PC57        PC58         PC59         PC60
215281_x_at 0.003529892 0.0003375981 0.009895923 -0.001564423 -0.006989092
                   PC61        PC62         PC63          PC64        PC65
215281_x_at 0.000971273 0.001345406 -0.003575415 -0.0005588113 0.006516669
                    PC66        PC67       PC68         PC69        PC70
215281_x_at -0.008770186 0.006699641 0.01284606 -0.005041574 0.007845653
                   PC71        PC72         PC73         PC74        PC75
215281_x_at 0.003964697 -0.01104367 -0.001506485 -0.001583824 0.003798343
                   PC76         PC77         PC78         PC79          PC80
215281_x_at 0.004817252 -0.001290033 -0.004402926 -0.003440367 -0.0001646198
                   PC81        PC82          PC83         PC84        PC85
215281_x_at 0.003923775 0.003179556 -0.0004388192 9.664648e-05 0.003501335
                   PC86        PC87          PC88         PC89         PC90
215281_x_at -0.00112973 0.006489667 -0.0005039785 -0.004296355 -0.002751513
                   PC91
215281_x_at -0.00383085
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">which.max</span><span class="p">(</span><span class="n">pc</span><span class="o">$</span><span class="n">loadings</span><span class="p">[,</span><span class="w"> </span><span class="m">2</span><span class="p">])</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 27
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pc</span><span class="o">$</span><span class="n">loadings</span><span class="p">[</span><span class="m">27</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                   PC1        PC2          PC3        PC4          PC5
211122_s_at 0.01649085 0.05090275 -0.003378728 0.05178144 -0.003742393
                    PC6         PC7          PC8        PC9        PC10
211122_s_at -0.00543753 -0.03522848 -0.006333521 0.01575401 0.004732546
                   PC11        PC12        PC13        PC14       PC15
211122_s_at 0.004687599 -0.01349892 0.005207937 -0.01731898 0.02323893
                   PC16       PC17        PC18       PC19        PC20
211122_s_at -0.02069509 0.01477432 0.005658529 0.02667751 -0.01333503
                    PC21        PC22       PC23         PC24        PC25
211122_s_at -0.003254036 0.003572342 0.01416779 -0.005511838 -0.02582847
                  PC26        PC27       PC28        PC29       PC30      PC31
211122_s_at 0.03405417 -0.01797345 0.01826328 0.005123959 0.01300763 0.0127127
                   PC32       PC33       PC34        PC35        PC36
211122_s_at 0.002477672 0.01933214 0.03017661 -0.01935071 -0.01960912
                   PC37        PC38        PC39        PC40       PC41
211122_s_at 0.004411188 -0.01263612 -0.02019279 -0.01441513 -0.0310399
                   PC42         PC43        PC44        PC45        PC46
211122_s_at -0.02540426 0.0007949801 -0.00200195 -0.01748543 0.006881834
                   PC47         PC48        PC49         PC50        PC51
211122_s_at 0.006690698 -0.004000732 -0.02747926 -0.006963189 -0.02232332
                     PC52        PC53        PC54        PC55       PC56
211122_s_at -0.0003089115 -0.01604491 0.005649511 -0.02629501 0.02332997
                   PC57        PC58        PC59        PC60         PC61
211122_s_at -0.01248022 -0.01563245 0.005369433 0.009445262 -0.005209349
                  PC62       PC63       PC64        PC65        PC66
211122_s_at 0.01787645 0.01629425 0.02457665 -0.02384242 0.002814479
                    PC67        PC68         PC69         PC70       PC71
211122_s_at 0.0004584731 0.007939733 -0.009554166 -0.003967123 0.01825668
                   PC72        PC73        PC74        PC75        PC76
211122_s_at -0.00580374 -0.02236727 0.001295688 -0.02264723 0.006855855
                   PC77         PC78       PC79         PC80        PC81
211122_s_at 0.004995447 -0.008404118 0.00442875 -0.001027912 0.006104406
                   PC82        PC83         PC84       PC85       PC86
211122_s_at -0.01988441 0.009667348 -0.008248781 0.01198369 0.01221713
                    PC87        PC88        PC89        PC90        PC91
211122_s_at -0.003864842 -0.02876816 -0.01771452 -0.02164973 0.004593411
</code></pre></div>    </div>
    <p>The function <code class="language-plaintext highlighter-rouge">pca()</code> is used to perform PCA, and uses as inputs a matrix
(<code class="language-plaintext highlighter-rouge">mat</code>) containing continuous numerical data
in which rows are data variables and columns are samples, and <code class="language-plaintext highlighter-rouge">metadata</code>
associated with the matrix in which rows represent samples and columns
represent data variables. It has options to centre or scale the input data
before a PCA is performed, although in this case gene expression data do
not need to be transformed prior to PCA being carried out as variables are
measured on a similar scale (values are comparable between rows). The output
of the <code class="language-plaintext highlighter-rouge">pca()</code> function includes a lot of information such as loading values
for each variable (<code class="language-plaintext highlighter-rouge">loadings</code>), principal component scores (<code class="language-plaintext highlighter-rouge">rotated</code>)
and the amount of variance in the data
explained by each principal component.</p>

    <p>Rotated data shows principal
component scores for each sample and each principal component. Loadings
the contribution each variable makes to each principal component.</p>
  </blockquote>
</blockquote>

<blockquote class="callout">
  <h2 id="scaling-variables-for-pca">Scaling variables for PCA</h2>

  <p>When running <code class="language-plaintext highlighter-rouge">pca()</code> above, we kept the default setting, <code class="language-plaintext highlighter-rouge">scale=FALSE</code>. That means genes with higher variation in
their expression levels should have higher loadings, which is what we are interested in.
Whether or not to scale variables for PCA will depend on your data and research question.</p>

  <p>Note that this is different from normalising gene expression data. Gene expression
data have to be normalised before donwstream analyses can be
carried out. This is to reduce to effect technical and other potentially confounding
factors. We assume that the expression data we use had been noralised previously.</p>
</blockquote>

<h2 id="choosing-how-many-components-are-important-to-explain-the-variance-in-the-data">Choosing how many components are important to explain the variance in the data</h2>

<p>As in the example using the <code class="language-plaintext highlighter-rouge">prostate</code> dataset we can use a screeplot to
compare the proportion of variance in the data explained by each principal
component. This allows us to understand how much information in the microarray
dataset is lost by projecting the observations onto the first few principal
components and whether these principal components represent a reasonable
amount of the variation. The proportion of variance explained should sum to one.</p>

<p>There are no clear guidelines on how many principal components should be
included in PCA: your choice depends on the total variability of the data and
the size of the dataset. We often look at the ‘elbow’ on the screeplot as an
indicator that the addition of principal components does not drastically
contribute to explain the remaining variance or choose an arbitory cut off for
proportion of variance explained.</p>

<blockquote class="challenge">
  <h2 id="challenge-4">Challenge 4</h2>

  <p>Using the <code class="language-plaintext highlighter-rouge">screeplot()</code> function in <strong><code class="language-plaintext highlighter-rouge">PCAtools</code></strong>, create a screeplot to show 
proportion of variance explained by each principal component. Explain the
output of the screeplot in terms of proportion of variance in data explained
by each principal component.</p>

  <blockquote class="solution">
    <h2 id="solution-3">Solution</h2>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">screeplot</span><span class="p">(</span><span class="n">pc</span><span class="p">,</span><span class="w"> </span><span class="n">axisLabSize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">titleLabSize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">8</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-05-scree-ex-1.png" alt="Alt" width="432" />
<p class="caption">Alt</p>
</div>
    <p>Note that first principal component (PC1) explains more variation than
other principal components (which is always the case in PCA). The screeplot
shows that the first principal component only explains ~33% of the total
variation in the micrarray data and many principal components explain very 
little variation. The red line shows the cumulative percentage of explained
variation with increasing principal components. Note that in this case 18
principal components are needed to explain over 75% of variation in the
data. This is not an unusual result for complex biological datasets
including genetic information as clear relationships between groups are
sometimes difficult to observe in the data. The screeplot shows that using
a PCA we have reduced 91 predictors to 18 in order to explain a significant
amount of variation in the data. See additional arguments in screeplot
function for improving the appearance of the plot.</p>
  </blockquote>
</blockquote>

<h2 id="investigating-the-principal-components">Investigating the principal components</h2>

<p>Once the most important principal components have been identified using
<code class="language-plaintext highlighter-rouge">screeplot()</code>, these can be explored in more detail by plotting principal components
against each other and highlighting points based on variables in the metadata.
This will allow any potential clustering of points according to demographic or
phenotypic variables to be seen.</p>

<p>We can use biplots to look for patterns in the output from the PCA. Note that there
are two functions called <code class="language-plaintext highlighter-rouge">biplot()</code>, one in the package <strong><code class="language-plaintext highlighter-rouge">PCAtools</code></strong> and one in
<strong><code class="language-plaintext highlighter-rouge">stats</code></strong>. Both functions produce biplots but their scales are different!</p>

<blockquote class="challenge">
  <h2 id="challenge-5">Challenge 5</h2>

  <p>Create a biplot of the first two principal components from your PCA
using <code class="language-plaintext highlighter-rouge">biplot()</code> function in <strong><code class="language-plaintext highlighter-rouge">PCAtools</code></strong>. See <code class="language-plaintext highlighter-rouge">help("PCAtools::biplot")</code> for
arguments and their meaning. For instance, <code class="language-plaintext highlighter-rouge">lab</code> or <code class="language-plaintext highlighter-rouge">colBy</code> may be useful.</p>

  <p>Examine whether the data appear to form clusters. Explain your results.</p>

  <blockquote class="solution">
    <h2 id="solution-4">Solution</h2>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">biplot</span><span class="p">(</span><span class="n">pc</span><span class="p">,</span><span class="w"> </span><span class="n">lab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">,</span><span class="w"> </span><span class="n">colby</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'Grade'</span><span class="p">,</span><span class="w"> </span><span class="n">legendPosition</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'top'</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-05-biplot-ex-1.png" alt="Alt" width="432" />
<p class="caption">Alt</p>
</div>
    <p>The biplot shows the position of patient samples relative to PC1 and PC2
in a 2-dimensional plot. Note that two groups are apparent along the PC1
axis according to expressions of different genes while no separation can be
seem along the PC2 axis. Labels of patient samples are automatically added
in the biplot. Labels for each sample are added by default, but can be
removed if there is too much overlap in names. Note that <strong><code class="language-plaintext highlighter-rouge">PCAtools</code></strong> does
not scale biplot in the same way as biplot using the stats package.</p>
  </blockquote>
</blockquote>

<p>Let’s consider this biplot in more detail, and also display the loadings:</p>

<blockquote class="challenge">
  <h2 id="challenge-6">Challenge 6</h2>

  <p>Use <code class="language-plaintext highlighter-rouge">colby</code> and <code class="language-plaintext highlighter-rouge">lab</code> arguments in <code class="language-plaintext highlighter-rouge">biplot()</code> to explore whether these two
groups may cluster by patient age or by whether or not the sample expresses
the oestrogen receptor gene (ER+ or ER-).</p>

  <p>Note: You may see a warning about <code class="language-plaintext highlighter-rouge">ggrepel</code>. This happens when there are many
labels but little space for plotting. This is not usually a serious problem - 
not all labels will be shown.</p>

  <blockquote class="solution">
    <h2 id="solution-5">Solution</h2>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">  </span><span class="n">PCAtools</span><span class="o">::</span><span class="n">biplot</span><span class="p">(</span><span class="n">pc</span><span class="p">,</span><span class="w">
    </span><span class="n">lab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">paste0</span><span class="p">(</span><span class="n">pc</span><span class="o">$</span><span class="n">metadata</span><span class="o">$</span><span class="n">Age</span><span class="p">,</span><span class="s1">'years'</span><span class="p">),</span><span class="w">
    </span><span class="n">colby</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'ER'</span><span class="p">,</span><span class="w">
    </span><span class="n">hline</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">vline</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w">
    </span><span class="n">legendPosition</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'right'</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext warning highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Warning: ggrepel: 35 unlabeled data points (too many overlaps). Consider
increasing max.overlaps
</code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-05-pca-biplot-ex2-1.png" alt="Alt" width="432" />
<p class="caption">Alt</p>
</div>
    <p>It appears that one cluster has more ER+ samples than the other group.</p>
  </blockquote>
</blockquote>

<p>So far, we have only looked at a biplot of PC1 versus PC2 which only gives part
of the picture. The <code class="language-plaintext highlighter-rouge">pairplots()</code> function in <strong><code class="language-plaintext highlighter-rouge">PCAtools</code></strong> can be used to create
multiple biplots including different principal components.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pairsplot</span><span class="p">(</span><span class="n">pc</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-05-pairsplot-1.png" alt="Alt" width="432" />
<p class="caption">Alt</p>
</div>

<p>The plots show two apparent clusters involving the first principal component
only. No other clusters are found involving other principal components. Each dot
is coloured differently along a gradient of blues. This can potentially help identifying
the same observation/individual in several panels. Here too, the argument <code class="language-plaintext highlighter-rouge">colby</code> allows
you to set custom colours.</p>

<p>Finally, it can sometimes be of interest to compare how certain variables contribute
to different principal components. This can be visualised with <code class="language-plaintext highlighter-rouge">plotloadings()</code> from
the <strong><code class="language-plaintext highlighter-rouge">PCAtools</code></strong> package. The function checks the range of loadings for each
principal component specified (default: first five PCs). It then selects the features
in the top and bottom 5% of these ranges and displays their loadings. This behaviour
can be adjusted with the <code class="language-plaintext highlighter-rouge">rangeRetain</code> argument, which has 0.1 as the default value (i.e.
5% on each end of the range). NB, if there are too many labels to be plotted, you will see
a warning. This is not a serious problem.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plotloadings</span><span class="p">(</span><span class="n">pc</span><span class="p">,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"PC1"</span><span class="p">),</span><span class="w"> </span><span class="n">rangeRetain</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.1</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-05-loadingsplots-1.png" alt="plot of chunk loadingsplots" width="432" />
<p class="caption">plot of chunk loadingsplots</p>
</div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plotloadings</span><span class="p">(</span><span class="n">pc</span><span class="p">,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"PC2"</span><span class="p">),</span><span class="w"> </span><span class="n">rangeRetain</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.1</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-05-loadingsplots-2.png" alt="plot of chunk loadingsplots" width="432" />
<p class="caption">plot of chunk loadingsplots</p>
</div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plotloadings</span><span class="p">(</span><span class="n">pc</span><span class="p">,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"PC1"</span><span class="p">,</span><span class="w"> </span><span class="s2">"PC2"</span><span class="p">),</span><span class="w"> </span><span class="n">rangeRetain</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.1</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-05-loadingsplots-3.png" alt="plot of chunk loadingsplots" width="432" />
<p class="caption">plot of chunk loadingsplots</p>
</div>

<p>You can see how the third code line prooces more dots, some of which do not have
extreme loadings. This is because all loadings selected for any PC are shown for all
other PCs. For instance, it is plausible that features which have high loadings on
PC1 may have lower ones on PC2.</p>

<h1 id="using-pca-output-in-further-analysis">Using PCA output in further analysis</h1>

<p>The output of PCA can be used to interpret data or can be used in further
analyses. For example, the PCA outputs new variables (principal components)
which represent several variables in the original dataset. These new variables
are useful for further exploring data, for example, comparing principal
component scores between groups or including the new variables in linear
regressions. Because the principal components are uncorrelated (and independent)
they can be included together in a single linear regression.</p>

<blockquote class="callout">
  <h2 id="principal-component-regression">Principal component regression</h2>

  <p>PCA is often used to reduce large numbers of correlated variables into fewer
uncorrelated variables that can then be included in linear regression or
other models. This technique is called principal component regression (PCR)
and it allows researchers to examine the effect of several correlated
explanatory variables on a single response variable in cases where a high
degree of correlation initially prevents them from being included in the same
model. This is called principal componenet regression (PCR) and is just one
example of how principal components can be used in further analysis of data.
When carrying out PCR, the variable of interest (response/dependent variable)
is regressed against the principal components calculated using PCA, rather
than against each individual explanatory variable from the original dataset.
As there as many principal components created from PCA as there are variables
in the dataset, we must select which principal components to include in PCR.
This can be done by examining the amount of variation in the data explained
by each principal component (see above).</p>
</blockquote>

<h1 id="further-reading">Further reading</h1>

<ul>
  <li>James, G., Witten, D., Hastie, T. &amp; Tibshirani, R. (2013) An Introduction to Statistical Learning with Applications in R. 
Chapter 6.3 (Dimension Reduction Methods), Chapter 10 (Unsupervised Learning).</li>
  <li><a href="http://dx.doi.org/10.1098/rsta.2015.0202">Jolliffe, I.T. &amp; Cadima, J. (2016) Principal component analysis: a review and recent developments. Phil. Trans. R. Soc A 374.</a>.</li>
  <li><a href="https://doi.org/10.1098/rsta.2009.0159">Johnstone, I.M. &amp; Titterington, D.M. (2009) Statistical challenges of high-dimensional data. Phil. Trans. R. Soc A 367.</a></li>
  <li><a href="https://www.analyticsvidhya.com/blog/2016/03/pca-practical-guide-principal-component-analysis-python/">PCA: A Practical Guide to Principal Component Analysis, Analytics Vidhya</a>.</li>
  <li><a href="https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c">A One-Stop Shop for Principal Component Analysis, Towards Data Science</a>.</li>
</ul>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>A principal component analysis is a statistical approach used to reduce dimensionality in high-dimensional datasets (i.e. where $p$ is equal or greater than $n$).</p>
</li>
    
    <li><p>PCA may be used to create a low-dimensional set of features from a larger set of variables. Examples of when a PCA may be useful include reducing high-dimensional datasets to fewer variables for use in a linear regression and for identifying groups with similar features.</p>
</li>
    
    <li><p>PCA is a useful dimensionality reduction technique used in the analysis of complex biological datasets (e.g. high throughput data or genetics data).</p>
</li>
    
    <li><p>The first principal component represents the dimension along which there is maximum variation in the data. Subsequent principal components represent dimensions with progressively less variation.</p>
</li>
    
    <li><p>Screeplots and biplots may be used to show: 1. how much variation in the data is explained by each principal component and 2. how data points cluster according to principal component scores and which variables are associated with these scores.</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="factor-analysis" class="maintitle">Factor analysis</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 25 min
      <br />
      <strong>Exercises:</strong> 10 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>What is factor analysis and when can it be used?</p>
</li>
	
	<li><p>What are communality and uniqueness in factor analysis?</p>
</li>
	
	<li><p>How to decide on the number of factors to use?</p>
</li>
	
	<li><p>How to interpret the output of factor analysis?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Perform a factor analysis on high-dimensional data.</p>
</li>
	
	<li><p>Select an appropriate number of factors.</p>
</li>
	
	<li><p>Interpret the output of factor analysis.</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h1 id="introduction">Introduction</h1>

<p>Biologists often encounter high-dimensional datasets from which they wish
to extract underlying features – they need to carry out dimensionality
reduction. The last episode dealt with one method to achieve this this,
called principal component analysis (PCA). Here, we introduce more general
set of methods called factor analysis (FA).</p>

<p>There are two types of FA, called exploratory and confirmatory factor analysis
(EFA and CFA). Both EFA and CFA aim to reproduce the observed relationships
among a group of features with a smaller set of latent variables. EFA
is used in a descriptive, data-driven manner to uncover which
measured variables are reasonable indicators of the various latent dimensions.
In contrast, CFA is conducted in an a-priori,
hypothesis-testing manner that requires strong empirical or theoretical foundations.
We will mainly focus on EFA here, which is used to group features into a specified
number of latent factors.</p>

<p>Unlike with PCA, researchers using FA have to specify the number of latent
variables (factors) at the point of running the analysis. Researchers may use
exploratory data analysis methods (including PCA) to provide an initial estimate
of how many factors adequately explain the variation observed in a dataset.
In practice, a range of different values is usually tested.</p>

<h2 id="an-example">An example</h2>

<p>One scenario for using FA would be whether student scores in different subjects
can be summarised by certain subject categories. Take a look at the hypothetical
dataset below. If we were to run and EFA on this, we might find that the scores
can be summarised well by two factors, which we can then interpret. We have
labelled these hypothetical factors “mathematical ability” and “writing ability”.</p>

<div class="figure" style="text-align: center">
<img src="../fig/table_for_fa.png" alt="plot of chunk table" />
<p class="caption">plot of chunk table</p>
</div>
<p>So, EFA is designed to identify a specified number of unobservable factors from
observable features contained in the original dataset. This is slightly
different from PCA, which does not do this directly. Just to recap, PCA creates
as many principal components as there are features in the dataset, each
component representing a different linear combination of features. The principal
components are ordered by the amount of variance they account for.</p>

<h1 id="advantages-and-disadvantages-of-factor-analysis">Advantages and disadvantages of Factor Analysis</h1>

<p>There are several advantages and disadvantages of using FA as a
dimensionality reduction method.</p>

<p>Advantages:</p>

<ul>
  <li>FA is a useful way of combining different groups of data into known
representative factors, thus reducing dimensionality in a dataset.</li>
  <li>FA can take into account researchers’ expert knowledge when choosing
the number of factors to use, and can be used to identify latent or hidden
variables which may not be apparent from using other analysis methods.</li>
  <li>It is easy to implement with many software tools available to carry out FA.</li>
  <li>Confirmatory FA can be used to test hypotheses.</li>
</ul>

<p>Disadvantages:</p>

<ul>
  <li>Justifying the choice of
number of factors to use may be difficult if little is known about the
structure of the data before analysis is carried out.</li>
  <li>Sometimes, it can be difficult to interpret what factors mean after
analysis has been completed.</li>
  <li>Like PCA, standard methods of carrying out FA assume that input variables
are continuous, although extensions to FA allow ordinal and binary
variables to be included (after transforming the input matrix).</li>
</ul>

<h1 id="prostate-cancer-patient-data">Prostate cancer patient data</h1>

<p>The prostate dataset represents data from 97 men who have prostate cancer.
The data come from a study which examined the correlation between the level
of prostate specific antigen and a number of clinical measures in men who were
about to receive a radical prostatectomy. The data have 97 rows and 9 columns.</p>

<p>Columns are:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">lcavol</code>: log (cancer volume)</li>
  <li><code class="language-plaintext highlighter-rouge">lweight</code>: log (prostate weight)</li>
  <li><code class="language-plaintext highlighter-rouge">age</code>: age (years)</li>
  <li><code class="language-plaintext highlighter-rouge">lbph</code>: log (benign prostatic hyperplasia amount)</li>
  <li><code class="language-plaintext highlighter-rouge">svi</code>: seminal vesicle invasion</li>
  <li><code class="language-plaintext highlighter-rouge">lcp</code>: log (capsular penetration); amount of spread of cancer in outer walls
of prostate</li>
  <li><code class="language-plaintext highlighter-rouge">gleason</code>: <a href="https://en.wikipedia.org/wiki/Gleason_grading_system">Gleason score</a></li>
  <li><code class="language-plaintext highlighter-rouge">pgg45</code>: percentage Gleason scores 4 or 5</li>
  <li><code class="language-plaintext highlighter-rouge">lpsa</code>: log (prostate specific antigen)</li>
</ul>

<p>In this example, we use the clinical variables to identify factors representing
various clinical variables from prostate cancer patients. Two principal
components have already been identified as explaining a large proportion
of variance in the data when these data were analysed in the PCA episode.
We may expect a similar number of factors to exist in the data.</p>

<p>Let’s subset the data to just include the log-transformed clinical variables
for the purposes of this episode:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prostate</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readRDS</span><span class="p">(</span><span class="n">here</span><span class="p">(</span><span class="s2">"data/prostate.rds"</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">View</span><span class="p">(</span><span class="n">prostate</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nrow</span><span class="p">(</span><span class="n">prostate</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 97
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">head</span><span class="p">(</span><span class="n">prostate</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  X     lcavol  lweight age      lbph svi       lcp gleason pgg45       lpsa
1 1 -0.5798185 2.769459  50 -1.386294   0 -1.386294       6     0 -0.4307829
2 2 -0.9942523 3.319626  58 -1.386294   0 -1.386294       6     0 -0.1625189
3 3 -0.5108256 2.691243  74 -1.386294   0 -1.386294       7    20 -0.1625189
4 4 -1.2039728 3.282789  58 -1.386294   0 -1.386294       6     0 -0.1625189
5 5  0.7514161 3.432373  62 -1.386294   0 -1.386294       6     0  0.3715636
6 6 -1.0498221 3.228826  50 -1.386294   0 -1.386294       6     0  0.7654678
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#select five log-transformed clinical variables for further analysis</span><span class="w">
</span><span class="n">pros2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">prostate</span><span class="p">[,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">6</span><span class="p">,</span><span class="w"> </span><span class="m">9</span><span class="p">)]</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">pros2</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  X     lcavol age svi pgg45
1 1 -0.5798185  50   0     0
2 2 -0.9942523  58   0     0
3 3 -0.5108256  74   0    20
4 4 -1.2039728  58   0     0
5 5  0.7514161  62   0     0
6 6 -1.0498221  50   0     0
</code></pre></div></div>

<h1 id="performing-exploratory-factor-analysis">Performing exploratory factor analysis</h1>

<p>EFA may be implemented in R using the <code class="language-plaintext highlighter-rouge">factanal()</code> function
from the <strong><code class="language-plaintext highlighter-rouge">stats</code></strong> package (which is a built-in package in base R). This
function fits a factor analysis by maximising the log-likelihood using a
data matrix as input. The number of factors to be fitted in the analysis
is specified by the user using the <code class="language-plaintext highlighter-rouge">factors</code> argument. Options for
transforming the factors by rotating the data in different ways are
available via the <code class="language-plaintext highlighter-rouge">rotation</code> argument (default is ‘none’).</p>

<blockquote class="challenge">
  <h2 id="challenge-1-3-mins">Challenge 1 (3 mins)</h2>

  <p>Use the <code class="language-plaintext highlighter-rouge">factanal()</code> function to identify the minimum number of factors
necessary to explain most of the variation in the data</p>

  <blockquote class="solution">
    <h2 id="solution">Solution</h2>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Include one factor only</span><span class="w">
</span><span class="n">pros_fa</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">factanal</span><span class="p">(</span><span class="n">pros2</span><span class="p">,</span><span class="w"> </span><span class="n">factors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">pros_fa</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Call:
factanal(x = pros2, factors = 1)

Uniquenesses:
     X lcavol    age    svi  pgg45 
 0.279  0.321  0.933  0.548  0.689 

Loadings:
       Factor1
X      0.849  
lcavol 0.824  
age    0.258  
svi    0.673  
pgg45  0.558  

               Factor1
SS loadings      2.229
Proportion Var   0.446

Test of the hypothesis that 1 factor is sufficient.
The chi square statistic is 6.2 on 5 degrees of freedom.
The p-value is 0.287 
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># p-value &lt;0.05 suggests that one factor is not sufficient </span><span class="w">
</span><span class="c1"># we reject the null hypothesis that one factor captures full</span><span class="w">
</span><span class="c1"># dimensionality in the dataset</span><span class="w">

</span><span class="c1"># Include two factors</span><span class="w">
</span><span class="n">pros_fa</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">factanal</span><span class="p">(</span><span class="n">pros2</span><span class="p">,</span><span class="w"> </span><span class="n">factors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">pros_fa</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Call:
factanal(x = pros2, factors = 2)

Uniquenesses:
     X lcavol    age    svi  pgg45 
 0.256  0.319  0.912  0.554  0.005 

Loadings:
       Factor1 Factor2
X      0.825   0.254  
lcavol 0.788   0.247  
age    0.171   0.242  
svi    0.584   0.324  
pgg45  0.248   0.966  

               Factor1 Factor2
SS loadings      1.732   1.222
Proportion Var   0.346   0.244
Cumulative Var   0.346   0.591

Test of the hypothesis that 2 factors are sufficient.
The chi square statistic is 0.99 on 1 degree of freedom.
The p-value is 0.32 
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># p-value &gt;0.05 suggests that two factors is sufficient </span><span class="w">
</span><span class="c1"># we cannot reject the null hypothesis that two factors captures</span><span class="w">
</span><span class="c1"># full dimensionality in the dataset</span><span class="w">

</span><span class="c1">#Include three factors</span><span class="w">
</span><span class="n">pros_fa</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">factanal</span><span class="p">(</span><span class="n">pros2</span><span class="p">,</span><span class="w"> </span><span class="n">factors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext error highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Error in factanal(pros2, factors = 3): 3 factors are too many for 5 variables
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Error shows that fitting three factors are not appropriate</span><span class="w">
</span><span class="c1"># for only 5 variables (number of factors too high)</span><span class="w">
</span></code></pre></div>    </div>
  </blockquote>
</blockquote>

<p>The output of <code class="language-plaintext highlighter-rouge">factanal()</code> shows the loadings for each of the input variables
associated with each factor. The loadings are values between -1 and 1 which
represent the relative contribution each input variable makes to the factors.
Positive values show that these variables are positively related to the
factors, while negative values show a negative relationship between variables
and factors. Loading values are missing for some variables because R does not
print loadings less than 0.1.</p>

<p>There are numerous ways to select the “best” number of factors. One is to use
the minimum number of features that does not leave a significant amount of
variance unaccounted for. In practise, we repeat the factor
analysis using different values in the <code class="language-plaintext highlighter-rouge">factors</code> argument. If we have an
idea of how many factors there will be before analysis, we can start with
that number. The final section of the analysis output shows the results of
a hypothesis test in which the null hypothesis is that the number of factors
used in the model is sufficient to capture most of the variation in the
dataset. If the p-value is less than 0.05, we reject the null hypothesis
and accept that the number of factors included is too small. If the p-value
is greater than 0.05, we accept the null hypothesis that the number of
factors used captures variation in the data.</p>

<p>Like PCA, the fewer factors that can explain most of the variation in the
dataset, the better. It is easier to explore and interpret results using a
smaller number of factors which represent underlying features in the data.</p>

<h1 id="variance-accounted-for-by-factors---communality-and-uniqueness">Variance accounted for by factors - communality and uniqueness</h1>

<p>The <em>communality</em> of a variable is the sum of its squared loadings. It
represents the proportion of the variance in a variable that is accounted
for by the FA model.</p>

<p><em>Uniqueness</em> is the opposite of communality and represents the amount of
variation in a variable that is not accounted for by the FA model. Uniqueness is
calculated by subtracting the communality value from 1. If uniqueness is high for
a given variable, that means this variable is not well explaind/accounted for
by the factors identified.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">apply</span><span class="p">(</span><span class="n">pros_fa</span><span class="o">$</span><span class="n">loadings</span><span class="o">^</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">sum</span><span class="p">)</span><span class="w">  </span><span class="c1">#communality</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>         X     lcavol        age        svi      pgg45 
0.74437666 0.68122690 0.08759426 0.44575518 0.99500020 
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="m">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">apply</span><span class="p">(</span><span class="n">pros_fa</span><span class="o">$</span><span class="n">loadings</span><span class="o">^</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">sum</span><span class="p">)</span><span class="w">  </span><span class="c1">#uniqueness</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        X    lcavol       age       svi     pgg45 
0.2556233 0.3187731 0.9124057 0.5542448 0.0049998 
</code></pre></div></div>

<h1 id="visualising-the-contribution-of-each-variable-to-the-factors">Visualising the contribution of each variable to the factors</h1>
<p>Similar to a biplot as we produced in the PCA episode, we can “plot the
loadings”. This shows how each original variable contributes to each of
the factors we chose to visualise.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#First, carry out factor analysis using two factors</span><span class="w">
</span><span class="n">pros_fa</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">factanal</span><span class="p">(</span><span class="n">pros2</span><span class="p">,</span><span class="w"> </span><span class="n">factors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w">

</span><span class="c1">#plot loadings for each factor</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="w">
  </span><span class="n">pros_fa</span><span class="o">$</span><span class="n">loadings</span><span class="p">[,</span><span class="w"> </span><span class="m">1</span><span class="p">],</span><span class="w"> 
  </span><span class="n">pros_fa</span><span class="o">$</span><span class="n">loadings</span><span class="p">[,</span><span class="w"> </span><span class="m">2</span><span class="p">],</span><span class="w">
  </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Factor 1"</span><span class="p">,</span><span class="w"> 
  </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Factor 2"</span><span class="p">,</span><span class="w"> 
  </span><span class="n">ylim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w">
  </span><span class="n">xlim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w">
  </span><span class="n">main</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Factor analysis of prostate data"</span><span class="w">
</span><span class="p">)</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">

</span><span class="c1">#add column names to each point</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="w">
  </span><span class="n">pros_fa</span><span class="o">$</span><span class="n">loadings</span><span class="p">[,</span><span class="w"> </span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">0.08</span><span class="p">,</span><span class="w"> 
  </span><span class="n">pros_fa</span><span class="o">$</span><span class="n">loadings</span><span class="p">[,</span><span class="w"> </span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0.08</span><span class="p">,</span><span class="w">
  </span><span class="n">colnames</span><span class="p">(</span><span class="n">pros2</span><span class="p">),</span><span class="w">
  </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"blue"</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-06-biplot-1.png" alt="plot of chunk biplot" width="432" />
<p class="caption">plot of chunk biplot</p>
</div>

<blockquote class="challenge">
  <h2 id="challenge-2-3-mins">Challenge 2 (3 mins)</h2>

  <p>Use the output from your factor analysis and the plots above to interpret
the results of your analysis.</p>

  <p>What variables are most important in explaining each factor? Do you think
this makes sense biologically? Discuss in groups.</p>

  <blockquote class="solution">
    <h2 id="solution-1">Solution</h2>

    <p>This plot suggests that the variables lweight and lbph are associated with
high values on factor 2 (but lower values on factor 1) and the variables
lcavol, lcp and lpsa are associated with high values on factor 1
(but lower values on factor 2). There appear to be two ‘clusters’ of
variables which can be represented by the two factors.</p>

    <p>The grouping of weight and enlargement (lweight and lbph) makes sense
biologically, as we would expect prostate enlargement to be associated
with greater weight. The groupings of lcavol, lcp, and lpsa also make
sense biologically, as larger cancer volume may be expected to be
associated with greater cancer spead and therefore higher PSA in the blood.</p>
  </blockquote>
</blockquote>

<h1 id="further-reading">Further reading</h1>

<ul>
  <li>Gundogdu et al. (2019) Comparison of performances of Principal Component Analysis (PCA) and Factor Analysis (FA) methods on the identification of cancerous and healthy colon tissues. International Journal of Mass Spectrometry 445:116204.</li>
  <li>Kustra et al. (2006) A factor analysis model for functional genomics. BMC Bioinformatics 7: doi:10.1186/1471-2105-7-21.</li>
  <li>Yong, A.G. &amp; Pearce, S. (2013) A beginner’s guide to factor analysis: focusing on exploratory factor analysis. Tutorials in Quantitative Methods for Psychology 9(2):79-94.</li>
  <li>Confirmatory factor analysis can be carried out with the package <a href="https://www.lavaan.ugent.be/index.html">Lavaan</a>.</li>
  <li>A more sophisticated implementation of EFA is available in the packages <a href="https://cran.r-project.org/web/packages/EFA.dimensions/index.html">EFA.dimensions</a> and <a href="https://personality-project.org/r/psych/">psych</a>.</li>
</ul>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>Factor analysis is a method used for reducing dimensionality in a dataset by reducing variation contained in multiple variables into a smaller number of uncorrelated factors.</p>
</li>
    
    <li><p>PCA can be used to identify the number of factors to initially use in factor analysis.</p>
</li>
    
    <li><p>The <code class="language-plaintext highlighter-rouge">factanal()</code> function in R can be used to fit a factor analysis, where the number of factors is specified by the user.</p>
</li>
    
    <li><p>Factor analysis can take into account expert knowledge when deciding on the number of factors to use, but a disadvantage is that the output requires careful interpretation.</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="k-means" class="maintitle">K-means</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 45 min
      <br />
      <strong>Exercises:</strong> 15 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>How do we detect real clusters in high-dimensional data?</p>
</li>
	
	<li><p>How does K-means work and when should it be used?</p>
</li>
	
	<li><p>How can we perform K-means in <code class="language-plaintext highlighter-rouge">R</code>?</p>
</li>
	
	<li><p>How can we appraise a clustering and test cluster robustness?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Understand the importance of clustering in high-dimensional data</p>
</li>
	
	<li><p>Understand and perform K-means clustering in <code class="language-plaintext highlighter-rouge">R</code>.</p>
</li>
	
	<li><p>Assess clustering performance using silhouette scores.</p>
</li>
	
	<li><p>Assess cluster robustness using bootstrapping.</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h1 id="introduction">Introduction</h1>

<p>High-dimensional data, especially in biological settings,  has
many sources of heterogeneity. Some of these are stochastic variation
arising from measurement error or random differences between organisms. 
In some cases, a known grouping causes this heterogeneity (sex, treatment
groups, etc). In other cases, this heterogeneity arises from the presence of
unknown subgroups in the data. <strong>Clustering</strong> is a set of techniques that allows
us to discover unknown groupings like this, which we can often use to
discover the nature of the heterogeneity we’re investigating.</p>

<p><strong>Cluster analysis</strong> involves finding groups of observations that are more
similar to each other (according to some feature) than they are to observations
in other groups. Cluster analysis is a useful statistical tool for exploring
high-dimensional datasets as 
visualising data with large numbers of features is difficult. It is commonly
used in fields such as bioinformatics, genomics, and image processing in which
large datasets that include many features are often produced. Once groups
(or clusters) of observations have been identified using cluster analysis,
further analyses or interpretation can be carried out on the groups, for
example, using metadata to further explore groups.</p>

<p>There are various ways to look for clusters of observations in a dataset using
different <em>clustering algorithms</em>. One way of clustering data is to minimise
distance between observations within a cluster and maximise distance between
proposed clusters. Clusters can be updated in an iterative process so that over
time we can become more confident in size and shape of clusters.</p>

<h1 id="believing-in-clusters">Believing in clusters</h1>

<p>When using clustering, it’s important to realise that data may seem to
group together even when these groups are created randomly. It’s especially 
important to remember this when making plots that add extra visual aids to
distinguish clusters. 
For example, if we cluster data from a single 2D normal distribution and draw
ellipses around the points, these clusters suddenly become almost visually
convincing. This is a somewhat extreme example, since there is genuinely no
heterogeneity in the data, but it does reflect what can happen if you allow
yourself to read too much into faint signals.</p>

<p>Let’s explore this further using an example. We create two columns of data
(‘x’ and ‘y’) and partition these data into three groups (‘a’, ‘b’, ‘c’)
according to data values. We then plot these data and their allocated clusters
and put ellipses around the clusters using the <code class="language-plaintext highlighter-rouge">stat_ellipse</code> function
in <code class="language-plaintext highlighter-rouge">ggplot</code>.</p>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-08-fake-cluster-1.png" alt="plot of chunk fake-cluster" width="432" />
<p class="caption">plot of chunk fake-cluster</p>
</div>
<p>The randomly created data used here appear to form three clusters when we
plot the data. Putting ellipses around the clusters can further convince us
that the clusters are ‘real’. But how do we tell if clusters identified
visually are ‘real’?</p>

<h1 id="what-is-k-means-clustering">What is K-means clustering?</h1>

<p><strong>K-means clustering</strong> is a clustering method which groups data points into a 
user-defined number of distinct non-overlapping clusters. In K-means clustering 
we are interested in minimising the <em>within-cluster variation</em>. This is the amount that
data points within a cluster differ from each other. In K-means clustering, the distance 
between data points within a cluster is used as a measure of within-cluster variation.
Using a specified clustering algorithm like K-means clustering increases our confidence
that our data can be partitioned into groups.</p>

<p>To carry out K-means clustering, we first pick $k$ initial points as centres or 
“centroids” of our clusters. There are a few ways to choose these initial “centroids”,
but for simplicity let’s imagine we just pick three random co-ordinates.
We then follow these two steps until convergence:</p>

<ol>
  <li>Assign each data point to the cluster with the closest centroid</li>
  <li>Update centroid positions as the average of the points in that cluster</li>
</ol>

<p>We can see this process in action in this animation:</p>

<div class="figure" style="text-align: center">
<img src="../fig/kmeans.gif" alt="Alt" />
<p class="caption">Cap</p>
</div>
<p>While K-means has some advantages over other clustering methods (easy to implement and
to understand), it does have some disadvantages, namely difficulties in identifying 
initial clusters which observations belong to and the need for the user to specifiy the
number of clusters that the data should be partitioned into.</p>

<blockquote class="callout">
  <h2 id="initialisation">Initialisation</h2>

  <p>The algorithm used in K-means clustering finds a <em>local</em> rather than a
<em>global</em> optimum, so that results of clustering are dependent on the initial
cluster that each observation is randomly assigned to. This initial
configuration can have a significant effect on the final configuration of the
clusters, so dealing with this limitation is an important part 
of K-means clustering. Some strategies to deal with this problem are:</p>
  <ul>
    <li>Choose $K$ points at random from the data as the cluster centroids.</li>
    <li>Randomly split the data into $K$ groups, and then average these groups.</li>
    <li>Use the K-means++ algorithm to choose initial values.</li>
  </ul>

  <p>These each have advantages and disadvantages. In general, it’s good to be
aware of this limitation of K-means clustering and that this limitation can
be addressed by choosing a good initialisation method, initialising clusters
manually, or running the algorithm from multiple different starting points.</p>

</blockquote>

<h1 id="k-means-clustering-applied-to-single-cell-rnaseq-data">K-means clustering applied to single-cell RNAseq data</h1>

<p>Let’s carry out K-means clustering in <code class="language-plaintext highlighter-rouge">R</code> using some real high-dimensional data.
We’re going to work with single-cell RNAseq data in these clustering challenges,
which is often <em>very</em> high-dimensional. Commonly, experiments profile the
expression level of 10,000+ genes in thousands of cells. Even after filtering
the data to remove low quality observations, the dataset we’re using in this
episode contains measurements for over 9,000 genes in over 3,000 cells.</p>

<p>One way to get a handle on a dataset of this size is to use something we covered
earlier in the course - dimensionality reduction. Dimensionality reduction
allows us to visualise this incredibly complex data in a small number of
dimensions. In this case, we’ll be using principal component analysis (PCA) to
compress the data by identifying the major axes of variation in the data,
before running our clustering algorithms on this lower-dimensional data.</p>

<p>The <code class="language-plaintext highlighter-rouge">scater</code> package has some easy-to-use tools to calculate a PCA for
<code class="language-plaintext highlighter-rouge">SummarizedExperiment</code> objects.
Let’s load the <code class="language-plaintext highlighter-rouge">scRNAseq</code> data and calculate some principal components.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"SingleCellExperiment"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"scater"</span><span class="p">)</span><span class="w">

</span><span class="n">scrnaseq</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readRDS</span><span class="p">(</span><span class="n">here</span><span class="o">::</span><span class="n">here</span><span class="p">(</span><span class="s2">"data/scrnaseq.rds"</span><span class="p">))</span><span class="w">
</span><span class="n">scrnaseq</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">runPCA</span><span class="p">(</span><span class="n">scrnaseq</span><span class="p">,</span><span class="w"> </span><span class="n">ncomponents</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">15</span><span class="p">)</span><span class="w">
</span><span class="n">pcs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">reducedDim</span><span class="p">(</span><span class="n">scrnaseq</span><span class="p">)[,</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">2</span><span class="p">]</span><span class="w">
</span></code></pre></div></div>
<p>The first two principal components capture almost 50% of the variation within
the data. For now, we’ll work with just these two principal components, since
we can visualise those easily, and they’re a quantitative representation of
the underlying data, representing the two largest axes of variation.</p>

<p>We can now run K-means clustering on the first and second principal components
of the <code class="language-plaintext highlighter-rouge">scRNAseq</code> data using the <code class="language-plaintext highlighter-rouge">kmeans</code> function.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">cluster</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">kmeans</span><span class="p">(</span><span class="n">pcs</span><span class="p">,</span><span class="w"> </span><span class="n">centers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="p">)</span><span class="w">
</span><span class="n">scrnaseq</span><span class="o">$</span><span class="n">kmeans</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">as.character</span><span class="p">(</span><span class="n">cluster</span><span class="o">$</span><span class="n">cluster</span><span class="p">)</span><span class="w">
</span><span class="n">plotReducedDim</span><span class="p">(</span><span class="n">scrnaseq</span><span class="p">,</span><span class="w"> </span><span class="s2">"PCA"</span><span class="p">,</span><span class="w"> </span><span class="n">colour_by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"kmeans"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-08-kmeans-1.png" alt="Alt" width="432" />
<p class="caption">Title</p>
</div>

<p>We can see that this produces a sensible-looking partition of the data. 
However, is it totally clear whether there might be more or fewer clusters
here?</p>

<blockquote class="challenge">
  <h2 id="challenge-1">Challenge 1</h2>

  <p>Cluster the data using a $K$ of 5, and plot it using <code class="language-plaintext highlighter-rouge">plotReducedDim</code>.
Save this with a variable name that’s different to what we just used,
because we’ll use this again later.</p>

  <blockquote class="solution">
    <h2 id="solution">Solution</h2>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">cluster5</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">kmeans</span><span class="p">(</span><span class="n">pcs</span><span class="p">,</span><span class="w"> </span><span class="n">centers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w">
</span><span class="n">scrnaseq</span><span class="o">$</span><span class="n">kmeans5</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">as.character</span><span class="p">(</span><span class="n">cluster5</span><span class="o">$</span><span class="n">cluster</span><span class="p">)</span><span class="w">
</span><span class="n">plotReducedDim</span><span class="p">(</span><span class="n">scrnaseq</span><span class="p">,</span><span class="w"> </span><span class="s2">"PCA"</span><span class="p">,</span><span class="w"> </span><span class="n">colour_by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"kmeans5"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-08-kmeans-ex-1.png" alt="plot of chunk kmeans-ex" width="432" />
<p class="caption">plot of chunk kmeans-ex</p>
</div>

  </blockquote>
</blockquote>

<blockquote class="callout">
  <h2 id="k-medoids-pam">K-medoids (PAM)</h2>

  <p>One problem with K-means is that using the mean to define cluster centroids
means that clusters can be very sensitive to outlying observations.
K-medoids, also known as “partitioning around medoids (PAM)” is similar to 
K-means, but uses the median rather than the mean as the method for defining
cluster centroids. Using the median rather than the mean reduces sensitivity of
clusters to outliers in the data. K-medioids has had popular application in
genomics, for example the well-known PAM50 gene set in breast cancer, which has seen some 
prognostic applications.
The following example shows how cluster centroids differ when created using 
medians rather than means.</p>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="m">20</span><span class="p">)</span><span class="w">
</span><span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="m">20</span><span class="p">)</span><span class="w">
</span><span class="n">x</span><span class="p">[</span><span class="m">10</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="m">10</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">10</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">16</span><span class="p">)</span><span class="w">
</span><span class="n">points</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">),</span><span class="w"> </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">16</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"firebrick"</span><span class="p">)</span><span class="w">
</span><span class="n">points</span><span class="p">(</span><span class="n">median</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="w"> </span><span class="n">median</span><span class="p">(</span><span class="n">y</span><span class="p">),</span><span class="w"> </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">16</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dodgerblue"</span><span class="p">)</span><span class="w">
</span></code></pre></div>  </div>

  <div class="figure" style="text-align: center">
<img src="../fig/rmd-08-unnamed-chunk-1-1.png" alt="plot of chunk unnamed-chunk-1" width="432" />
<p class="caption">plot of chunk unnamed-chunk-1</p>
</div>
  <p>PAM can be carried out using <code class="language-plaintext highlighter-rouge">pam()</code> form the <strong><code class="language-plaintext highlighter-rouge">cluster</code></strong> package.</p>

</blockquote>

<h1 id="cluster-separation">Cluster separation</h1>
<p>When performing clustering, it is important for us to be able to measure
how well our clusters are separated. One measure to test this is silhouette width.
This is a number that is computed for every observation. It can range from -1 to 1.
A high silhouette width means an observation is closer to other observations
within the same cluster. For each cluster, the silhouette widths can then be
averaged or an overall average can be taken.</p>

<blockquote class="callout">
  <h2 id="more-detail-on-silhouette-widths">More detail on silhouette widths</h2>
  <p>In more detail, each observation’s silhouette width is computed as follows:</p>
  <ol>
    <li>Compute the average distance between the focal observation and all other
observations in the same cluster.</li>
    <li>For each of the other clusters, compute the average distance between
focal observation and all observations in the other cluster. Keep the
smallest of these average distances.</li>
    <li>Subtract (1.)-(2.) then divivde by whichever is smaller (1.) or (2).</li>
  </ol>
</blockquote>

<p>Ideally, we would have only large positive silhouette widths, indicating
that each data point is much more similar to points within its cluster than it
is to the points in any other cluster. However, this is rarely the case. Often,
clusters are very fuzzy, and even if we are relatively sure about the existence
of discrete groupings in the data, observations on the boundaries can be difficult
to confidently place in either cluster.</p>

<p>Here we use the <code class="language-plaintext highlighter-rouge">silhouette</code> function from the <code class="language-plaintext highlighter-rouge">cluster</code> package to calculate the
silhouette width of our K-means clustering using a distance matrix of distances
between points in the clusters.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"cluster"</span><span class="p">)</span><span class="w">
</span><span class="n">dist_mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dist</span><span class="p">(</span><span class="n">pcs</span><span class="p">)</span><span class="w">
</span><span class="n">sil</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">silhouette</span><span class="p">(</span><span class="n">cluster</span><span class="o">$</span><span class="n">cluster</span><span class="p">,</span><span class="w"> </span><span class="n">dist</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dist_mat</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">sil</span><span class="p">,</span><span class="w"> </span><span class="n">border</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NA</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-08-silhouette-1.png" alt="plot of chunk silhouette" width="432" />
<p class="caption">plot of chunk silhouette</p>
</div>

<p>Let’s plot the silhouette score on the original dimensions used to cluster
the data. Here, we’re mapping cluster membership to point shape, and silhouette
width to colour.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pc</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.data.frame</span><span class="p">(</span><span class="n">pcs</span><span class="p">)</span><span class="w">
</span><span class="n">colnames</span><span class="p">(</span><span class="n">pc</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"x"</span><span class="p">,</span><span class="w"> </span><span class="s2">"y"</span><span class="p">)</span><span class="w">
</span><span class="n">pc</span><span class="o">$</span><span class="n">sil</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sil</span><span class="p">[,</span><span class="w"> </span><span class="s2">"sil_width"</span><span class="p">]</span><span class="w">
</span><span class="n">pc</span><span class="o">$</span><span class="n">clust</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">factor</span><span class="p">(</span><span class="n">cluster</span><span class="o">$</span><span class="n">cluster</span><span class="p">)</span><span class="w">
</span><span class="n">mean</span><span class="p">(</span><span class="n">sil</span><span class="p">[,</span><span class="w"> </span><span class="s2">"sil_width"</span><span class="p">])</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 0.7065662
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ggplot</span><span class="p">(</span><span class="n">pc</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sil</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_point</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">scale_colour_gradient2</span><span class="p">(</span><span class="w">
        </span><span class="n">low</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dodgerblue"</span><span class="p">,</span><span class="w"> </span><span class="n">high</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"firebrick"</span><span class="w">
    </span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">scale_shape_manual</span><span class="p">(</span><span class="w">
        </span><span class="n">values</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">setNames</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">4</span><span class="p">)</span><span class="w">
    </span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-08-plot-silhouette-1.png" alt="plot of chunk plot-silhouette" width="432" />
<p class="caption">plot of chunk plot-silhouette</p>
</div>

<p>This plot shows that silhouette values for individual observations tends to be
very high in the centre of clusters, but becomes quite low towards the edges.
This makes sense, as points that are “between” two clusters may be more similar
to points in another cluster than they are to the points in the cluster one they
belong to.</p>

<blockquote class="challenge">
  <h2 id="challenge-2">Challenge 2</h2>

  <p>Calculate the silhouette width for the K of 5 clustering we did earlier.
Is it better or worse than before?</p>

  <p>Can you identify where the differences lie?</p>

  <blockquote class="solution">
    <h2 id="solution-1">Solution</h2>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sil5</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">silhouette</span><span class="p">(</span><span class="n">cluster5</span><span class="o">$</span><span class="n">cluster</span><span class="p">,</span><span class="w"> </span><span class="n">dist</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dist_mat</span><span class="p">)</span><span class="w">
</span><span class="n">scrnaseq</span><span class="o">$</span><span class="n">kmeans5</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">as.character</span><span class="p">(</span><span class="n">cluster5</span><span class="o">$</span><span class="n">cluster</span><span class="p">)</span><span class="w">
</span><span class="n">plotReducedDim</span><span class="p">(</span><span class="n">scrnaseq</span><span class="p">,</span><span class="w"> </span><span class="s2">"PCA"</span><span class="p">,</span><span class="w"> </span><span class="n">colour_by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"kmeans5"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-08-silhouette-ex-1.png" alt="plot of chunk silhouette-ex" width="432" />
<p class="caption">plot of chunk silhouette-ex</p>
</div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mean</span><span class="p">(</span><span class="n">sil5</span><span class="p">[,</span><span class="w"> </span><span class="s2">"sil_width"</span><span class="p">])</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 0.5849979
</code></pre></div>    </div>
    <p>The average silhouette width is lower when k=5.</p>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">sil5</span><span class="p">,</span><span class="w"> </span><span class="n">border</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NA</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/silhouette5.png" alt="plot of chunk unnamed-chunk-4" />
<p class="caption">plot of chunk unnamed-chunk-4</p>
</div>
    <p>This seems to be because some observations in clusters 3 and 5 seem to be
more similar to other clusters than the one they have been assigned to.
This may indicate that K is too high.</p>
  </blockquote>
</blockquote>

<blockquote class="callout">
  <h2 id="gap-statistic">Gap statistic</h2>

  <p>Another measure of how good our clustering is is the “gap statistic”.
This compares the observed squared distance between observations in a cluster
and the centre of the cluster to an “expected” squared distances.
The expected distances are calculated by randomly distributing cells within
the range of the original data. Larger values represent lower
squared distances within clusters, and thus better clustering.
We can see how this is calculated in the following example.</p>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"cluster"</span><span class="p">)</span><span class="w">
</span><span class="n">gaps</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">clusGap</span><span class="p">(</span><span class="n">pcs</span><span class="p">,</span><span class="w"> </span><span class="n">kmeans</span><span class="p">,</span><span class="w"> </span><span class="n">K.max</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">iter.max</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">)</span><span class="w">
</span><span class="n">best_k</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">maxSE</span><span class="p">(</span><span class="n">gaps</span><span class="o">$</span><span class="n">Tab</span><span class="p">[,</span><span class="w"> </span><span class="s2">"gap"</span><span class="p">],</span><span class="w"> </span><span class="n">gaps</span><span class="o">$</span><span class="n">Tab</span><span class="p">[,</span><span class="w"> </span><span class="s2">"SE.sim"</span><span class="p">])</span><span class="w">
</span><span class="n">best_k</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">gaps</span><span class="o">$</span><span class="n">Tab</span><span class="p">[,</span><span class="s2">"gap"</span><span class="p">],</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Number of clusters"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Gap statistic"</span><span class="p">)</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">best_k</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"red"</span><span class="p">)</span><span class="w">
</span></code></pre></div>  </div>
</blockquote>

<h1 id="cluster-robustness">Cluster robustness</h1>

<p>When we cluster data, we want to be sure that the clusters we identify are
not a result of the exact properties of the input data. That is, if the
data we observed were slightly different, the clusters we would identify
in this different data would be very similar. This makes it more
likely that these can be reproduced.</p>

<p>To assess this, we can use the <em>bootstrap</em>. What we do here is to take a sample
from the data with replacement. Sampling with replacement means that in the 
sample that we take, we can include points from the input data more than once.
This is maybe easier to see with an example. First, we define some data:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">5</span><span class="w">
</span></code></pre></div></div>

<p>Then, we can take a sample from this data without replacement:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 4 1 3 5 2
</code></pre></div></div>

<p>This sample is a subset of the original data, and points are only present once.
This is the case every time even if we do it many times:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Each column is a sample</span><span class="w">
</span><span class="n">replicate</span><span class="p">(</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    5    2    5    2    3    1    3    5    5     3
[2,]    4    5    4    5    4    4    1    3    1     2
[3,]    2    1    1    3    2    5    2    2    3     4
[4,]    1    4    2    1    5    3    5    1    2     5
[5,]    3    3    3    4    1    2    4    4    4     1
</code></pre></div></div>

<p>However, if we sample <em>with replacement</em>, then sometimes individual data points
are present more than once.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">replicate</span><span class="p">(</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    3    1    2    2    1    3    3    2    4     2
[2,]    1    3    2    4    2    5    2    1    2     5
[3,]    5    5    4    4    2    2    1    1    1     3
[4,]    1    1    4    2    1    4    4    5    5     4
[5,]    3    1    2    1    4    2    5    3    3     2
</code></pre></div></div>

<blockquote class="callout">
  <h2 id="bootstrapping">Bootstrapping</h2>

  <p>The bootstrap is a powerful and common statistical technique.</p>

  <p>We would like to know about the sampling distribution of a statistic,
but we don’t have any knowledge of its behaviour under the null hypothesis.</p>

  <p>For example, we might want to understand the uncertainty around an estimate
of the mean of our data. To do this, we could resample the data with
replacement and calculate the mean of each average.</p>

  <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">boots</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">replicate</span><span class="p">(</span><span class="m">1000</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)))</span><span class="w">
</span><span class="n">hist</span><span class="p">(</span><span class="n">boots</span><span class="p">,</span><span class="w">
    </span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"FD"</span><span class="p">,</span><span class="w">
    </span><span class="n">main</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"1,000 bootstrap samples"</span><span class="p">,</span><span class="w">
    </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Mean of sample"</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div>  </div>

  <div class="figure" style="text-align: center">
<img src="../fig/rmd-08-boots-1.png" alt="plot of chunk boots" width="432" />
<p class="caption">plot of chunk boots</p>
</div>

  <p>In this case, the example is simple, but it’s possible to
devise more complex statistical tests using this kind of approach.</p>

  <p>The bootstrap, along with permutation testing, can be a very flexible and 
general solution to many statistical problems.</p>

</blockquote>

<p>In applying the bootstrap to clustering, we want to see two things:</p>
<ol>
  <li>Will observations within a cluster consistently cluster together in
different bootstrap replicates?</li>
  <li>Will observations frequently swap between clusters?</li>
</ol>

<p>In the plot below, the diagonal of the plot shows how often the clusters
are reproduced in boostrap replicates. High scores on
the diagonal mean that the clusters are consistently reproduced in each 
boostrap replicate. Similarly, the off-diagonal elements represent how often
observations swap between clusters in bootstrap replicates. High scores 
indicate that observations rarely swap between clusters.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"pheatmap"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"bluster"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"viridis"</span><span class="p">)</span><span class="w">

</span><span class="n">km_fun</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">kmeans</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">centers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="p">)</span><span class="o">$</span><span class="n">cluster</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="n">ratios</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">bootstrapStability</span><span class="p">(</span><span class="n">pcs</span><span class="p">,</span><span class="w"> </span><span class="n">FUN</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">km_fun</span><span class="p">,</span><span class="w"> </span><span class="n">clusters</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cluster</span><span class="o">$</span><span class="n">cluster</span><span class="p">)</span><span class="w">
</span><span class="n">pheatmap</span><span class="p">(</span><span class="n">ratios</span><span class="p">,</span><span class="w">
    </span><span class="n">cluster_rows</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">cluster_cols</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w">
    </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">viridis</span><span class="p">(</span><span class="m">10</span><span class="p">),</span><span class="w">
    </span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">length.out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-08-bs-heatmap-1.png" alt="plot of chunk bs-heatmap" width="432" />
<p class="caption">plot of chunk bs-heatmap</p>
</div>

<p>Yellow boxes indicate values slightly greater than 1, which may be observed.
These are “good” (despite missing in the colour bar).</p>

<blockquote class="challenge">
  <h2 id="challenge-3">Challenge 3</h2>

  <p>Repeat the bootstrapping process with K=5. Are the results better or worse?
Can you identify where the differences occur on the <code class="language-plaintext highlighter-rouge">plotReducedDim</code>?</p>

  <blockquote class="solution">
    <h2 id="solution-2">Solution</h2>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">km_fun5</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">kmeans</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">centers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="o">$</span><span class="n">cluster</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">ratios5</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">bootstrapStability</span><span class="p">(</span><span class="n">pcs</span><span class="p">,</span><span class="w"> </span><span class="n">FUN</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">km_fun5</span><span class="p">,</span><span class="w"> </span><span class="n">clusters</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cluster5</span><span class="o">$</span><span class="n">cluster</span><span class="p">)</span><span class="w">
</span><span class="n">pheatmap</span><span class="p">(</span><span class="n">ratios5</span><span class="p">,</span><span class="w">
    </span><span class="n">cluster_rows</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">cluster_cols</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w">
    </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">viridis</span><span class="p">(</span><span class="m">10</span><span class="p">),</span><span class="w">
    </span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">length.out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-08-bs-ex-1.png" alt="plot of chunk bs-ex" width="432" />
<p class="caption">plot of chunk bs-ex</p>
</div>
    <p>When K=5, we can see that the values on the diagonal of the matrix are 
smaller, indicating that the clusters aren’t exactly reproducible in the
bootstrap samples.</p>

    <p>Similarly, the off-diagonal elements are considerably lower for some
elements.
This indicates that observations are “swapping” between these clusters
in bootstrap replicates.</p>
  </blockquote>
</blockquote>

<blockquote class="callout">
  <h2 id="consensus-clustering">Consensus clustering</h2>

  <p>One useful and generic method of clustering is <em>consensus clustering</em>.
This method can use k-means, or other clustering methods.</p>

  <p>The idea behind this is to bootstrap the data repeatedly, and cluster
it each time, perhaps using different numbers of clusters.
If a pair of data points always end up in the same cluster,
it’s likely that they really belong to the same underlying cluster.</p>

  <p>This is really computationally demanding but has been shown to perform very
well in some situations. It also allows you to visualise how cluster
membership changes over different values of K.</p>

</blockquote>

<blockquote class="callout">
  <h2 id="speed">Speed</h2>

  <p>It’s worth noting that a lot of the methods we’ve discussed here are very
computationally demanding. 
When clustering data, we may have to compare points to each other many times.
This becomes more and more difficult when we have many observations and
many features. This is especially problematic when we want to do things like
bootstrapping that requires us to cluster the data over and over.</p>

  <p>As a result, there are a lot of approximate methods for finding clusters
in the data. For example, the
<a href="http://www.bioconductor.org/packages/3.13/bioc/html/mbkmeans.html">mbkmeans</a>
package includes an algorithm for clustering extremely large data. The idea
behind this algorithm
is that if the clusters we find are robust, we don’t need to look at all of
the data every time. This is very helpful because it reduces the amount of
data that needs to be held in memory at once, but also because it minimises the 
computational cost.</p>

  <p>Similarly, approximate nearest neighbour methods like 
<a href="https://pypi.org/project/annoy/">Annoy</a> can be used to identify what the
$K$ closest points are in the data, and this can be used in some clustering 
methods (for example, graph-based clustering).</p>

  <p>Generally, these methods sacrifice a bit of accuracy for a big gain in speed.</p>
</blockquote>

<h2 id="further-reading">Further reading</h2>

<ul>
  <li><a href="https://doi.org/10.1007/978-3-642-29807-3_1">Wu, J. (2012) Cluster analysis and K-means clustering: An Introduction. In: Advances in K-means Clustering. Springer Berlin, Heidelberg.</a>.</li>
  <li><a href="https://web.stanford.edu/class/bios221/book/Chap-Clustering.html">Modern statistics for modern biology, Susan Holmes and Wolfgang Huber (Chapter 5)</a>.</li>
  <li><a href="https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1">Understanding K-means clustering in machine learning, Towards Data Science</a>.</li>
</ul>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>K-means is an intuitive algorithm for clustering data.</p>
</li>
    
    <li><p>K-means has various advantages but can be computationally intensive.</p>
</li>
    
    <li><p>Apparent clusters in high-dimensional data should always be treated with some scepticism.</p>
</li>
    
    <li><p>Silhouette width and bootstrapping can be used to assess how well our clustering algorithm has worked.</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="hierarchical-clustering" class="maintitle">Hierarchical clustering</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 60 min
      <br />
      <strong>Exercises:</strong> 10 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>What is hierarchical clustering and how does it differ from other clustering methods?</p>
</li>
	
	<li><p>How do we carry out hierarchical clustering in R?</p>
</li>
	
	<li><p>What distance matrix and linkage methods should we use?</p>
</li>
	
	<li><p>How can we validate identified clusters?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Understand when to use hierarchical clustering on high-dimensional data.</p>
</li>
	
	<li><p>Perform hierarchical clustering on high-dimensional data and evaluate dendrograms.</p>
</li>
	
	<li><p>Explore different distance matrix and linkage methods.</p>
</li>
	
	<li><p>Use the Dunn index to validate clustering methods.</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h1 id="why-use-hierarchical-clustering-on-high-dimensional-data">Why use hierarchical clustering on high-dimensional data?</h1>

<p>When analysing high-dimensional data in the life sciences, it is often useful
to identify groups of similar data points to understand more about the relationships
within the dataset. In <em>hierarchical clustering</em> an algorithm groups similar
data points (or observations) into groups (or clusters). This results in a set
of clusters, where each cluster is distinct, and the data points within each
cluster have similar characteristics. The clustering algorithm works by iteratively
grouping data points so that different clusters may exist at different stages
of the algorithm’s progression.</p>

<p>Unlike K-means clustering, <em>hierarchical clustering</em> does not require the
number of clusters $k$ to be specified by the user before the analysis is carried
out. Hierarchical clustering also provides an attractive <em>dendrogram</em>, a
tree-like diagram showing the degree of similarity between clusters.</p>

<p>The dendrogram is a key feature of hierarchical clustering. This tree-shaped graph allows
the similarity between data points in a dataset to be visualised and the
arrangement of clusters produced by the analysis to be illustrated. Dendrograms are created
using a distance (or dissimilarity) that quantify how different are pairs of observations,
and a clustering algorithm to fuse groups of similar data points together.</p>

<p>In this episode we will explore hierarchical clustering for identifying
clusters in high-dimensional data. We will use <em>agglomerative</em> hierarchical
clustering (see box) in this episode.</p>

<blockquote class="callout">
  <h2 id="agglomerative-and-divisive-hierarchical-clustering">Agglomerative and Divisive hierarchical clustering</h2>

  <p>There are two main methods of carrying out hierarchical clustering:
agglomerative clustering and divisive clustering. 
The former is a ‘bottom-up’ approach to clustering whereby the clustering
approach begins with each data point (or observation) 
being regarded as being in its own separate cluster. Pairs of data points are
merged as we move up the tree. 
Divisive clustering is a ‘top-down’ approach in which all data points start
in a single cluster and an algorithm is used to split groups of data points
from this main group.</p>
</blockquote>

<h1 id="the-agglomerative-hierarchical-clustering-algorithm">The agglomerative hierarchical clustering algorithm</h1>

<p>To start with, we measure distance
(or dissimilarity) between pairs of observations. Initially, and at the bottom
of the dendrogram, each observation is considered to be in its own individual
cluster. We start the clustering procedure by fusing the two observations that
are most similar according to a distance matrix. Next, the next-most similar observations are fused
so that the total number of clusters is <em>number of observations</em> - 2 (see
panel below). Groups of observations may then be merged into a larger cluster
(see next panel below, green box). This process continues until all the observations are included
in a single cluster.</p>

<div class="figure" style="text-align: center">
<img src="../fig/hierarchical_clustering_1.png" alt="Figure 1a: Example data showing two clusters of observation pairs" width="500px" />
<p class="caption">Figure 1a: Example data showing two clusters of observation pairs</p>
</div>

<div class="figure" style="text-align: center">
<img src="../fig/hierarchical_clustering_2.png" alt="Figure 1b: Example data showing fusing of one observation into larger cluster" width="500px" />
<p class="caption">Figure 1b: Example data showing fusing of one observation into larger cluster</p>
</div>

<h1 id="a-motivating-example">A motivating example</h1>

<p>To motivate this lesson, let’s first look at an example where hierarchical
clustering is really useful, and then we can understand how to apply it in more
detail. To do this, we’ll return to the large methylation dataset we worked
with in the regression lessons. Let’s load the data and look at it.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"minfi"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"here"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"ComplexHeatmap"</span><span class="p">)</span><span class="w">

</span><span class="n">methyl</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readRDS</span><span class="p">(</span><span class="n">here</span><span class="p">(</span><span class="s2">"data/methylation.rds"</span><span class="p">))</span><span class="w">

</span><span class="c1"># transpose this Bioconductor dataset to show features in columns</span><span class="w">
</span><span class="n">methyl_mat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">assay</span><span class="p">(</span><span class="n">methyl</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p>Looking at a heatmap of these data, we may spot some patterns – many columns
appear to have a similar methylation levels across all rows. However, they are
all quite jumbled at the moment, so it’s hard to tell how many line up exactly.</p>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-heatmap-noclust-1.png" alt="plot of chunk heatmap-noclust" width="432" />
<p class="caption">plot of chunk heatmap-noclust</p>
</div>

<p>We can order these data to make the patterns more clear using hierarchical
clustering. To do this, we can change the arguments we pass to 
<code class="language-plaintext highlighter-rouge">Heatmap()</code> from the <strong><code class="language-plaintext highlighter-rouge">ComplexHeatmap</code></strong> package. <code class="language-plaintext highlighter-rouge">Heatmap()</code>
groups features based on dissimilarity (here, Euclidean distance) and orders
rows and columns to show clustering of features and observations.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Heatmap</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">,</span><span class="w">
  </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Methylation level"</span><span class="p">,</span><span class="w">
  </span><span class="n">cluster_rows</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">cluster_columns</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w">
  </span><span class="n">row_dend_width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">unit</span><span class="p">(</span><span class="m">0.2</span><span class="p">,</span><span class="w"> </span><span class="s2">"npc"</span><span class="p">),</span><span class="w">
  </span><span class="n">column_dend_height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">unit</span><span class="p">(</span><span class="m">0.2</span><span class="p">,</span><span class="w"> </span><span class="s2">"npc"</span><span class="p">),</span><span class="w">
  </span><span class="n">show_row_names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">show_column_names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w">
  </span><span class="n">row_title</span><span class="o">=</span><span class="s2">"Individuals"</span><span class="p">,</span><span class="w"> </span><span class="n">column_title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Methylation sites"</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-heatmap-clust-1.png" alt="plot of chunk heatmap-clust" width="432" />
<p class="caption">plot of chunk heatmap-clust</p>
</div>

<p>We can see that clustering the features (CpG sites) results in an overall
gradient of high to low methylation levels from left to right. Maybe more
interesting is the fact that the rows (corresponding to individuals) are now
grouped according to methylation patterns. For example, 12 samples seem to have
lower methylation levels for a small subset of CpG sites in the middle, relative
to all the other samples. It’s not clear without investigating further what the
cause of this is – it could be a batch effect, or a known grouping (e.g., old
vs young samples). However, clustering like this can be a useful part of
exploratory analysis of data to build hypotheses.</p>

<p>Now, let’s cover the inner workings of hierarchical clustering in more detail.
There are two things to consider before carrying out clustering:</p>
<ul>
  <li>how to define dissimilarity between observations using a distance matrix, and</li>
  <li>how to define dissimilarity between clusters and when to fuse separate clusters.</li>
</ul>

<h1 id="creating-the-distance-matrix">Creating the distance matrix</h1>
<p>Agglomerative hierarchical clustering is performed in two steps: calculating
the distance matrix (containing distances between pairs of observations) and
iteratively grouping observations into clusters using this matrix.</p>

<p>There are different ways to
specify a distance matrix for clustering:</p>

<ul>
  <li>Specify distance as a pre-defined option using the <code class="language-plaintext highlighter-rouge">method</code> argument in
<code class="language-plaintext highlighter-rouge">dist()</code>. Methods include <code class="language-plaintext highlighter-rouge">euclidean</code> (default), <code class="language-plaintext highlighter-rouge">maximum</code> and <code class="language-plaintext highlighter-rouge">manhattan</code>.</li>
  <li>Create a self-defined function which calculates distance from a matrix or
from two vectors. The function should only contain one argument.</li>
</ul>

<p>Of pre-defined methods of calculating the distance matrix, Euclidean is one of
the most commonly used. This method calculates the shortest straight-line
distances between pairs of observations.</p>

<p>Another option is to use a correlation matrix as the input matrix to the
clustering algorithm. The type of distance matrix used in hierarchical
clustering can have a big effect on the resulting tree. The decision of which
distance matrix to use before carrying out hierarchical clustering depends on the
type of data and question to be addressed.</p>

<h1 id="linkage-methods">Linkage methods</h1>

<p>The second step in performing hierarchical clustering after defining the
distance matrix (or another function defining similarity between data points)
is determining how to fuse different clusters.</p>

<p><em>Linkage</em> is used to define dissimilarity between groups of observations
(or clusters) and is used to create the hierarchical structure in the
dendrogram. Different linkage methods of creating a dendrogram are discussed
below.</p>

<p><code class="language-plaintext highlighter-rouge">hclust()</code> supports various linkage methods (e.g <code class="language-plaintext highlighter-rouge">complete</code>,
<code class="language-plaintext highlighter-rouge">single</code>, <code class="language-plaintext highlighter-rouge">ward D</code>, <code class="language-plaintext highlighter-rouge">ward D2</code>, <code class="language-plaintext highlighter-rouge">average</code>, <code class="language-plaintext highlighter-rouge">median</code>) and these are also supported
within the <code class="language-plaintext highlighter-rouge">Heatmap()</code> function. The method used to perform hierarchical
clustering in <code class="language-plaintext highlighter-rouge">Heatmap()</code> can be specified by the arguments
<code class="language-plaintext highlighter-rouge">clustering_method_rows</code> and <code class="language-plaintext highlighter-rouge">clustering_method_columns</code>. Each linkage method
uses a slightly different algorithm to calculate how clusters are fused together
and therefore different clustering decisions are made depending on the linkage
method used.</p>

<p>Complete linkage (the default in <code class="language-plaintext highlighter-rouge">hclust()</code>) works by computing all pairwise
dissimilarities between data points in different clusters. For each pair of two clusters,
it sets their dissimilarity ($d$) to the maximum dissimilarity value observed
between any of these clusters’ constituent points. The two clusters
with smallest value of $d$ are then fused.</p>

<h1 id="computing-a-dendrogram">Computing a dendrogram</h1>

<p>Dendograms are useful tools to visualise the grouping of points and clusters into bigger clusters.
We can create and plot dendrograms in R using <code class="language-plaintext highlighter-rouge">hclust()</code> which takes
a distance matrix as input and creates the associated tree using hierarchical
clustering. Here we create some example data to carry out hierarchical
clustering.</p>

<p>Let’s generate 20 data points in 2D space. Each
point belongs to one of three classes. Suppose we did not know which class
data points belonged to and we want to identify these via cluster analysis.
Hierarchical clustering carried out on the data can be used to produce a
dendrogram showing how the data is partitioned into clusters. But how do we
interpret this dendrogram? Let’s explore this using our example data.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#First, create some example data with two variables x1 and x2</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">450</span><span class="p">)</span><span class="w">
</span><span class="n">example_data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="w">
    </span><span class="n">x1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="m">8</span><span class="p">,</span><span class="w"> </span><span class="m">4.5</span><span class="p">),</span><span class="w">
    </span><span class="n">x2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="m">6</span><span class="p">,</span><span class="w"> </span><span class="m">3.4</span><span class="p">)</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="c1">#plot the data and name data points by row numbers</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">example_data</span><span class="o">$</span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">example_data</span><span class="o">$</span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"n"</span><span class="p">)</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="w">
    </span><span class="n">example_data</span><span class="o">$</span><span class="n">x1</span><span class="p">,</span><span class="w">
    </span><span class="n">example_data</span><span class="o">$</span><span class="n">x2</span><span class="p">,</span><span class="w">
    </span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rownames</span><span class="p">(</span><span class="n">example_data</span><span class="p">),</span><span class="w">
    </span><span class="n">cex</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.7</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plotexample-1.png" alt="plot of chunk plotexample" width="432" />
<p class="caption">plot of chunk plotexample</p>
</div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## calculate distance matrix using euclidean distance</span><span class="w">
</span><span class="n">dist_m</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dist</span><span class="p">(</span><span class="n">example_data</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"euclidean"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<blockquote class="challenge">
  <h2 id="challenge-1">Challenge 1</h2>

  <p>Use <code class="language-plaintext highlighter-rouge">hclust()</code> to implement hierarchical clustering using the
distance matrix <code class="language-plaintext highlighter-rouge">dist_m</code> and 
the <code class="language-plaintext highlighter-rouge">complete</code> linkage method and plot the results as a dendrogram using
<code class="language-plaintext highlighter-rouge">plot()</code>.</p>

  <blockquote class="solution">
    <h2 id="solution">Solution:</h2>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">clust</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">hclust</span><span class="p">(</span><span class="n">dist_m</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"complete"</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">clust</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plotclustex-1.png" alt="plot of chunk plotclustex" width="432" />
<p class="caption">plot of chunk plotclustex</p>
</div>
  </blockquote>
</blockquote>

<p>This dendrogram shows similarities/differences in distances between data points.
Each leaf of the dendrogram represents one of the 20 data points. These leaves
fuse into branches as the height increases. Observations that are similar fuse into
the same branches. The height at which any two
data points fuse indicates how different these two points are. Points that fuse
at the top of the tree are very different from each other compared with two
points that fuse at the bottom of the tree, which are quite similar. You can
see this by comparing the position of similar/dissimilar points according to
the scatterplot with their position on the tree.</p>

<h1 id="identifying-clusters-based-on-the-dendrogram">Identifying clusters based on the dendrogram</h1>

<p>To do this, we can make a horizontal cut through the dendrogram at a user-defined height. 
The sets of observations beneath this cut can be thought of as distinct clusters. For
example, a cut at height 10 produces two downstream clusters while a cut at
height 4 produces six downstream clusters.</p>

<p>We can cut the dendrogram to determine number of clusters at different heights
using <code class="language-plaintext highlighter-rouge">cutree()</code>. This function cuts a dendrogram into several
groups (or clusters) where the number of desired groups is controlled by the
user, by defining either <code class="language-plaintext highlighter-rouge">k</code> (number of groups) or <code class="language-plaintext highlighter-rouge">h</code> (height at which tree is
cut).</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## k is a user defined parameter determining</span><span class="w">
</span><span class="c1">## the desired number of clusters at which to cut the treee</span><span class="w">
</span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code> [1] 1 1 1 2 1 1 1 1 1 2 2 1 1 3 1 1 2 2 1 2
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## h is a user defined parameter determining</span><span class="w">
</span><span class="c1">## the numeric height at which to cut the tree</span><span class="w">
</span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code> [1] 1 1 1 2 1 1 1 1 1 2 2 1 1 3 1 1 2 2 1 2
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## both give same results </span><span class="w">

</span><span class="n">four_cut</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="p">)</span><span class="w">

</span><span class="c1">## we can produce the cluster each observation belongs to</span><span class="w">
</span><span class="c1">## using the mutate and count functions</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">
</span><span class="n">example_cl</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mutate</span><span class="p">(</span><span class="n">example_data</span><span class="p">,</span><span class="w"> </span><span class="n">cluster</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">four_cut</span><span class="p">)</span><span class="w">
</span><span class="n">count</span><span class="p">(</span><span class="n">example_cl</span><span class="p">,</span><span class="w"> </span><span class="n">cluster</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  cluster n
1       1 2
2       2 4
3       3 1
4       4 3
5       5 4
6       6 2
7       7 3
8       8 1
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#plot cluster each point belongs to on original scatterplot</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">example_cl</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">factor</span><span class="p">(</span><span class="n">cluster</span><span class="p">)))</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">geom_point</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-cutree-1.png" alt="plot of chunk cutree" width="432" />
<p class="caption">plot of chunk cutree</p>
</div>

<p>Note that this cut produces 8 clusters (two before the cut and another six
downstream of the cut).</p>

<blockquote class="challenge">
  <h2 id="challenge-2">Challenge 2:</h2>

  <p>Identify the value of <code class="language-plaintext highlighter-rouge">k</code> in <code class="language-plaintext highlighter-rouge">cutree()</code> that gives the same
output as <code class="language-plaintext highlighter-rouge">h = 5</code></p>

  <blockquote class="solution">
    <h2 id="solution-1">Solution:</h2>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">clust</span><span class="p">)</span><span class="w">
</span><span class="c1">## create horizontal line at height = 5</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-h-k-ex-plot-1.png" alt="plot of chunk h-k-ex-plot" width="432" />
<p class="caption">plot of chunk h-k-ex-plot</p>
</div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code> [1] 1 2 2 3 4 5 2 5 1 6 6 2 5 7 4 4 6 6 5 6
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">7</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code> [1] 1 2 2 3 4 5 2 5 1 6 6 2 5 7 4 4 6 6 5 6
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">five_cut</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w">

</span><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">
</span><span class="n">example_cl</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mutate</span><span class="p">(</span><span class="n">example_data</span><span class="p">,</span><span class="w"> </span><span class="n">cluster</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">five_cut</span><span class="p">)</span><span class="w">
</span><span class="n">count</span><span class="p">(</span><span class="n">example_cl</span><span class="p">,</span><span class="w"> </span><span class="n">cluster</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  cluster n
1       1 2
2       2 4
3       3 1
4       4 3
5       5 4
6       6 5
7       7 1
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">example_cl</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">factor</span><span class="p">(</span><span class="n">cluster</span><span class="p">)))</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">geom_point</span><span class="p">()</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-h-k-ex-plot-2.png" alt="plot of chunk h-k-ex-plot" width="432" />
<p class="caption">plot of chunk h-k-ex-plot</p>
</div>

    <p>Seven clusters (<code class="language-plaintext highlighter-rouge">k = 7</code>) gives similar results to <code class="language-plaintext highlighter-rouge">h = 5</code>. You can plot a
horizontal line on the dendrogram at <code class="language-plaintext highlighter-rouge">h = 5</code> to help identify
corresponding value of <code class="language-plaintext highlighter-rouge">k</code>.</p>
  </blockquote>
</blockquote>

<h1 id="highlighting-dendrogram-branches">Highlighting dendrogram branches</h1>

<p>In addition to visualising cluster identity in scatter plots, it is also possible to
highlight branches in dentrograms. In this example, we calculate a distance matrix between
samples in the <code class="language-plaintext highlighter-rouge">methyl_mat</code> dataset. We then draw boxes round clusters obtained with <code class="language-plaintext highlighter-rouge">cutree</code>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## create a distance matrix using euclidean method</span><span class="w">
</span><span class="n">distmat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dist</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">)</span><span class="w">
</span><span class="c1">## hierarchical clustering using complete method</span><span class="w">
</span><span class="n">clust</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">hclust</span><span class="p">(</span><span class="n">distmat</span><span class="p">)</span><span class="w">
</span><span class="c1">## plot resulting dendrogram</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">clust</span><span class="p">)</span><span class="w">

</span><span class="c1">## draw border around three clusters</span><span class="w">
</span><span class="n">rect.hclust</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">border</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="o">:</span><span class="m">6</span><span class="p">)</span><span class="w">
</span><span class="c1">## draw border around two clusters</span><span class="w">
</span><span class="n">rect.hclust</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">border</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="o">:</span><span class="m">6</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-method-1.png" alt="plot of chunk plot-clust-method" width="432" />
<p class="caption">plot of chunk plot-clust-method</p>
</div>
<p>We can also colour clusters downstream of a specified cut using <code class="language-plaintext highlighter-rouge">color_branches()</code>
from the <strong><code class="language-plaintext highlighter-rouge">dendextend</code></strong> package.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## cut tree at height = 4</span><span class="w">
</span><span class="n">cut</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">50</span><span class="p">)</span><span class="w">

</span><span class="n">library</span><span class="p">(</span><span class="s2">"dendextend"</span><span class="p">)</span><span class="w">
</span><span class="n">avg_dend_obj</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.dendrogram</span><span class="p">(</span><span class="n">clust</span><span class="p">)</span><span class="w">      
</span><span class="c1">## colour branches of dendrogram depending on clusters</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">color_branches</span><span class="p">(</span><span class="n">avg_dend_obj</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">50</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-coloured-branches-1.png" alt="plot of chunk plot-coloured-branches" width="432" />
<p class="caption">plot of chunk plot-coloured-branches</p>
</div>

<h1 id="the-effect-of-different-linkage-methods">The effect of different linkage methods</h1>
<p>Now let us look into changing the default behaviour of <code class="language-plaintext highlighter-rouge">hclust()</code>. Imagine we have two crescent-shaped point clouds as shown below.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># These two functions are to help us make crescents. Don't worry it you do not understand all this code.</span><span class="w">
</span><span class="c1"># The importent bit is the object "cres", which consists of two columns (x and y coordinates of two crescents).</span><span class="w">
</span><span class="n">is.insideCircle</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">co</span><span class="p">,</span><span class="w"> </span><span class="n">r</span><span class="o">=</span><span class="m">0.5</span><span class="p">,</span><span class="w"> </span><span class="n">offs</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">0</span><span class="p">)){</span><span class="w">
  </span><span class="nf">sqrt</span><span class="p">((</span><span class="n">co</span><span class="p">[,</span><span class="m">1</span><span class="p">]</span><span class="o">+</span><span class="n">offs</span><span class="p">[</span><span class="m">1</span><span class="p">])</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">co</span><span class="p">[,</span><span class="m">2</span><span class="p">]</span><span class="o">+</span><span class="n">offs</span><span class="p">[</span><span class="m">2</span><span class="p">])</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">r</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="n">make.crescent</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">n</span><span class="p">){</span><span class="w">
  </span><span class="n">raw</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cbind</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">runif</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="m">-0.5</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">runif</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="m">-0.5</span><span class="p">)</span><span class="w">
  </span><span class="n">raw</span><span class="p">[</span><span class="n">is.insideCircle</span><span class="p">(</span><span class="n">raw</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="o">!</span><span class="n">is.insideCircle</span><span class="p">(</span><span class="n">raw</span><span class="p">,</span><span class="w"> </span><span class="n">offs</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">-0.2</span><span class="p">)),]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="c1"># make x/y data in shape of two crescents</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span><span class="w">
</span><span class="n">cres1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">make.crescent</span><span class="p">(</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="c1"># 1st crescent</span><span class="w">
</span><span class="n">cres2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">make.crescent</span><span class="p">(</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="c1"># 2nd crescent</span><span class="w">
</span><span class="n">cres2</span><span class="p">[,</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="o">-</span><span class="n">cres2</span><span class="p">[,</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="m">-0.1</span><span class="w"> </span><span class="c1"># flip 2nd crescent upside-down and shift down</span><span class="w">
</span><span class="n">cres2</span><span class="p">[,</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cres2</span><span class="p">[,</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0.5</span><span class="w"> </span><span class="c1"># shift second crescent to the right</span><span class="w">

</span><span class="n">cres</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rbind</span><span class="p">(</span><span class="n">cres1</span><span class="p">,</span><span class="w"> </span><span class="n">cres2</span><span class="p">)</span><span class="w"> </span><span class="c1"># concatente x/y values</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-crescents-1.png" alt="plot of chunk crescents" width="432" />
<p class="caption">plot of chunk crescents</p>
</div>
<p>We might expect that the crescents are resolved into separate clusters. But if we
run hierarchical clustering with the default arguments, we get this:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cresClass</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">cres</span><span class="p">)),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="c1"># save partition for colouring</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">cresClass</span><span class="p">)</span><span class="w"> </span><span class="c1"># colour scatterplot by partition</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-cresClustDefault-1.png" alt="plot of chunk cresClustDefault" width="432" />
<p class="caption">plot of chunk cresClustDefault</p>
</div>

<blockquote class="challenge">
  <h2 id="challenge-3">Challenge 3</h2>

  <p>Carry out hierarchical clustering on the <code class="language-plaintext highlighter-rouge">cres</code> data that we generated above.
Try out different linkage methods and use <code class="language-plaintext highlighter-rouge">cutree()</code> to split each resulting
dendrogram into two clusters. Plot the results colouring the dots according to
their inferred cluster identity.</p>

  <p>Which method(s) give you the expected clustering outcome?</p>

  <p>Hint: Check <code class="language-plaintext highlighter-rouge">?hclust</code> to see the possible values of the argument <code class="language-plaintext highlighter-rouge">method</code> (the linkage method used).</p>

  <blockquote class="solution">
    <h2 id="solution-2">Solution:</h2>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#?hclust</span><span class="w">
</span><span class="c1"># "complete", "single", "ward.D", "ward.D2", "average", "mcquitty", "median" or "centroid"</span><span class="w">
</span><span class="n">cresClassSingle</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">cres</span><span class="p">),</span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"single"</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">cresClassSingle</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"single"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-comp-1.png" alt="plot of chunk plot-clust-comp" width="432" />
<p class="caption">plot of chunk plot-clust-comp</p>
</div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cresClassWard.D</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">cres</span><span class="p">),</span><span class="w"> </span><span class="n">method</span><span class="o">=</span><span class="s2">"ward.D"</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">cresClassWard.D</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"ward.D"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-wardD-1.png" alt="plot of chunk plot-clust-wardD" width="432" />
<p class="caption">plot of chunk plot-clust-wardD</p>
</div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cresClassWard.D2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">cres</span><span class="p">),</span><span class="w"> </span><span class="n">method</span><span class="o">=</span><span class="s2">"ward.D2"</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">cresClassWard.D2</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"ward.D2"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-wardD2-1.png" alt="plot of chunk plot-clust-wardD2" width="432" />
<p class="caption">plot of chunk plot-clust-wardD2</p>
</div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cresClassAverage</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">cres</span><span class="p">),</span><span class="w"> </span><span class="n">method</span><span class="o">=</span><span class="s2">"average"</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">cresClassAverage</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"average"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-average-1.png" alt="plot of chunk plot-clust-average" width="432" />
<p class="caption">plot of chunk plot-clust-average</p>
</div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cresClassMcquitty</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">cres</span><span class="p">),</span><span class="w"> </span><span class="n">method</span><span class="o">=</span><span class="s2">"mcquitty"</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">cresClassMcquitty</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"mcquitty"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-mcq-1.png" alt="plot of chunk plot-clust-mcq" width="432" />
<p class="caption">plot of chunk plot-clust-mcq</p>
</div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cresClassMedian</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">cres</span><span class="p">),</span><span class="w"> </span><span class="n">method</span><span class="o">=</span><span class="s2">"median"</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w"> 
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">cresClassMedian</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"median"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-median-1.png" alt="plot of chunk plot-clust-median" width="432" />
<p class="caption">plot of chunk plot-clust-median</p>
</div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cresClassCentroid</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">cres</span><span class="p">),</span><span class="w"> </span><span class="n">method</span><span class="o">=</span><span class="s2">"centroid"</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cres</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="n">cresClassCentroid</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"centroid"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-centroid-1.png" alt="plot of chunk plot-clust-centroid" width="432" />
<p class="caption">plot of chunk plot-clust-centroid</p>
</div>

    <p>The linkage methods <code class="language-plaintext highlighter-rouge">single</code>, <code class="language-plaintext highlighter-rouge">ward.D</code>, and <code class="language-plaintext highlighter-rouge">average</code> resolve each crescent as a separate cluster.</p>

  </blockquote>
</blockquote>

<p>The help page of <code class="language-plaintext highlighter-rouge">hclust()</code> gives some intuition on linkage methods. It describes <code class="language-plaintext highlighter-rouge">complete</code>
(the default) and <code class="language-plaintext highlighter-rouge">single</code> as opposite ends of a spectrum with all other methods in between.
When using complete linkage, the distance between two clusters is assumed to be the distance
between both clusters’ most distant points. This opposite it true for single linkage, where
the minimum distance between any two points, one from each of two clusters is used. Single
linkage is described as friends-of-friends appporach - and really, it groups all close-together
points into the same cluster (thus resolving one cluster per crescent). Complete linkage on the
other hand recognises that some points a the tip of a crescent are much closer to points in the
other crescent and so it splits both crescents.</p>

<h1 id="using-different-distance-methods">Using different distance methods</h1>

<p>So far, we’ve been using Euclidean distance to define the dissimilarity
or distance between observations. However, this isn’t always the best
metric for how dissimilar different observations are. Let’s make an
example to demonstrate. Here, we’re creating two samples each with
ten observations of random noise:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">20</span><span class="p">)</span><span class="w">
</span><span class="n">cor_example</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="w">
  </span><span class="n">sample_a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="m">10</span><span class="p">),</span><span class="w">
  </span><span class="n">sample_b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="m">10</span><span class="p">)</span><span class="w">
</span><span class="p">)</span><span class="w">
</span><span class="n">rownames</span><span class="p">(</span><span class="n">cor_example</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">paste</span><span class="p">(</span><span class="w">
  </span><span class="s2">"Feature"</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">nrow</span><span class="p">(</span><span class="n">cor_example</span><span class="p">)</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>Now, let’s create a new sample that has exactly the same pattern across all
our features as <code class="language-plaintext highlighter-rouge">sample_a</code>, just offset by 5:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cor_example</span><span class="o">$</span><span class="n">sample_c</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cor_example</span><span class="o">$</span><span class="n">sample_a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">5</span><span class="w">
</span></code></pre></div></div>

<p>You can see that this is a lot like the <code class="language-plaintext highlighter-rouge">assay()</code> of our methylation object
from earlier, where columns are observations or samples, and rows are features:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">head</span><span class="p">(</span><span class="n">cor_example</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            sample_a    sample_b sample_c
Feature 1  1.1626853 -0.02013537 6.162685
Feature 2 -0.5859245 -0.15038222 4.414076
Feature 3  1.7854650 -0.62812676 6.785465
Feature 4 -1.3325937  1.32322085 3.667406
Feature 5 -0.4465668 -1.52135057 4.553433
Feature 6  0.5696061 -0.43742787 5.569606
</code></pre></div></div>

<p>If we plot a heatmap of this, we can see that <code class="language-plaintext highlighter-rouge">sample_a</code> and <code class="language-plaintext highlighter-rouge">sample_b</code> are
grouped together because they have a small distance to each other, despite
being quite different in their pattern across the different features.
In contrast, <code class="language-plaintext highlighter-rouge">sample_a</code> and <code class="language-plaintext highlighter-rouge">sample_c</code> are very distant, despite having
<em>exactly</em> the same pattern across the different features.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Heatmap</span><span class="p">(</span><span class="n">as.matrix</span><span class="p">(</span><span class="n">cor_example</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-heatmap-cor-example-1.png" alt="plot of chunk heatmap-cor-example" width="432" />
<p class="caption">plot of chunk heatmap-cor-example</p>
</div>

<p>We can see that more clearly if we do a line plot:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## create a blank plot (type = "n" means don't draw anything)</span><span class="w">
</span><span class="c1">## with an x range to hold the number of features we have.</span><span class="w">
</span><span class="c1">## the range of y needs to be enough to show all the values for every feature</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="w">
  </span><span class="m">1</span><span class="o">:</span><span class="n">nrow</span><span class="p">(</span><span class="n">cor_example</span><span class="p">),</span><span class="w">
  </span><span class="nf">rep</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">cor_example</span><span class="p">),</span><span class="w"> </span><span class="m">5</span><span class="p">),</span><span class="w">
  </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"n"</span><span class="w">
</span><span class="p">)</span><span class="w">
</span><span class="c1">## draw a red line for sample_a</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="n">cor_example</span><span class="o">$</span><span class="n">sample_a</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"firebrick"</span><span class="p">)</span><span class="w">
</span><span class="c1">## draw a blue line for sample_b</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="n">cor_example</span><span class="o">$</span><span class="n">sample_b</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dodgerblue"</span><span class="p">)</span><span class="w">
</span><span class="c1">## draw a green line for sample_c</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="n">cor_example</span><span class="o">$</span><span class="n">sample_c</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"forestgreen"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-lineplot-cor-example-1.png" alt="plot of chunk lineplot-cor-example" width="432" />
<p class="caption">plot of chunk lineplot-cor-example</p>
</div>

<p>We can see that <code class="language-plaintext highlighter-rouge">sample_a</code> and <code class="language-plaintext highlighter-rouge">sample_c</code> have exactly the same pattern across
all of the different features. However, due to the overall difference between
the values, they have a high distance to each other.
We can see that if we cluster and plot the data ourselves using Euclidean
distance:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">clust_dist</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">hclust</span><span class="p">(</span><span class="n">dist</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">cor_example</span><span class="p">)))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">clust_dist</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-clust-euc-cor-example-1.png" alt="plot of chunk clust-euc-cor-example" width="432" />
<p class="caption">plot of chunk clust-euc-cor-example</p>
</div>

<p>In some cases, we might want to ensure that samples that have similar patterns,
whether that be of gene expression, or DNA methylation, have small distances
to each other. Correlation is a measure of this kind of similarity in pattern.
However, high correlations indicate similarity, while for a distance measure
we know that high distances indicate dissimilarity. Therefore, if we wanted
to cluster observations based on the correlation, or the similarity of patterns,
we can use <code class="language-plaintext highlighter-rouge">1 - cor(x)</code> as the distance metric.
The input to <code class="language-plaintext highlighter-rouge">hclust()</code> must be a <code class="language-plaintext highlighter-rouge">dist</code> object, so we also need to call
<code class="language-plaintext highlighter-rouge">as.dist()</code> on it before passing it in.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cor_as_dist</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.dist</span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">cor</span><span class="p">(</span><span class="n">cor_example</span><span class="p">))</span><span class="w">
</span><span class="n">clust_cor</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">hclust</span><span class="p">(</span><span class="n">cor_as_dist</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">clust_cor</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-clust-cor-cor-example-1.png" alt="plot of chunk clust-cor-cor-example" width="432" />
<p class="caption">plot of chunk clust-cor-cor-example</p>
</div>

<p>Now, <code class="language-plaintext highlighter-rouge">sample_a</code> and <code class="language-plaintext highlighter-rouge">sample_c</code> that have identical patterns across the features
are grouped together, while <code class="language-plaintext highlighter-rouge">sample_b</code> is seen as distant because it has a
different pattern, even though its values are closer to <code class="language-plaintext highlighter-rouge">sample_a</code>.
Using your own distance function is often useful, especially if you have missing
or unusual data. It’s often possible to use correlation and other custom
distance functions to functions that perform hierarchical clustering, such as
<code class="language-plaintext highlighter-rouge">pheatmap()</code> and <code class="language-plaintext highlighter-rouge">stats::heatmap()</code>:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## pheatmap allows you to select correlation directly</span><span class="w">
</span><span class="n">pheatmap</span><span class="p">(</span><span class="n">as.matrix</span><span class="p">(</span><span class="n">cor_example</span><span class="p">),</span><span class="w"> </span><span class="n">clustering_distance_cols</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"correlation"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-heatmap-cor-cor-example-1.png" alt="plot of chunk heatmap-cor-cor-example" width="432" />
<p class="caption">plot of chunk heatmap-cor-cor-example</p>
</div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Using the built-in stats::heatmap </span><span class="w">
</span><span class="n">heatmap</span><span class="p">(</span><span class="w">
  </span><span class="n">as.matrix</span><span class="p">(</span><span class="n">cor_example</span><span class="p">),</span><span class="w">
  </span><span class="n">distfun</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="n">as.dist</span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">cor</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-heatmap-cor-cor-example-2.png" alt="plot of chunk heatmap-cor-cor-example" width="432" />
<p class="caption">plot of chunk heatmap-cor-cor-example</p>
</div>

<h1 id="validating-clusters">Validating clusters</h1>

<p>Now that we know how to carry out hierarchical clustering, how do we know how
many clusters are optimal for the dataset?</p>

<p>Hierarchical clustering carried out on any dataset will produce clusters,
even when there are no ‘real’ clusters in the data! We need to be able to
determine whether identified clusters represent true groups in the data, or
whether clusters have been identified just due to chance. In the last episode,
we have introduced silhouette scores as a measure of cluster compactness and
bootstrapping to assess cluster robustness. Such tests can be used to compare
different clustering algorithms, for example, those fitted using different linkage
methods.</p>

<p>Here, we introduce the Dunn index, which is a measure of cluster compactness. The
Dunn index is the ratio of the smallest distance between any two clusters
and to the largest intra-cluster distance found within any cluster. This can be 
seen as a family of indices which differ depending on the method used to compute
distances. The Dunn index is a metric that penalises clusters that have
larger intra-cluster variance and smaller inter-cluster variance. The higher the
Dunn index, the better defined the clusters.</p>

<p>Let’s calculate the Dunn index for clustering carried out on the
<code class="language-plaintext highlighter-rouge">methyl_mat</code> dataset using the <strong><code class="language-plaintext highlighter-rouge">clValid</code></strong> package.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## calculate dunn index</span><span class="w">
</span><span class="c1">## (ratio of the smallest distance between obs not in the same cluster</span><span class="w">
</span><span class="c1">## to the largest intra-cluster distance)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"clValid"</span><span class="p">)</span><span class="w">
</span><span class="c1">## calculate euclidean distance between points </span><span class="w">
</span><span class="n">distmat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dist</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">)</span><span class="w">  
</span><span class="n">clust</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">hclust</span><span class="p">(</span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"complete"</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">clust</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-plot-clust-dunn-1.png" alt="plot of chunk plot-clust-dunn" width="432" />
<p class="caption">plot of chunk plot-clust-dunn</p>
</div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cut</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">50</span><span class="p">)</span><span class="w">

</span><span class="c1">## retrieve Dunn's index for given matrix and clusters</span><span class="w">
</span><span class="n">dunn</span><span class="p">(</span><span class="n">distance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">cut</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 0.8823501
</code></pre></div></div>

<p>The value of the Dunn index has no meaning in itself, but is used to compare
between sets of clusters with larger values being preferred.</p>

<blockquote class="challenge">
  <h2 id="challenge-4">Challenge 4</h2>

  <p>Examine how changing the <code class="language-plaintext highlighter-rouge">h</code> or <code class="language-plaintext highlighter-rouge">k</code> arguments in <code class="language-plaintext highlighter-rouge">cutree()</code>
affects the value of the Dunn index.</p>

  <blockquote class="solution">
    <h2 id="solution-3">Solution:</h2>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="s2">"clValid"</span><span class="p">)</span><span class="w">

</span><span class="n">distmat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dist</span><span class="p">(</span><span class="n">methyl_mat</span><span class="p">)</span><span class="w">
</span><span class="n">clust</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">hclust</span><span class="p">(</span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"complete"</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">clust</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="figure" style="text-align: center">
<img src="../fig/rmd-09-dunn-ex-1.png" alt="plot of chunk dunn-ex" width="432" />
<p class="caption">plot of chunk dunn-ex</p>
</div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Varying h</span><span class="w">
</span><span class="c1">## Obtaining the clusters</span><span class="w">
</span><span class="n">cut_h_20</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">)</span><span class="w">
</span><span class="n">cut_h_30</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">30</span><span class="p">)</span><span class="w">

</span><span class="c1">## How many clusters?</span><span class="w">
</span><span class="nf">length</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="n">cut_h_20</span><span class="p">))</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 36
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">length</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="n">cut_h_30</span><span class="p">))</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 14
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dunn</span><span class="p">(</span><span class="n">distance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">cut_h_20</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 1.61789
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dunn</span><span class="p">(</span><span class="n">distance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">cut_h_30</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 0.8181846
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Varying k</span><span class="w">
</span><span class="c1">## Obtaining the clusters</span><span class="w">
</span><span class="n">cut_k_10</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w">
</span><span class="n">cut_k_5</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w">

</span><span class="c1">## How many clusters?</span><span class="w">
</span><span class="nf">length</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="n">cut_k_5</span><span class="p">))</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 5
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">length</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="n">cut_k_10</span><span class="p">))</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 10
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dunn</span><span class="p">(</span><span class="n">distance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">cut_k_5</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 0.8441528
</code></pre></div>    </div>

    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dunn</span><span class="p">(</span><span class="n">distance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">cut_k_10</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1] 0.7967132
</code></pre></div>    </div>
  </blockquote>
</blockquote>

<p>The figures below show in a more systematic way how changing the values of <code class="language-plaintext highlighter-rouge">k</code> and
<code class="language-plaintext highlighter-rouge">h</code> using <code class="language-plaintext highlighter-rouge">cutree()</code> affect the Dunn index.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">h_seq</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">70</span><span class="o">:</span><span class="m">10</span><span class="w">
</span><span class="n">h_dunn</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">h_seq</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="n">dunn</span><span class="p">(</span><span class="n">distance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">)))</span><span class="w">
</span><span class="n">k_seq</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w">
</span><span class="n">k_dunn</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">k_seq</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="n">dunn</span><span class="p">(</span><span class="n">distance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">distmat</span><span class="p">,</span><span class="w"> </span><span class="n">cutree</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">)))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">h_seq</span><span class="p">,</span><span class="w"> </span><span class="n">h_dunn</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Height (h)"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Dunn index"</span><span class="p">)</span><span class="w">
</span><span class="n">grid</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-hclust-fig3-1.png" alt="Figure 3: Dunn index" width="432" />
<p class="caption">Figure 3: Dunn index</p>
</div>
<p>You can see that at low values of <code class="language-plaintext highlighter-rouge">h</code>, the Dunn index can be high. But this
is not very useful - cutting the given tree at a low <code class="language-plaintext highlighter-rouge">h</code> value like 15 leads to allmost all observations
ending up each in its own cluster. More relevant is the second maximum in the plot, around <code class="language-plaintext highlighter-rouge">h=55</code>.
Looking at the dendrogram, this corresponds to <code class="language-plaintext highlighter-rouge">k=4</code>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">k_seq</span><span class="p">,</span><span class="w"> </span><span class="n">k_dunn</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Number of clusters (k)"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Dunn index"</span><span class="p">)</span><span class="w">
</span><span class="n">grid</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>

<div class="figure" style="text-align: center">
<img src="../fig/rmd-09-hclust-fig4-1.png" alt="Figure 4: Dunn index continued" width="432" />
<p class="caption">Figure 4: Dunn index continued</p>
</div>
<p>For the given range of <code class="language-plaintext highlighter-rouge">k</code> values explored, we obtain the highest Dunn index with <code class="language-plaintext highlighter-rouge">k=4</code>.
This is in agreement with the previous plot.</p>

<p>There have been criticisms of the use of the Dunn index in validating
clustering results, due to its high sensitivity to noise in the dataset.
An alternative is to use silhouette scores (see the k-means clustering episode).</p>

<p>As we said before (see previous episode), clustering is a non-trivial task.
It is important to think about the nature of your data and your expactations
rather than blindly using a some algorithm for clustering or cluster validation.</p>

<h1 id="further-reading">Further reading</h1>

<ul>
  <li>Dunn, J. C. (1974) Well-separated clusters and optimal fuzzy partitions. Journal of Cybernetics 4(1):95–104.</li>
  <li>Halkidi, M., Batistakis, Y. &amp; Vazirgiannis, M. (2001) On clustering validation techniques. Journal of Intelligent Information Systems 17(2/3):107-145.</li>
  <li>James, G., Witten, D., Hastie, T. &amp; Tibshirani, R. (2013) An Introduction to Statistical Learning with Applications in R. 
Section 10.3.2 (Hierarchical Clustering).</li>
  <li><a href="https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec">Understanding the concept of Hierarchical clustering Technique. towards data science blog</a>.</li>
</ul>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>Hierarchical clustering uses an algorithm to group similar data points into clusters. A dendrogram is used to plot relationships between clusters (using the <code class="language-plaintext highlighter-rouge">hclust()</code> function in R).</p>
</li>
    
    <li><p>Hierarchical clustering differs from k-means clustering as it does not require the user to specify expected number of clusters</p>
</li>
    
    <li><p>The distance (dissimilarity) matrix can be calculated in various ways, and different clustering algorithms (linkage methods) can affect the resulting dendrogram.</p>
</li>
    
    <li><p>The Dunn index can be used to validate clusters using the original dataset.</p>
</li>
    
  </ul>
</blockquote>

<hr />


</article>


      
      






<footer>
  <hr/>
  <div class="row">
    <div class="col-md-6 license" id="license-info" align="left">
	
        Licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a> 2024 by <a href="../CITATION">the authors</a>.
	
    </div>
    <div class="col-md-6 help-links" align="right">
	
	<a href="/edit//aio.md" data-checker-ignore>Edit on GitHub</a>
	
	/
	<a href="/blob//CONTRIBUTING.md" data-checker-ignore>Contributing</a>
	/
	<a href="/">Source</a>
	/
	<a href="/blob//CITATION" data-checker-ignore>Cite</a>
	/
	<a href="mailto:alan.ocallaghan@outlook.com">Contact</a>
    </div>
  </div>
  <p class="text-muted text-right">
    <small><i>Using <a href="https://github.com/carpentries/carpentries-theme/">The Carpentries theme</a> &mdash; Site last built on: 2024-02-27 09:14:21 +0000.</i></small>
  </p>
</footer>

      
    </div>
    
<script src="../assets/js/jquery.min.js"></script>
<script src="../assets/js/bootstrap.min.js"></script>
<script src="../assets/js/lesson.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-37305346-2', 'auto');
  ga('send', 'pageview');
</script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        "HTML-CSS": { fonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  </body>
</html>
