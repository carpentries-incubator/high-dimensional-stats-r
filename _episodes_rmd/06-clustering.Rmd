---
title: "Clustering"
teaching: 0
exercises: 0
questions:
- "What does clustering mean?"
- "How can we cluster data with a model?"
- "How can we cluster data without a model?"
- "How can we check if clusters are robust?"
objectives:
- "Perform clustering with K-means, mixture models, and hierarchical clustering."
- "Assess clustering performance with silhouette score and bootstrapping/consensus clustering."
keypoints:
- "KP1"
math: yes
---


```{r settings, include=FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("06-")
if (!file.exists(here::here("data/scrnaseq.rds"))) {
    source(here::here("data/scrnaseq.R"))
}
```
```{r pkgs, echo=FALSE}
## preamble
suppressPackageStartupMessages({
    library("scRNAseq")
    library("mixtools")
    library("irlba")
    library("scater")
    library("scuttle")
    library("scran")
    library("Rtsne")
    library("mclust")
    library("igraph")
    library("bluster")
    library("here")
})
```

# Introduction


```{r data}
library("SingleCellExperiment")
zd <- readRDS(here("data/scrnaseq.rds"))
set.seed(42)
```

# Mixture model introduction

```{r mixture-animation, echo=FALSE, fig.cap="Cap", fig.alt="Alt"}
set.seed(66)
true_means <- c(-1, 4)
true_sds <- c(2, 1)
df <- data.frame(
    x = c(
        rnorm(30, mean = true_means[[1]], sd = true_sds[[1]]),
        rnorm(50, mean = true_means[[2]], sd = true_sds[[1]])
    ),
    cluster = c(rep("a", 30), rep("b", 50))
)
means <- c(1, 1.1)
sds <- c(1, 1)
dfs <- c(1, 1)

label_points_mixture <- function(x, means, sds) {
    sapply(
        x,
        function(i) {
            which.max(
                c(
                    dnorm(i, mean = means[[1]], sd = sds[[1]]),
                    dnorm(i, mean = means[[2]], sd = sds[[2]])
                )
            )
        }
    )
}

iterate_mixture <- function(x, means, sds) {
    components <- label_points_mixture(x, means, sds)
    means[[1]] <- mean(x[components == 1])
    means[[2]] <- mean(x[components == 2])
    sds[[1]] <- sd(x[components == 1])
    sds[[2]] <- sd(x[components == 2])
    list(components = components, means = means, sds = sds)
}

plot_mixture <- function(x, means, sds, components) {
    ggplot(data.frame(x = x)) +
        aes(x) +
        geom_histogram(aes(y = ..density..)) +
        lims(x = range(x) * 1.5) +
        geom_function(
            fun = function(...) dnorm(...) * mean(components == 1),
            args = list(mean = means[[1]], sd = sds[[1]]),
            aes(colour = "component 1")
        ) +
        geom_function(
            fun = function(...) dnorm(...) * mean(components == 2),
            args = list(mean = means[[2]], sd = sds[[2]]),
            aes(colour = "component 2")
        )
}


components <- label_points_mixture(df$x, means, sds)
plot_mixture(df$x, means, sds, components)
out <- iterate_mixture(df$x, means, sds)
plot_mixture(df$x, out$means, out$sds, out$components)
```


# Mixture models proper (multi-dimensional)

https://web.stanford.edu/class/bios221/book/Chap-Mixtures.html

```{r dimred}
library("scater")
zd <- runPCA(zd, ncomponents = 15)
zd <- runTSNE(zd, dimred = "PCA")
```

```{r mixture, echo=FALSE, fig.cap = "Title", fig.alt = "Alt"}
library("mclust")
## model-based
clust <- mclustBIC(reducedDim(zd), modelNames = "VVV")
plot(clust)

model <- Mclust(reducedDim(zd), x = clust)

zd$mixture <- as.character(model$classification)

plotReducedDim(zd, "TSNE", colour_by = "mixture")
```


# K-means

Generalisation of mixture models to have arbitrary density, just using distance.

```{r kmeans-animation, echo = FALSE, fig.cap="Cap", fig.alt="Alt"}
set.seed(66)
true_means <- c(-1, 4)
true_sds <- c(2, 1)
df <- data.frame(
    x = c(
        rnorm(30, mean = true_means[[1]], sd = true_sds[[1]]),
        rnorm(50, mean = true_means[[2]], sd = true_sds[[1]])
    ),
    cluster = c(rep("a", 30), rep("b", 50))
)
means <- c(1, 1.1)
dfs <- c(1, 1)

label_points_kmeans <- function(x, means) {
    sapply(x,
        function(i) {
            which.min(
                c(
                    dist(rbind(i, means[[1]])),
                    dist(rbind(i, means[[2]]))
                )
            )
        }
    )
}

iterate_kmeans <- function(x, means, sds) {
    components <- label_points_kmeans(x, means)
    means[[1]] <- mean(x[components == 1])
    means[[2]] <- mean(x[components == 2])
    list(components = components, means = means)
}

plot_kmeans <- function(x, means, components) {
    ggplot(data.frame(x = x)) +
        geom_histogram(aes(x = x, fill = factor(components))) +
        geom_vline(xintercept = means[[1]]) +
        geom_vline(xintercept = means[[2]]) +
        lims(x = range(x) * 1.5)
}

components <- label_points_kmeans(df$x, means)
plot_kmeans(df$x, means, components)
out <- iterate_kmeans(df$x, means)
plot_kmeans(df$x, out$means, out$components)
```



https://web.stanford.edu/class/bios221/book/Chap-Clustering.html

```{r kmeans, fig.cap = "Title", fig.alt = "Alt"}
## ideas: vary centers, low iter.max, low nstart
cluster <- kmeans(reducedDim(zd), centers = 7, iter.max = 1000, nstart = 100)
zd$kmeans <- as.character(cluster$cluster)

plotReducedDim(zd, "TSNE", colour_by = "kmeans")
```

> ## k-medioids (PAM)
> 
> This is like k-means but 
>
> 
{: .callout}


# Choosing K

We can't use measures like sum of squares, because then we get $K=N$. As with
regression, if we keep adding parameters (in this case, clusters)
the fit will always get better. In fact, it has to! We will get a perfect
fit (zero error) when each point is its own "cluster".

Therefore we again need to use BIC or something.


# Hierarchical clustering

Recall the heatmap I showed in regression lesson.
If we do this without hierarchical clustering, it's very noisy.
```{r, fig.cap="Cap", fig.alt="Alt"}
library("minfi")
library("here")
library("ComplexHeatmap")

methylation <- readRDS(here("data/methylation.rds"))

age <- methylation$Age
methyl_mat <- t(assay(methylation))
small <- methyl_mat[, 1:500]
cor_mat <- cor(small)
col <- circlize::colorRamp2(
    breaks = seq(-1, 1, length.out = 9),
    colors = rev(RColorBrewer::brewer.pal(9, "RdYlBu"))
)
Heatmap(cor_mat,
    column_title = "Feature-feature correlation in methylation data",
    name = "Pearson correlation",
    col = col,
    cluster_rows = FALSE, cluster_columns = FALSE,
    show_row_dend = FALSE, show_column_dend = FALSE,
    show_row_names = FALSE, show_column_names = FALSE
)
```



With clustering you can see some nice groupings.

```{r, fig.cap="Cap", fig.alt="Alt"}
Heatmap(cor_mat,
    column_title = "Feature-feature correlation in methylation data",
    name = "Pearson correlation",
    col = col,
    row_dend_width = unit(0.2, "npc"),
    column_dend_height = unit(0.2, "npc"),
    show_row_names = FALSE, show_column_names = FALSE
)
```

# Distance functions

# Linkage methods

Complete linkage (the default) uses the maximum distance between clusters as
the distance when merging them.

```{r, fig.cap="Cap", fig.alt="Alt"}
distmat <- dist(cor_mat)
clust <- hclust(distmat, method = "complete")
plot(clust)
```

Wardâ€™s method says that the distance between two clusters, A and B, is how much
the sum of squares will increase when we merge them.

Details: https://jbhender.github.io/Stats506/F18/GP/Group10.html

```{r, fig.cap="Cap", fig.alt="Alt"}
clust <- hclust(distmat, method = "ward.D")
plot(clust)
```


```{r metrics, echo = FALSE, eval = FALSE, fig.cap = "Title", fig.alt = "Alt"}

# Graph-based clustering (maybe)

# ```{r graph, fig.cap = "Title", fig.alt = "Alt"}
## graph-based
g <- buildSNNGraph(zd, k = 10, use.dimred = "PCA")
clust <- igraph::cluster_walktrap(g)$membership
reducedDim(zd, "force") <- igraph::layout_with_fr(g)
colLabels(zd) <- factor(clust)
plotReducedDim(zd, colour_by = "label", dimred = "force")
# ```


# bluster::clusterRows - maybe?

## measures: silhouette, bootstrap
## approx silhouette? purity?
```


{% include links.md %}

