<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>High dimensional statistics with R: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css">
<script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../favicons/incubator/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicons/incubator/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../favicons/incubator/favicon-16x16.png">
<link rel="manifest" href="../favicons/incubator/site.webmanifest">
<link rel="mask-icon" href="../favicons/incubator/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Lesson Description" src="../assets/images/incubator-logo.svg"><span class="badge text-bg-info">
          <abbr title="This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-circle" style="border-radius: 5px"></i>
              Beta
            </a>
            <span class="visually-hidden">This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='../aio.html';">Learner View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Lesson Description" src="../assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      High dimensional statistics with R
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            High dimensional statistics with R
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<li><a class="dropdown-item" href="data.html">Data</a></li>
<hr>
</ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  High dimensional statistics with R
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../aio.html">Learner View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction-to-high-dimensional-data.html">1. Introduction to high-dimensional data</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-high-dimensional-regression.html">2. Regression with many outcomes</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-regression-regularisation.html">3. Regularised regression</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-principal-component-analysis.html">4. Principal component analysis</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="05-factor-analysis.html">5. Factor analysis</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-k-means.html">6. K-means</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="07-hierarchical.html">7. Hierarchical clustering</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <li><a href="data.html">Data</a></li>
<hr>
</ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-01-introduction-to-high-dimensional-data"><p>Content from <a href="01-introduction-to-high-dimensional-data.html">Introduction to high-dimensional data</a></p>
<hr>
<p>Last updated on 2025-09-02 |

        <a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/edit/main/episodes/01-introduction-to-high-dimensional-data.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 50 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are high-dimensional data and what do these data look like in
the biosciences?</li>
<li>What are the challenges when analysing high-dimensional data?</li>
<li>What statistical methods are suitable for analysing these data?</li>
<li>How can Bioconductor be used to access high-dimensional data in the
biosciences?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explore examples of high-dimensional data in the biosciences.</li>
<li>Appreciate challenges involved in analysing high-dimensional
data.</li>
<li>Explore different statistical methods used for analysing
high-dimensional data.</li>
<li>Work with example data created from biological studies.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="what-are-high-dimensional-data">What are high-dimensional data?<a class="anchor" aria-label="anchor" href="#what-are-high-dimensional-data"></a>
</h2>
<hr class="half-width">
<p><em>High-dimensional data</em> are defined as data with many features
(variables observed). In recent years, advances in information
technology have allowed large amounts of data to be collected and stored
with relative ease. As such, high-dimensional data have become more
common in many scientific fields, including the biological sciences,
where datasets in subjects like genomics and medical sciences often have
a large numbers of features. For example, hospital data may record many
variables, including symptoms, blood test results, behaviours, and
general health. An example of what high-dimensional data might look like
in a biomedical study is shown in the figure below.</p>
<figure style="text-align: center"><img src="../fig/intro-table.png" alt="Table displaying a high-dimensional data set with many columns representing features related to health, such as blood pressure, heart rate and respiratory rate. Each row contains the data for an individual patient. This type of high-dimensional data could contain hundreds or thousands of columns (features/variables) and thousands or even millions of rows (observations/samples/patients)." width="5572" class="figure mx-auto d-block"><figcaption>
Example of a high-dimensional data table with features in the columns
and individual observations (patients) in rows.
</figcaption></figure><p>Researchers often want to relate such features to specific patient
outcomes (e.g. survival, length of time spent in hospital). However,
analysing high-dimensional data can be extremely challenging since
standard methods of analysis, such as individual plots of features and
linear regression, are no longer appropriate when we have many features.
In this lesson, we will learn alternative methods for dealing with
high-dimensional data and discover how these can be applied for
practical high-dimensional data analysis in the biological sciences.</p>
<div id="challenge-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-1" class="callout-inner">
<h3 class="callout-title">Challenge 1</h3>
<div class="callout-content">
<p>Descriptions of four research questions and their datasets are given
below. Which of these scenarios use high-dimensional data?</p>
<ol style="list-style-type: decimal">
<li>Predicting patient blood pressure using: cholesterol level in blood,
age, and BMI measurements, collected from 100 patients.</li>
<li>Predicting patient blood pressure using: cholesterol level in blood,
age, and BMI, as well as information on 200,000 single nucleotide
polymorphisms from 100 patients.</li>
<li>Predicting the length of time patients spend in hospital with
pneumonia infection using: measurements on age, BMI, length of time with
symptoms, number of symptoms, and percentage of neutrophils in blood,
using data from 200 patients.</li>
<li>Predicting probability of a patient’s cancer progressing using gene
expression data from 20,000 genes, as well as data associated with
general patient health (age, weight, BMI, blood pressure) and cancer
growth (tumour size, localised spread, blood test results).</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li>No. The number of features is relatively small (4 including the
response variable since this is an observed variable).</li>
<li>Yes, this is an example of high-dimensional data. There are 200,004
features.</li>
<li>No. The number of features is relatively small (6).</li>
<li>Yes. There are 20,008 features.</li>
</ol>
</div>
</div>
</div>
</div>
<p>Now that we have an idea of what high-dimensional data look like we
can think about the challenges we face in analysing them.</p>
</section><section><h2 class="section-heading" id="why-is-dealing-with-high-dimensional-data-challenging">Why is dealing with high-dimensional data challenging?<a class="anchor" aria-label="anchor" href="#why-is-dealing-with-high-dimensional-data-challenging"></a>
</h2>
<hr class="half-width">
<p>Most classical statistical methods are set up for use on
low-dimensional data (i.e. with a small number of features, <span class="math inline">\(p\)</span>). This is because low-dimensional data
were much more common in the past when data collection was more
difficult and time consuming.</p>
<p>One challenge when analysing high-dimensional data is visualising the
many variables. When exploring low-dimensional datasets, it is possible
to plot the response variable against each of features to get an idea
which of these are important predictors of the response. With
high-dimensional data, the large number of features makes doing this
difficult. In addition, in some high-dimensional datasets it can also be
difficult to identify a single response variable, making standard data
exploration and analysis techniques less useful.</p>
<p>Let’s have a look at a simple dataset with lots of features to
understand some of the challenges we are facing when working with
high-dimensional data. For reference, all data used throughout the
lesson are described in the <a href="https://carpentries-incubator.github.io/high-dimensional-stats-r/data/index.html" class="external-link">data
page</a>.</p>
<div id="challenge-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-2" class="callout-inner">
<h3 class="callout-title">Challenge 2</h3>
<div class="callout-content">
<p>For illustrative purposes, we start with a simple dataset that is not
technically high-dimensional but contains many features. This will
illustrate the general problems encountered when working with many
features in a high-dimensional data set.</p>
<p>First, make sure you have completed <a href="https://carpentries-incubator.github.io/high-dimensional-stats-r/setup.html" class="external-link">the
setup instructions</a>. Next, let’s load the <a href="https://carpentries-incubator.github.io/high-dimensional-stats-r/data/index.html" class="external-link"><code>prostate</code></a>
dataset as follows:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">prostate</span> <span class="op">&lt;-</span> <span class="fu">readRDS</span><span class="op">(</span><span class="st">"data/prostate.rds"</span><span class="op">)</span></span></code></pre>
</div>
<p>Examine the dataset (in which each row represents a single patient)
to:</p>
<ol style="list-style-type: decimal">
<li>Determine how many observations (<span class="math inline">\(n\)</span>) and features (<span class="math inline">\(p\)</span>) are available (hint: see the
<code>dim()</code> function).</li>
<li>Examine what variables were measured (hint: see the
<code>names()</code> and <code>head()</code> functions).</li>
<li>Plot the relationship between the variables (hint: see the
<code>pairs()</code> function). What problem(s) with high-dimensional
data analysis does this illustrate?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">dim</span><span class="op">(</span><span class="va">prostate</span><span class="op">)</span>    <span class="co"># print the number of rows and columns</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">names</span><span class="op">(</span><span class="va">prostate</span><span class="op">)</span>  <span class="co"># examine the variable names</span></span>
<span><span class="fu">head</span><span class="op">(</span><span class="va">prostate</span><span class="op">)</span>   <span class="co"># print the first 6 rows</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">names</span><span class="op">(</span><span class="va">prostate</span><span class="op">)</span>  <span class="co"># examine column names</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] "lcavol"  "lweight" "age"     "lbph"    "svi"     "lcp"     "gleason"
[8] "pgg45"   "lpsa"   </code></pre>
</div>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">pairs</span><span class="op">(</span><span class="va">prostate</span><span class="op">)</span>  <span class="co"># plot each pair of variables against each other</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/01-introduction-to-high-dimensional-data-rendered-pairs-prostate-1.png" alt="A set of pairwise scatterplots of variables in the 'prostate' dataset, namely lcavol, lweight, age, lbph, svi, lcp, gleason, pgg45, lpsa. The plots are shown in a grid." class="figure mx-auto d-block"><figcaption>
Pairwise plots of the ‘prostate’ dataset.
</figcaption></figure><p>The <code>pairs()</code> function plots relationships between each of
the variables in the <code>prostate</code> dataset. This is possible for
datasets with smaller numbers of variables, but for datasets in which
<span class="math inline">\(p\)</span> is larger it becomes difficult
(and time consuming) to visualise relationships between all variables in
the dataset. Even where visualisation is possible, fitting models to
datasets with many variables is difficult due to the potential for
overfitting and difficulties in identifying a response variable.</p>
</div>
</div>
</div>
</div>
<p>Note that function documentation and information on function
arguments will be useful throughout this lesson. We can access these
easily in R by running <code>?</code> followed by the package name. For
example, the documentation for the <code>dim</code> function can be
accessed by running <code>?dim</code>.</p>
<div id="locating-data-with-r---the-here-package" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="locating-data-with-r---the-here-package" class="callout-inner">
<h3 class="callout-title">Locating data with R - the
<strong><code>here</code></strong> package</h3>
<div class="callout-content">
<p>It is often desirable to access external datasets from inside R and
to write code that does this reliably on different computers. While R
has an inbulit function <code>setwd()</code> that can be used to denote
where external datasets are stored, this usually requires the user to
adjust the code to their specific system and folder structure. The
<strong><code>here</code></strong> package is meant to be used in R
projects. It allows users to specify the data location relative to the R
project directory. This makes R code more portable and can contribute to
improve the reproducibility of an analysis.</p>
</div>
</div>
</div>
<p>As well as many variables causing problems when working with
high-dimensional data, having relatively few observations (<span class="math inline">\(n\)</span>) compared to the number of features
(<span class="math inline">\(p\)</span>) causes additional challenges.
To illustrate these challenges, imagine we are carrying out least
squares regression on a dataset with 25 observations. Fitting a best fit
line through these data produces a plot shown in the left-hand panel of
the figure below.</p>
<p>However, imagine a situation in which the number of observations and
features in a dataset are almost equal. In that situation the effective
number of observations per feature is low. The result of fitting a best
fit line through few observations can be seen in the right-hand panel
below.</p>
<figure style="text-align: center"><img src="../fig/intro-scatterplot.png" alt="Two scatter plots side-by-side, each plotting the relationship between two variables. The scatter plot on the left hand side shows 25 observations and a regression line with the points evenly scattered around. The scatter plot on the right hand side shows 2 observations and a regression line that goes through both points." width="5307" class="figure mx-auto d-block"><figcaption>
Scatter plot of two variables (x and y) from a data set with 25
observations (left) and 2 observations (right) with a fitted regression
line (red).
</figcaption></figure><p>In the first situation, the least squares regression line does not
fit the data perfectly and there is some error around the regression
line. But, when there are only two observations the regression line will
fit through the points exactly, resulting in overfitting of the data.
This suggests that carrying out least squares regression on a dataset
with few data points per feature would result in difficulties in
applying the resulting model to further datsets. This is a common
problem when using regression on high-dimensional datasets.</p>
<p>Another problem in carrying out regression on high-dimensional data
is dealing with correlations between explanatory variables. The large
numbers of features in these datasets makes high correlations between
variables more likely. Let’s explore why high correlations might be an
issue in a Challenge.</p>
<div id="challenge-3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-3" class="callout-inner">
<h3 class="callout-title">Challenge 3</h3>
<div class="callout-content">
<p>Use the <code>cor()</code> function to examine correlations between
all variables in the <code>prostate</code> dataset. Are some pairs of
variables highly correlated using a threshold of 0.75 for the
correlation coefficients?</p>
<p>Use the <code>lm()</code> function to fit univariate regression
models to predict patient age using two variables that are highly
correlated as predictors. Which of these variables are statistically
significant predictors of age? Hint: the <code>summary()</code> function
can help here.</p>
<p>Fit a multiple linear regression model predicting patient age using
both variables. What happened?</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>Create a correlation matrix of all variables in the
<code>prostate</code> dataset</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">cor</span><span class="op">(</span><span class="va">prostate</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>           lcavol      lweight       age         lbph         svi          lcp
lcavol  1.0000000  0.194128286 0.2249999  0.027349703  0.53884500  0.675310484
lweight 0.1941283  1.000000000 0.3075286  0.434934636  0.10877851  0.100237795
age     0.2249999  0.307528614 1.0000000  0.350185896  0.11765804  0.127667752
lbph    0.0273497  0.434934636 0.3501859  1.000000000 -0.08584324 -0.006999431
svi     0.5388450  0.108778505 0.1176580 -0.085843238  1.00000000  0.673111185
lcp     0.6753105  0.100237795 0.1276678 -0.006999431  0.67311118  1.000000000
gleason 0.4324171 -0.001275658 0.2688916  0.077820447  0.32041222  0.514830063
pgg45   0.4336522  0.050846821 0.2761124  0.078460018  0.45764762  0.631528245
lpsa    0.7344603  0.354120390 0.1695928  0.179809410  0.56621822  0.548813169
             gleason      pgg45      lpsa
lcavol   0.432417056 0.43365225 0.7344603
lweight -0.001275658 0.05084682 0.3541204
age      0.268891599 0.27611245 0.1695928
lbph     0.077820447 0.07846002 0.1798094
svi      0.320412221 0.45764762 0.5662182
lcp      0.514830063 0.63152825 0.5488132
gleason  1.000000000 0.75190451 0.3689868
pgg45    0.751904512 1.00000000 0.4223159
lpsa     0.368986803 0.42231586 1.0000000</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">round</span><span class="op">(</span><span class="fu">cor</span><span class="op">(</span><span class="va">prostate</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span> <span class="co"># rounding helps to visualise the correlations</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>        lcavol lweight  age  lbph   svi   lcp gleason pgg45 lpsa
lcavol    1.00    0.19 0.22  0.03  0.54  0.68    0.43  0.43 0.73
lweight   0.19    1.00 0.31  0.43  0.11  0.10    0.00  0.05 0.35
age       0.22    0.31 1.00  0.35  0.12  0.13    0.27  0.28 0.17
lbph      0.03    0.43 0.35  1.00 -0.09 -0.01    0.08  0.08 0.18
svi       0.54    0.11 0.12 -0.09  1.00  0.67    0.32  0.46 0.57
lcp       0.68    0.10 0.13 -0.01  0.67  1.00    0.51  0.63 0.55
gleason   0.43    0.00 0.27  0.08  0.32  0.51    1.00  0.75 0.37
pgg45     0.43    0.05 0.28  0.08  0.46  0.63    0.75  1.00 0.42
lpsa      0.73    0.35 0.17  0.18  0.57  0.55    0.37  0.42 1.00</code></pre>
</div>
<p>As seen above, some variables are highly correlated. In particular,
the correlation between <code>gleason</code> and <code>pgg45</code> is
equal to 0.75.</p>
<p>Fitting univariate regression models to predict age using gleason and
pgg45 as predictors.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model_gleason</span> <span class="op">&lt;-</span> <span class="fu">lm</span><span class="op">(</span><span class="va">age</span> <span class="op">~</span> <span class="va">gleason</span>, data <span class="op">=</span> <span class="va">prostate</span><span class="op">)</span></span>
<span><span class="va">model_pgg45</span> <span class="op">&lt;-</span> <span class="fu">lm</span><span class="op">(</span><span class="va">age</span> <span class="op">~</span> <span class="va">pgg45</span>, data <span class="op">=</span> <span class="va">prostate</span><span class="op">)</span></span></code></pre>
</div>
<p>Check which covariates have a significant efffect</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">summary</span><span class="op">(</span><span class="va">model_gleason</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Call:
lm(formula = age ~ gleason, data = prostate)

Residuals:
    Min      1Q  Median      3Q     Max
-20.780  -3.552   1.448   4.220  13.448

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)   45.146      6.918   6.525 3.29e-09 ***
gleason        2.772      1.019   2.721  0.00774 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 7.209 on 95 degrees of freedom
Multiple R-squared:  0.0723,	Adjusted R-squared:  0.06254
F-statistic: 7.404 on 1 and 95 DF,  p-value: 0.007741</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">summary</span><span class="op">(</span><span class="va">model_pgg45</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Call:
lm(formula = age ~ pgg45, data = prostate)

Residuals:
     Min       1Q   Median       3Q      Max
-21.0889  -3.4533   0.9111   4.4534  15.1822

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) 62.08890    0.96758   64.17  &lt; 2e-16 ***
pgg45        0.07289    0.02603    2.80  0.00619 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 7.193 on 95 degrees of freedom
Multiple R-squared:  0.07624,	Adjusted R-squared:  0.06651
F-statistic:  7.84 on 1 and 95 DF,  p-value: 0.006189</code></pre>
</div>
<p>Based on these results we conclude that both <code>gleason</code> and
<code>pgg45</code> have a statistically significant univariate effect
(also referred to as a marginal effect) as predictors of age (5%
significance level).</p>
<p>Fitting a multivariate regression model using both both
<code>gleason</code> and <code>pgg45</code> as predictors</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model_multivar</span> <span class="op">&lt;-</span> <span class="fu">lm</span><span class="op">(</span><span class="va">age</span> <span class="op">~</span> <span class="va">gleason</span> <span class="op">+</span> <span class="va">pgg45</span>, data <span class="op">=</span> <span class="va">prostate</span><span class="op">)</span></span>
<span><span class="fu">summary</span><span class="op">(</span><span class="va">model_multivar</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Call:
lm(formula = age ~ gleason + pgg45, data = prostate)

Residuals:
    Min      1Q  Median      3Q     Max
-20.927  -3.677   1.323   4.323  14.420

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) 52.95548    9.74316   5.435  4.3e-07 ***
gleason      1.45363    1.54299   0.942    0.349
pgg45        0.04490    0.03951   1.137    0.259
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 7.198 on 94 degrees of freedom
Multiple R-squared:  0.08488,	Adjusted R-squared:  0.06541
F-statistic: 4.359 on 2 and 94 DF,  p-value: 0.01547</code></pre>
</div>
<p>Although <code>gleason</code> and <code>pgg45</code> have
statistically significant univariate effects, this is no longer the case
when both variables are simultaneously included as covariates in a
multivariate regression model.</p>
</div>
</div>
</div>
</div>
<p>Including highly correlated variables such as <code>gleason</code>
and <code>pgg45</code> simultaneously the same regression model can lead
to problems in fitting a regression model and interpreting its output.
Although each variable appears to be associated with the response
individually, the model cannot distinguish the contribution of each
variable to the model. This can also increase the risk of over-fitting
since the model may fit redundant variables to noise rather than true
relationships.</p>
<p>To allow the information from variables to be included in the same
model despite high levels of correlation, we can use dimensionality
reduction methods to collapse multiple variables into a single new
variable (we will explore this dataset further in the dimensionality
reduction lesson). We can also use modifications to linear regression
like regularisation, which we will discuss in the lesson on
high-dimensional regression.</p>
</section><section><h2 class="section-heading" id="what-statistical-methods-are-used-to-analyse-high-dimensional-data">What statistical methods are used to analyse high-dimensional
data?<a class="anchor" aria-label="anchor" href="#what-statistical-methods-are-used-to-analyse-high-dimensional-data"></a>
</h2>
<hr class="half-width">
<p>We have discussed so far that high-dimensional data analysis can be
challenging since variables are difficult to visualise, leading to
challenges identifying relationships between variables and suitable
response variables; we may have relatively few observations compared to
features, leading to over-fitting; and features may be highly
correlated, leading to challenges interpreting models. We therefore
require alternative approaches to examine whether, for example, groups
of observations show similar characteristics and whether these groups
may relate to other features in the data (e.g. phenotype in genetics
data).</p>
<p>In this course, we will cover four methods that help in dealing with
high-dimensional data: (1) regression with numerous outcome variables,
(2) regularised regression, (3) dimensionality reduction, and (4)
clustering. Here are some examples of when each of these approaches may
be used:</p>
<ol style="list-style-type: decimal">
<li><p>Regression with numerous outcomes refers to situations in which
there are many variables of a similar kind (expression values for many
genes, methylation levels for many sites in the genome) and when one is
interested in assessing whether these variables are associated with a
specific covariate of interest, such as experimental condition or age.
In this case, multiple univariate regression models (one per each
outcome, using the covariate of interest as predictor) could be fitted
independently. In the context of high-dimensional molecular data, a
typical example are <em>differential gene expression</em> analyses. We
will explore this type of analysis in the <em>Regression with many
outcomes</em> episode.</p></li>
<li><p>Regularisation (also known as <em>regularised regression</em> or
<em>penalised regression</em>) is typically used to fit regression
models when there is a single outcome variable or interest but the
number of potential predictors is large, e.g. there are more predictors
than observations. Regularisation can help to prevent overfitting and
may be used to identify a small subset of predictors that are associated
with the outcome of interest. For example, regularised regression has
been often used when building <em>epigenetic clocks</em>, where
methylation values across several thousands of genomic sites are used to
predict chronological age. We will explore this in more detail in the
<em>Regularised regression</em> episode.</p></li>
<li><p>Dimensionality reduction is commonly used on high-dimensional
datasets for data exploration or as a preprocessing step prior to other
downstream analyses. For instance, a low-dimensional visualisation of a
gene expression dataset may be used to inform <em>quality control</em>
steps (e.g. are there any anomalous samples?). This course contains two
episodes that explore dimensionality reduction techniques: <em>Principal
component analysis</em> and <em>Factor analysis</em>.</p></li>
<li><p>Clustering methods can be used to identify potential grouping
patterns within a dataset. A popular example is the <em>identification
of distinct cell types</em> through clustering cells with similar gene
expression patterns. The <em>K-means</em> episode will explore a
specific method to perform clustering analysis.</p></li>
</ol>
<div id="using-bioconductor-to-access-high-dimensional-data-in-the-biosciences" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="using-bioconductor-to-access-high-dimensional-data-in-the-biosciences" class="callout-inner">
<h3 class="callout-title">Using Bioconductor to access high-dimensional
data in the biosciences</h3>
<div class="callout-content">
<p>In this workshop, we will look at statistical methods that can be
used to visualise and analyse high-dimensional biological data using
packages available from Bioconductor, open source software for analysing
high throughput genomic data. Bioconductor contains useful packages and
example datasets as shown on the website <a href="https://www.bioconductor.org/" class="external-link uri">https://www.bioconductor.org/</a>.</p>
<p>Bioconductor packages can be installed and used in <code>R</code>
using the <strong><code>BiocManager</code></strong> package. Let’s load
the <strong><code>limma</code></strong> package from Bioconductor (a
package for running linear models).</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"limma"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">browseVignettes</span><span class="op">(</span><span class="st">"limma"</span><span class="op">)</span></span></code></pre>
</div>
<p>We can explore these packages by browsing the vignettes provided in
Bioconductor. Bioconductor has various packages that can be used to load
and examine datasets in <code>R</code> that have been made available in
Bioconductor, usually along with an associated paper or package.</p>
<p>Next, we load the <a href="https://carpentries-incubator.github.io/high-dimensional-stats-r/data/index.html" class="external-link"><code>methylation</code></a>
dataset which represents data collected using Illumina Infinium
methylation arrays which are used to examine methylation across the
human genome. These data include information collected from the assay as
well as associated metadata from individuals from whom samples were
taken.</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">methylation</span> <span class="op">&lt;-</span> <span class="fu">readRDS</span><span class="op">(</span><span class="st">"data/methylation.rds"</span><span class="op">)</span></span>
<span><span class="fu">head</span><span class="op">(</span><span class="fu">colData</span><span class="op">(</span><span class="va">methylation</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in colData(methylation): could not find function "colData"</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">methyl_mat</span> <span class="op">&lt;-</span> <span class="fu">t</span><span class="op">(</span><span class="fu">assay</span><span class="op">(</span><span class="va">methylation</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in assay(methylation): could not find function "assay"</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## calculate correlations between cells in matrix</span></span>
<span><span class="va">cor_mat</span> <span class="op">&lt;-</span> <span class="fu">cor</span><span class="op">(</span><span class="va">methyl_mat</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'methyl_mat' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cor_mat</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span> <span class="co"># print the top-left corner of the correlation matrix</span></span></code></pre>
</div>
<p>The <code>assay()</code> function creates a matrix-like object where
rows represent probes for genes and columns represent samples. We
calculate correlations between features in the <code>methylation</code>
dataset and examine the first 100 cells of this matrix. The size of the
dataset makes it difficult to examine in full, a common challenge in
analysing high-dimensional genomics data.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="further-reading-and-resources">Further reading and resources<a class="anchor" aria-label="anchor" href="#further-reading-and-resources"></a>
</h2>
<hr class="half-width">
<ul>
<li>Buhlman, P. &amp; van de Geer, S. (2011) Statistics for
High-Dimensional Data. Springer, London.</li>
<li>
<a href="https://doi.org/10.1146/annurev-statistics-022513-115545" class="external-link">Buhlman,
P., Kalisch, M. &amp; Meier, L. (2014) High-dimensional statistics with
a view toward applications in biology. Annual Review of Statistics and
Its Application</a>.</li>
<li>Johnstone, I.M. &amp; Titterington, D.M. (2009) Statistical
challenges of high-dimensional data. Philosophical Transactions of the
Royal Society A 367:4237-4253.</li>
<li>
<a href="https://www.bioconductor.org/packages/release/workflows/vignettes/methylationArrayAnalysis/inst/doc/methylationArrayAnalysis.html" class="external-link">Bioconductor
ethylation array analysis vignette</a>.</li>
<li>The <em>Introduction to Machine Learning with Python</em> course
covers additional methods that could be used to analyse high-dimensional
data. See <a href="https://carpentries-incubator.github.io/machine-learning-novice-python/" class="external-link">Introduction
to machine learning</a>, <a href="https://carpentries-incubator.github.io/machine-learning-trees-python/" class="external-link">Tree
models</a> and <a href="https://carpentries-incubator.github.io/machine-learning-neural-python/" class="external-link">Neural
networks</a>. Some related (and important!) content is also available in
<a href="https://carpentries-incubator.github.io/machine-learning-responsible-python/" class="external-link">Responsible
machine learning</a>.</li>
<li>
<a href="https://www.youtube.com/c/joshstarmer" class="external-link">Josh Starmer’s</a>
youtube channel.</li>
</ul>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>High-dimensional data are data in which the number of features,
<span class="math inline">\(p\)</span>, are close to or larger than the
number of observations, <span class="math inline">\(n\)</span>.</li>
<li>These data are becoming more common in the biological sciences due
to increases in data storage capabilities and computing power.</li>
<li>Standard statistical methods, such as linear regression, run into
difficulties when analysing high-dimensional data.</li>
<li>In this workshop, we will explore statistical methods used for
analysing high-dimensional data using datasets available on
Bioconductor.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-02-high-dimensional-regression"><p>Content from <a href="02-high-dimensional-regression.html">Regression with many outcomes</a></p>
<hr>
<p>Last updated on 2025-09-02 |

        <a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/edit/main/episodes/02-high-dimensional-regression.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 120 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can we apply linear regression in a high-dimensional
setting?</li>
<li>How can we benefit from the fact that we have many outcomes?</li>
<li>How can we control for the fact that we do many tests?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Perform and critically analyse high-dimensional regression.</li>
<li>Understand methods for shrinkage of noise parameters in
high-dimensional regression.</li>
<li>Perform multiple testing adjustment.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="dna-methylation-data">DNA methylation data<a class="anchor" aria-label="anchor" href="#dna-methylation-data"></a>
</h2>
<hr class="half-width">
<p>For the following few episodes, we will be working with human DNA
methylation data from flow-sorted blood samples, described in <a href="https://carpentries-incubator.github.io/high-dimensional-stats-r/data/index.html" class="external-link">data</a>.
DNA methylation assays measure, for each of many sites in the genome,
the proportion of DNA that carries a methyl mark (a chemical
modification that does not alter the DNA sequence). In this case, the
methylation data come in the form of a matrix of normalised methylation
levels (M-values), where negative values correspond to unmethylated DNA
and positive values correspond to methylated DNA. Along with this, we
have a number of sample phenotypes (eg, age in years, BMI).</p>
<p>Let’s read in the data for this episode:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"SummarizedExperiment"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: MatrixGenerics</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: matrixStats</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'MatrixGenerics'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:matrixStats':

    colAlls, colAnyNAs, colAnys, colAvgsPerRowSet, colCollapse,
    colCounts, colCummaxs, colCummins, colCumprods, colCumsums,
    colDiffs, colIQRDiffs, colIQRs, colLogSumExps, colMadDiffs,
    colMads, colMaxs, colMeans2, colMedians, colMins, colOrderStats,
    colProds, colQuantiles, colRanges, colRanks, colSdDiffs, colSds,
    colSums2, colTabulates, colVarDiffs, colVars, colWeightedMads,
    colWeightedMeans, colWeightedMedians, colWeightedSds,
    colWeightedVars, rowAlls, rowAnyNAs, rowAnys, rowAvgsPerColSet,
    rowCollapse, rowCounts, rowCummaxs, rowCummins, rowCumprods,
    rowCumsums, rowDiffs, rowIQRDiffs, rowIQRs, rowLogSumExps,
    rowMadDiffs, rowMads, rowMaxs, rowMeans2, rowMedians, rowMins,
    rowOrderStats, rowProds, rowQuantiles, rowRanges, rowRanks,
    rowSdDiffs, rowSds, rowSums2, rowTabulates, rowVarDiffs, rowVars,
    rowWeightedMads, rowWeightedMeans, rowWeightedMedians,
    rowWeightedSds, rowWeightedVars</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: GenomicRanges</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: stats4</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: BiocGenerics</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'BiocGenerics'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:stats':

    IQR, mad, sd, var, xtabs</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:base':

    anyDuplicated, aperm, append, as.data.frame, basename, cbind,
    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,
    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,
    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,
    Position, rank, rbind, Reduce, rownames, sapply, saveRDS, setdiff,
    table, tapply, union, unique, unsplit, which.max, which.min</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: S4Vectors</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'S4Vectors'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following object is masked from 'package:utils':

    findMatches</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:base':

    expand.grid, I, unname</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: IRanges</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: GenomeInfoDb</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: Biobase</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Welcome to Bioconductor

    Vignettes contain introductory material; view with
    'browseVignettes()'. To cite Bioconductor, see
    'citation("Biobase")', and for packages 'citation("pkgname")'.</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'Biobase'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following object is masked from 'package:MatrixGenerics':

    rowMedians</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:matrixStats':

    anyMissing, rowMedians</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: replacing previous import 'S4Arrays::makeNindexFromArrayViewport' by
'DelayedArray::makeNindexFromArrayViewport' when loading 'SummarizedExperiment'</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">methylation</span> <span class="op">&lt;-</span> <span class="fu">readRDS</span><span class="op">(</span><span class="st">"data/methylation.rds"</span><span class="op">)</span></span></code></pre>
</div>
<p>Note: if you want to view the code used to download these data, the
code is available in this repository in the <a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/blob/main/episodes/data/methylation.R" class="external-link"><code>methylation.R</code>
script</a></p>
<p><code>methylation</code> is actually a special Bioconductor
<code>SummarizedExperiment</code> object that summarises lots of
different information about the data. These objects are very useful for
storing all of the information about a dataset in a high-throughput
context. The structure of <code>SummarizedExperiment</code> objects is
described in the <a href="https://www.bioconductor.org/packages/release/bioc/vignettes/SummarizedExperiment/inst/doc/SummarizedExperiment.html" class="external-link">vignettes
on Bioconductor</a>. Here, we show how to extract the information for
analysis.</p>
<p>We can extract</p>
<ul>
<li>the dimensions of the dataset using <code>dim()</code>. Importantly,
in these objects and data structures for computational biology in R
generally, observations are stored as columns and features (in this
case, sites in the genome) are stored as rows. This is in contrast to
usual tabular data, where features or variables are stored as columns
and observations are stored as rows;</li>
<li>assays, (normalised methylation levels here), using
<code>assay()</code>;</li>
<li>sample-level information using <code>colData()</code>.</li>
</ul>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">dim</span><span class="op">(</span><span class="va">methylation</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 5000   37</code></pre>
</div>
<p>You can see in this output that this object has a <code>dim()</code>
of <span class="math inline">\(5000 \\times 37\)</span>, meaning it has
5000 <em>features</em> and 37 <em>observations</em>. To extract the
matrix of methylation M-values, we can use the <code>assay()</code>
function.</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">methyl_mat</span> <span class="op">&lt;-</span> <span class="fu">assay</span><span class="op">(</span><span class="va">methylation</span><span class="op">)</span></span></code></pre>
</div>
<p>The distribution of these M-values looks like this:</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">hist</span><span class="op">(</span><span class="va">methyl_mat</span>, xlab <span class="op">=</span> <span class="st">"M-value"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/02-high-dimensional-regression-rendered-histx-1.png" alt="Histogram of M-values for all features. The distribution appears to be bimodal, with a large number of unmethylated features as well as many methylated features, and many intermediate features." class="figure mx-auto d-block"><figcaption>
Methylation levels are generally bimodally distributed.
</figcaption></figure><p>You can see that there are two peaks in this distribution,
corresponding to features which are largely unmethylated and methylated,
respectively.</p>
<p>Similarly, we can examine the <code>colData()</code>, which
represents the sample-level metadata we have relating to these data. In
this case, the metadata, phenotypes, and groupings in the
<code>colData</code> look like this for the first 6 samples:</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">head</span><span class="op">(</span><span class="fu">colData</span><span class="op">(</span><span class="va">methylation</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<table class="table">
<colgroup>
<col width="8%">
<col width="8%">
<col width="5%">
<col width="2%">
<col width="2%">
<col width="7%">
<col width="6%">
<col width="6%">
<col width="8%">
<col width="11%">
<col width="11%">
<col width="5%">
<col width="5%">
<col width="9%">
</colgroup>
<thead><tr class="header">
<th align="left">Sample_Well</th>
<th align="left">Sample_Name</th>
<th align="right">purity</th>
<th align="left">Sex</th>
<th align="right">Age</th>
<th align="right">weight_kg</th>
<th align="right">height_m</th>
<th align="right">bmi</th>
<th align="left">bmi_clas</th>
<th align="left">Ethnicity_wide</th>
<th align="left">Ethnic_self</th>
<th align="left">smoker</th>
<th align="left">Array</th>
<th align="right">Slide</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">A07</td>
<td align="left">PCA0612</td>
<td align="right">94</td>
<td align="left">M</td>
<td align="right">39</td>
<td align="right">88.45051</td>
<td align="right">1.8542</td>
<td align="right">25.72688</td>
<td align="left">Overweight</td>
<td align="left">Mixed</td>
<td align="left">Hispanic</td>
<td align="left">No</td>
<td align="left">R01C01</td>
<td align="right">201868500150</td>
</tr>
<tr class="even">
<td align="left">C07</td>
<td align="left">NKpan2510</td>
<td align="right">95</td>
<td align="left">M</td>
<td align="right">49</td>
<td align="right">81.19303</td>
<td align="right">1.6764</td>
<td align="right">28.89106</td>
<td align="left">Overweight</td>
<td align="left">Indo-European</td>
<td align="left">Caucasian</td>
<td align="left">No</td>
<td align="left">R03C01</td>
<td align="right">201868500150</td>
</tr>
<tr class="odd">
<td align="left">E07</td>
<td align="left">WB1148</td>
<td align="right">95</td>
<td align="left">M</td>
<td align="right">20</td>
<td align="right">80.28585</td>
<td align="right">1.7526</td>
<td align="right">26.13806</td>
<td align="left">Overweight</td>
<td align="left">Indo-European</td>
<td align="left">Persian</td>
<td align="left">No</td>
<td align="left">R05C01</td>
<td align="right">201868500150</td>
</tr>
<tr class="even">
<td align="left">G07</td>
<td align="left">B0044</td>
<td align="right">97</td>
<td align="left">M</td>
<td align="right">49</td>
<td align="right">82.55381</td>
<td align="right">1.7272</td>
<td align="right">27.67272</td>
<td align="left">Overweight</td>
<td align="left">Indo-European</td>
<td align="left">Caucasian</td>
<td align="left">No</td>
<td align="left">R07C01</td>
<td align="right">201868500150</td>
</tr>
<tr class="odd">
<td align="left">H07</td>
<td align="left">NKpan1869</td>
<td align="right">95</td>
<td align="left">F</td>
<td align="right">33</td>
<td align="right">87.54333</td>
<td align="right">1.7272</td>
<td align="right">29.34525</td>
<td align="left">Overweight</td>
<td align="left">Indo-European</td>
<td align="left">Caucasian</td>
<td align="left">No</td>
<td align="left">R08C01</td>
<td align="right">201868500150</td>
</tr>
<tr class="even">
<td align="left">B03</td>
<td align="left">NKpan1850</td>
<td align="right">93</td>
<td align="left">F</td>
<td align="right">21</td>
<td align="right">87.54333</td>
<td align="right">1.6764</td>
<td align="right">31.15070</td>
<td align="left">Obese</td>
<td align="left">Mixed</td>
<td align="left">Finnish/Creole</td>
<td align="left">No</td>
<td align="left">R02C01</td>
<td align="right">201868590193</td>
</tr>
</tbody>
</table>
<p>In this episode, we will focus on the association between age and
methylation. The following heatmap summarises age and methylation levels
available in the methylation dataset:</p>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"pheatmap"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">age</span> <span class="op">&lt;-</span> <span class="va">methylation</span><span class="op">$</span><span class="va">Age</span></span>
<span></span>
<span><span class="co"># sort methylation values by age </span></span>
<span><span class="va">order</span> <span class="op">&lt;-</span> <span class="fu">order</span><span class="op">(</span><span class="va">age</span><span class="op">)</span></span>
<span><span class="va">age_ord</span> <span class="op">&lt;-</span> <span class="va">age</span><span class="op">[</span><span class="va">order</span><span class="op">]</span></span>
<span><span class="va">methyl_mat_ord</span> <span class="op">&lt;-</span> <span class="va">methyl_mat</span><span class="op">[</span>, <span class="va">order</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># plot heatmap</span></span>
<span><span class="fu">pheatmap</span><span class="op">(</span><span class="va">methyl_mat_ord</span>, </span>
<span>         cluster_cols <span class="op">=</span> <span class="cn">FALSE</span>, </span>
<span>         show_rownames <span class="op">=</span> <span class="cn">FALSE</span>, </span>
<span>         show_colnames <span class="op">=</span> <span class="cn">FALSE</span>, </span>
<span>         legend_title <span class="op">=</span> <span class="st">"M-value"</span>, </span>
<span>         main <span class="op">=</span> <span class="st">"Feature vs Sample"</span>, </span>
<span>         annotation_col <span class="op">=</span> <span class="fu">data.frame</span><span class="op">(</span>age <span class="op">=</span> <span class="va">age_ord</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning in min(x): no non-missing arguments to min; returning Inf</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning in max(x): no non-missing arguments to max; returning -Inf</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in seq.int(rx[1L], rx[2L], length.out = nb): 'from' must be a finite number</code></pre>
</div>
<div id="challenge-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-1" class="callout-inner">
<h3 class="callout-title">Challenge 1</h3>
<div class="callout-content">
<p>Why can we not just fit many linear regression models relating every
combination of feature (<code>colData</code> and assays) and draw
conclusions by associating all variables with significant model
p-values?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>There are a number of problems that this kind of approach presents.
For example:</p>
<ol style="list-style-type: decimal">
<li>If we perform 5000 tests for each of 14 variables, even if there
were no true associations in the data, we’d be likely to observe some
strong spurious associations that arise just from random noise.</li>
<li>We may not have a representative sample for each of these
covariates. For example, we may have very small sample sizes for some
ethnicities, leading to spurious findings.</li>
<li>Without a research question in mind when creating a model, it’s not
clear how we can interpret each model, and rationalising the results
after the fact can be dangerous; it’s easy to make up a “story” that
isn’t grounded in anything but the fact that we have significant
findings.</li>
</ol>
</div>
</div>
</div>
</div>
<p>In general, it is scientifically interesting to explore two modelling
problems using the three types of data:</p>
<ol style="list-style-type: decimal">
<li><p>Predicting methylation levels using age as a predictor. In this
case, we would have 5000 outcomes (methylation levels across the genome)
and a single covariate (age).</p></li>
<li><p>Predicting age using methylation levels as predictors. In this
case, we would have a single outcome (age) which will be predicted using
5000 covariates (methylation levels across the genome).</p></li>
</ol>
<p>The examples in this episode will focus on the first type of problem,
whilst the next episode will focus on the second.</p>
<div id="measuring-dna-methylation" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="measuring-dna-methylation" class="callout-inner">
<h3 class="callout-title">Measuring DNA Methylation</h3>
<div class="callout-content">
<p>DNA methylation is an epigenetic modification of DNA. Generally, we
are interested in the proportion of methylation at many sites or regions
in the genome. DNA methylation microarrays, as we are using here,
measure DNA methylation using two-channel microarrays, where one channel
captures signal from methylated DNA and the other captures unmethylated
signal. These data can be summarised as “Beta values” (<span class="math inline">\(\\beta\)</span> values), which is the ratio of the
methylated signal to the total signal (methylated plus unmethylated).
The <span class="math inline">\(\\beta\)</span> value for site <span class="math inline">\(i\)</span> is calculated as</p>
<p><span class="math display">\[
\beta_i = \frac{
m_i
} {
u_{i} + m_{i}
}
\]</span></p>
<p>where <span class="math inline">\(m\_i\)</span> is the methylated
signal for site <span class="math inline">\(i\)</span> and <span class="math inline">\(u\_i\)</span> is the unmethylated signal for site
<span class="math inline">\(i\)</span>. <span class="math inline">\(\\beta\)</span> values take on a value in the
range <span class="math inline">\([0, 1]\)</span>, with 0 representing a
completely unmethylated site and 1 representing a completely methylated
site.</p>
<p>The M-values we use here are the <span class="math inline">\(\\log\_2\)</span> ratio of methylated versus
unmethylated signal:</p>
<p><span class="math display">\[
M_i = \log_2\left(\frac{m_i}{u_i}\right)
\]</span></p>
<p>M-values are not bounded to an interval as Beta values are, and
therefore can be easier to work with in statistical models.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="regression-with-many-outcomes">Regression with many outcomes<a class="anchor" aria-label="anchor" href="#regression-with-many-outcomes"></a>
</h2>
<hr class="half-width">
<p>In high-throughput studies, it is common to have one or more
phenotypes or groupings that we want to relate to features of interest
(eg, gene expression, DNA methylation levels). In general, we want to
identify differences in the features of interest that are related to a
phenotype or grouping of our samples. Identifying features of interest
that vary along with phenotypes or groupings can allow us to understand
how phenotypes arise or manifest. Analysis of this type is sometimes
referred to using the term <em>differential analysis</em>.</p>
<p>For example, we might want to identify genes that are expressed at a
higher level in mutant mice relative to wild-type mice to understand the
effect of a mutation on cellular phenotypes. Alternatively, we might
have samples from a set of patients, and wish to identify epigenetic
features that are different in young patients relative to old patients,
to help us understand how ageing manifests.</p>
<p>Using linear regression, it is possible to identify differences like
these. However, high-dimensional data like the ones we’re working with
require some special considerations. A first consideration, as we saw
above, is that there are far too many features to fit each one-by-one as
we might do when analysing low-dimensional datasets (for example using
<code>lm()</code> on each feature and checking the linear model
assumptions). A second consideration is that statistical approaches may
behave slightly differently when applied to very high-dimensional data,
compared to low-dimensional data. A third consideration is the speed at
which we can actually compute statistics for data this large – methods
optimised for low-dimensional data may be very slow when applied to
high-dimensional data.</p>
<p>Ideally when performing regression, we want to identify cases like
this, where there is a clear association, and we probably “don’t need”
statistics:</p>
<figure style="text-align: center"><img src="../fig/02-high-dimensional-regression-rendered-example1-1.png" alt="An example of a strong linear association between a continuous phenotype (age) on the x-axis and a feature of interest (DNA methylation at a given locus) on the y-axis. A strong linear relationship with a positive slope exists between the two." class="figure mx-auto d-block"><figcaption>
A scatter plot of age and a feature of interest.
</figcaption></figure><p>or equivalently for a discrete covariate:</p>
<figure style="text-align: center"><img src="../fig/02-high-dimensional-regression-rendered-example2-1.png" alt="An example of a strong linear association between a discrete phenotype (group) on the x-axis and a feature of interest (DNA methylation at a given locus) on the y-axis. The two groups clearly differ with respect to DNA methylation." class="figure mx-auto d-block"><figcaption>
A scatter plot of a grouping and a feature of interest.
</figcaption></figure><p>However, often due to small differences and small sample sizes, the
problem is more difficult:</p>
<figure style="text-align: center"><img src="../fig/02-high-dimensional-regression-rendered-example3-1.png" alt="An example of a strong linear association between a discrete phenotype (group) on the x-axis and a feature of interest (DNA methylation at a given locus) on the y-axis. The two groups seem to differ with respect to DNA methylation, but the relationship is weak." class="figure mx-auto d-block"><figcaption>
A scatter plot of a grouping and a feature of interest.
</figcaption></figure><p>And, of course, we often have an awful lot of features and need to
prioritise a subset of them! We need a rigorous way to prioritise genes
for further analysis.</p>
</section><section><h2 class="section-heading" id="fitting-a-linear-model">Fitting a linear model<a class="anchor" aria-label="anchor" href="#fitting-a-linear-model"></a>
</h2>
<hr class="half-width">
<p>So, in the data we have read in, we have a matrix of methylation
values <span class="math inline">\(X\)</span> and a vector of ages,
<span class="math inline">\(y\)</span>. One way to model this is to see
if we can use age to predict the expected (average) methylation value
for sample <span class="math inline">\(j\)</span> at a given locus <span class="math inline">\(i\)</span>, which we can write as <span class="math inline">\(X\_{ij}\)</span>. We can write that model as:</p>
<p><span class="math display">\[
\mathbf{E}(X_{ij}) = \beta_0 + \beta_1 \text{Age}_j
\]</span></p>
<p>where <span class="math inline">\(\\text{Age}\_j\)</span> is the age
of sample <span class="math inline">\(j\)</span>. In this model, <span class="math inline">\(\\beta\_1\)</span> represents the unit change in
mean methylation level for each unit (year) change in age. For a
specific CpG, we can fit this model and get more information from the
model object. For illustration purposes, here we arbitrarily select the
first CpG in the <code>methyl_mat</code> matrix (the one on its first
row).</p>
<div class="codewrapper sourceCode" id="cb34">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">age</span> <span class="op">&lt;-</span> <span class="va">methylation</span><span class="op">$</span><span class="va">Age</span></span>
<span><span class="co"># methyl_mat[1, ] indicates that the 1st CpG will be used as outcome variable</span></span>
<span><span class="va">lm_age_methyl1</span> <span class="op">&lt;-</span> <span class="fu">lm</span><span class="op">(</span><span class="va">methyl_mat</span><span class="op">[</span><span class="fl">1</span>, <span class="op">]</span> <span class="op">~</span> <span class="va">age</span><span class="op">)</span></span>
<span><span class="va">lm_age_methyl1</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Call:
lm(formula = methyl_mat[1, ] ~ age)

Coefficients:
(Intercept)          age
   0.902334     0.008911  </code></pre>
</div>
<p>We now have estimates for the expected methylation level when age
equals 0 (the intercept) and the change in methylation level for a unit
change in age (the slope). We could plot this linear model:</p>
<div class="codewrapper sourceCode" id="cb36">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">plot</span><span class="op">(</span><span class="va">age</span>, <span class="va">methyl_mat</span><span class="op">[</span><span class="fl">1</span>, <span class="op">]</span>, xlab <span class="op">=</span> <span class="st">"Age"</span>, ylab <span class="op">=</span> <span class="st">"Methylation level"</span>, pch <span class="op">=</span> <span class="fl">16</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span><span class="va">lm_age_methyl1</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/02-high-dimensional-regression-rendered-plot-lm-methyl1-1.png" alt="An example of the relationship between age (x-axis) and methylation levels (y-axis) for an arbitrarily selected CpG. In this case, the y-axis shows methylation levels for the first CpG in our data. The black line shows the fitted regression line (based on the intercept and slope estimates shown above). For this feature, we can see that there is no strong relationship between methylation and age." class="figure mx-auto d-block"><figcaption>
A scatter plot of age versus the methylation level for an arbitrarily
selected CpG side (the one stored as the first column of methyl_mat).
Each dot represents an individual. The black line represents the
estimated linear model.
</figcaption></figure><p>For this feature, we can see that there is no strong relationship
between methylation and age. We could try to repeat this for every
feature in our dataset; however, we have a lot of features! We need an
approach that allows us to assess associations between all of these
features and our outcome while addressing the three considerations we
outlined previously. Before we introduce this approach, let’s go into
detail about how we generally check whether the results of a linear
model are statistically significant.</p>
</section><section><h2 class="section-heading" id="hypothesis-testing-in-linear-regression">Hypothesis testing in linear regression<a class="anchor" aria-label="anchor" href="#hypothesis-testing-in-linear-regression"></a>
</h2>
<hr class="half-width">
<p>Using the linear model we defined above, we can ask questions based
on the estimated value for the regression coefficients. For example, do
individuals with different age have different methylation values for a
given CpG? We usually do this via <em>hypothesis testing</em>. This
framework compares the results that we observed (here, estimated linear
model coefficients) to the results you would expect under a <em>null
hypothesis</em> associated to our question. In the example above, a
suitable null hypothesis would test whether the regression coefficient
associated to age (<span class="math inline">\(\\beta\_1\)</span>) is
equal to zero or not. If <span class="math inline">\(\\beta\_1\)</span>
is equal to zero, the linear model indicates that there is no linear
relationship between age and the methylation level for the CpG
(remember: as its name suggests, linear regression can only be used to
model linear relationships between predictors and outcomes!). In other
words, the answer to our question would be: no!</p>
<p>The output of a linear model typically returns the results associated
with the null hypothesis described above (this may not always be the
most realistic or useful null hypothesis, but it is the one we have by
default!). To be more specific, the test compares our observed results
with a set of hypothetical counter-examples of what we would expect to
observe if we repeated the same experiment and analysis over and over
again under the null hypothesis.</p>
<p>For this linear model, we can use <code>tidy()</code> from the
<strong><code>broom</code></strong> package to extract detailed
information about the coefficients and the associated hypothesis tests
in this model:</p>
<div class="codewrapper sourceCode" id="cb37">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"broom"</span><span class="op">)</span></span>
<span><span class="fu">tidy</span><span class="op">(</span><span class="va">lm_age_methyl1</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code># A tibble: 2 × 5
  term        estimate std.error statistic p.value
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
1 (Intercept)  0.902      0.344      2.62   0.0129
2 age          0.00891    0.0100     0.888  0.381 </code></pre>
</div>
<p>The standard errors (<code>std.error</code>) represent the
statistical uncertainty in our regression coefficient estimates (often
referred to as <em>effect size</em>). The test statistics and p-values
(<code>statistic</code> and <code>p.value</code>) represent measures of
how (un)likely it would be to observe results like this under the “null
hypothesis”. For coefficient <span class="math inline">\(k\)</span> in a
linear model (in our case, it would be the slope), the test statistic
calculated in <code>statistic</code> above is a t-statistic given
by:</p>
<p><span class="math display">\[
t_{k} = \frac{\hat{\beta}_{k}}{SE\left(\hat{\beta}_{k}\right)}
\]</span></p>
<p><span class="math inline">\(SE\\left(\\hat{\\beta}\_{k}\\right)\)</span>
measures the uncertainty we have in our effect size estimate. Knowing
what distribution these t-statistics follow under the null hypothesis
allows us to determine how unlikely it would be for us to observe what
we have under those circumstances, if we repeated the experiment and
analysis over and over again. To demonstrate how the t-statistics are
calculated, we can compute them “by hand”:</p>
<div class="codewrapper sourceCode" id="cb39">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">table_age_methyl1</span> <span class="op">&lt;-</span> <span class="fu">tidy</span><span class="op">(</span><span class="va">lm_age_methyl1</span><span class="op">)</span></span>
<span><span class="va">tvals</span> <span class="op">&lt;-</span> <span class="va">table_age_methyl1</span><span class="op">$</span><span class="va">estimate</span> <span class="op">/</span> <span class="va">table_age_methyl1</span><span class="op">$</span><span class="va">std.error</span></span>
<span><span class="fu">all.equal</span><span class="op">(</span><span class="va">tvals</span>, <span class="va">table_age_methyl1</span><span class="op">$</span><span class="va">statistic</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] TRUE</code></pre>
</div>
<p>We can see that the t-statistic is just the ratio between the
coefficient estimate and the standard error.</p>
<p>Calculating the p-values is a bit more tricky. Specifically, it is
the proportion of the distribution of the test statistic under the null
hypothesis that is <em>as extreme or more extreme</em> than the observed
value of the test statistic. This is easy to observe visually, by
plotting the theoretical distribution of the test statistic under the
null hypothesis (see next call-out box for more details about it):</p>
<figure style="text-align: center"><img src="../fig/02-high-dimensional-regression-rendered-tdist-1.png" alt="Density plot of a t-distribution showing the observed test statistics (here, t-statistics). The p-values, visualised here with shaded regions, represent the portion of the null distribution that is as extreme or more extreme as the observed test statistics, which are shown as dashed lines." class="figure mx-auto d-block"><figcaption>
The p-value for a regression coefficient represents how often it’d be
observed under the null.
</figcaption></figure><p>The red-ish shaded region represents the portion of the distribution
of the test statistic under the null hypothesis that is equal or greater
to the value we observe for the intercept term. As our null hypothesis
relates to a 2-tailed test (as the null hypothesis states that the
regression coefficient is equal to zero, we would reject it if the
regression coefficient is substantially larger <strong>or</strong>
smaller than zero), the p-value for the test is twice the value of the
shaded region. In this case, the shaded region is small relative to the
total area of the null distribution; therefore, the p-value is small
(<span class="math inline">\(p=0.013\)</span>). The blue-ish shaded
region represents the same measure for the slope term; this is larger,
relative to the total area of the distribution, therefore the p-value is
larger than the one for the intercept term (<span class="math inline">\(p=0.381\)</span>). The p-value is a function of
the test statistic: the ratio between the effect size we’re estimating
and the uncertainty we have in that effect. A large effect with large
uncertainty may not lead to a small p-value, and a small effect with
small uncertainty may lead to a small p-value.</p>
<div id="calculating-p-values-from-a-linear-model" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="calculating-p-values-from-a-linear-model" class="callout-inner">
<h3 class="callout-title">Calculating p-values from a linear model</h3>
<div class="callout-content">
<p>Manually calculating the p-value for a linear model is a little bit
more complex than calculating the t-statistic. The intuition posted
above is definitely sufficient for most cases, but for completeness,
here is how we do it:</p>
<p>Since the statistic in a linear model is a t-statistic, it follows a
student t distribution under the null hypothesis, with degrees of
freedom (a parameter of the student t-distribution) given by the number
of observations minus the number of coefficients fitted, in this case
<span class="math inline">\(37 - 2 = 35\)</span>. We want to know what
portion of the distribution function of the test statistic is as extreme
as, or more extreme than, the value we observed. The function
<code>pt()</code>(similar to<code>pnorm()</code>, etc) can give us this
information.</p>
<p>Since we’re not sure if the coefficient will be larger or smaller
than zero, we want to do a 2-tailed test. Therefore we take the absolute
value of the t-statistic, and look at the upper rather than lower tail.
In the figure above the shaded areas are only looking at “half” of the
t-distribution (which is symmetric around zero), therefore we multiply
the shaded area by 2 in order to calculate the p-value.</p>
<p>Combining all of this gives us:</p>
<div class="codewrapper sourceCode" id="cb41">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pvals</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="fu">pt</span><span class="op">(</span><span class="fu">abs</span><span class="op">(</span><span class="va">tvals</span><span class="op">)</span>, df <span class="op">=</span> <span class="va">lm_age_methyl1</span><span class="op">$</span><span class="va">df</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu">all.equal</span><span class="op">(</span><span class="va">table_age_methyl1</span><span class="op">$</span><span class="va">p.value</span>, <span class="va">pvals</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] TRUE</code></pre>
</div>
</div>
</div>
</div>
<div id="challenge-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-2" class="callout-inner">
<h3 class="callout-title">Challenge 2</h3>
<div class="callout-content">
<p>In the model we fitted, the estimate for the intercept is 0.902 and
its associated p-value is 0.0129. What does this mean?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>The first coefficient in a linear model like this is the intercept,
which measures the mean of the outcome (in this case, the methylation
value for the first CpG) when age is zero. In this case, the intercept
estimate is 0.902. However, this is not a particularly noteworthy
finding as we do not have any observations with age zero (nor even any
with age &lt; 20!).</p>
<p>The reported p-value is associated to the following null hypothesis:
the intercept (<span class="math inline">\(\\beta\_0\)</span> above) is
equal to zero. Using the usual significance threshold of 0.05, we reject
the null hypothesis as the p-value is smaller than 0.05. However, it is
not really interesting if this intercept is zero or not, since we
probably do not care what the methylation level is when age is zero. In
fact, this question does not even make much sense! In this example, we
are more interested in the regression coefficient associated to age, as
that can tell us whether there is a linear relationship between age and
methylation for the CpG.</p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="fitting-a-lot-of-linear-models">Fitting a lot of linear models<a class="anchor" aria-label="anchor" href="#fitting-a-lot-of-linear-models"></a>
</h2>
<hr class="half-width">
<p>In the linear model above, we are generally interested in the second
regression coefficient (often referred to as <em>slope</em>) which
measures the linear relationship between age and methylation levels. For
the first CpG, here is its estimate:</p>
<div class="codewrapper sourceCode" id="cb43">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">coef_age_methyl1</span> <span class="op">&lt;-</span> <span class="fu">tidy</span><span class="op">(</span><span class="va">lm_age_methyl1</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span>, <span class="op">]</span></span>
<span><span class="va">coef_age_methyl1</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code># A tibble: 1 × 5
  term  estimate std.error statistic p.value
  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
1 age    0.00891    0.0100     0.888   0.381</code></pre>
</div>
<p>In this case, the p-value is equal to 0.381 and therefore we cannot
reject the null hypothesis: there is no statistical evidence to suggest
that the regression coefficient associated to age is not equal to
zero.</p>
<p>Now, we could do this for every feature (CpG) in the dataset and rank
the results based on their test statistic or associated p-value.
However, fitting models in this way to 5000 features is not very
computationally efficient, and it would also be laborious to do
programmatically. There are ways to get around this, but first let us
talk about what exactly we are doing when we look at significance tests
in this context.</p>
</section><section><h2 class="section-heading" id="sharing-information-across-outcome-variables">Sharing information across outcome variables<a class="anchor" aria-label="anchor" href="#sharing-information-across-outcome-variables"></a>
</h2>
<hr class="half-width">
<p>We are going to introduce an idea that allows us to take advantage of
the fact that we carry out many tests at once on structured data. We can
leverage this fact to <em>share information</em> between model
parameters. The insight that we use to perform <em>information
pooling</em> or sharing is derived from our knowledge about the
structure of the data. For example, in a high-throughput experiment like
a DNA methylation assay, we know that all of the features were measured
simultaneously, using the same technique. This means that generally, we
expect the base-level variability for each feature to be broadly
similar.</p>
<p>This can enable us to get a better estimate of the uncertainty of
model parameters than we could get if we consider each feature in
isolation. So, to share information between features allows us to get
more robust estimators. Remember that the t-statistic for coefficient
<span class="math inline">\(\\beta\_k\)</span> in a linear model is the
ratio between the coefficient estimate and its standard error:</p>
<p><span class="math display">\[
t_{k} = \frac{\hat{\beta}_{k}}{SE\left(\hat{\beta}_{k}\right)}
\]</span></p>
<p>It is clear that large effect sizes will likely lead to small
p-values, as long as the standard error for the coefficent is not large.
However, the standard error is affected by the amount of noise, as we
saw earlier. If we have a small number of observations, it is common for
the noise for some features to be extremely small simply by chance.
This, in turn, causes small p-values for these features, which may give
us unwarranted confidence in the level of certainty we have in the
results (false positives).</p>
<p>There are many statistical methods in genomics that use this type of
approach to get better estimates by pooling information between features
that were measured simultaneously using the same techniques. Here we
will focus on the package <strong><code>limma</code></strong>, which is
an established software package used to fit linear models, originally
for the gene expression micro-arrays that were common in the 2000s, but
which is still in use in RNAseq experiments, among others. The authors
of <strong><code>limma</code></strong> made some assumptions about the
distributions that these follow, and pool information across genes to
get a better estimate of the uncertainty in effect size estimates. It
uses the idea that noise levels should be similar between features to
<em>moderate</em> the estimates of the test statistic by shrinking the
estimates of standard errors towards a common value. This results in a
<em>moderated t-statistic</em>.</p>
<p>The process of running a model in <strong><code>limma</code></strong>
is somewhat different to what you may have seen when running linear
models. Here, we define a <em>model matrix</em> or <em>design
matrix</em>, which is a way of representing the coefficients that should
be fit in each linear model. These are used in similar ways in many
different modelling libraries.</p>
<div id="what-is-a-model-matrix" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="what-is-a-model-matrix" class="callout-inner">
<h3 class="callout-title">What is a model matrix?</h3>
<div class="callout-content">
<p>R fits a regression model by choosing the vector of regression
coefficients that minimises the differences between outcome values and
predicted values using the covariates (or predictor variables). To get
predicted values, we multiply the matrix of predictors by the
coefficients. The latter matrix is called the <strong>model
matrix</strong> (or design matrix). It has one row for each observation
and one column for each predictor, plus one additional column of ones
(the intercept column). Many R libraries contruct the model matrix
behind the scenes, but <strong><code>limma</code></strong> does not.
Usually, the model matrix can be extracted from a model fit using the
function <code>model.matrix()</code>. Here is an example of a model
matrix for the methylation model:</p>
<div class="codewrapper sourceCode" id="cb45">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">design_age</span> <span class="op">&lt;-</span> <span class="fu">model.matrix</span><span class="op">(</span><span class="va">lm_age_methyl1</span><span class="op">)</span> <span class="co"># model matrix</span></span>
<span><span class="fu">head</span><span class="op">(</span><span class="va">design_age</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>                    (Intercept) age
201868500150_R01C01           1  39
201868500150_R03C01           1  49
201868500150_R05C01           1  20
201868500150_R07C01           1  49
201868500150_R08C01           1  33
201868590193_R02C01           1  21</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb47">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">dim</span><span class="op">(</span><span class="va">design_age</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 37  2</code></pre>
</div>
<p>As you can see, the model matrix has the same number of rows as our
methylation data has samples. It also has two columns - one for the
intercept (similar to the linear model we fit above) and one for age.
This happens “under the hood” when fitting a linear model with
<code>lm()</code>, but here we have to specify it directly. The <a href="https://www.bioconductor.org/packages/release/bioc/vignettes/limma/inst/doc/usersguide.pdf" class="external-link">limma
user manual</a> has more detail on how to make model matrices for
different types of experimental design, but here we are going to stick
with this simple two-variable case.</p>
</div>
</div>
</div>
<p>We pass our methylation data to <code>lmFit()</code>, specifying the
model matrix. Internally, this function runs <code>lm()</code> on each
row of the data in an efficient way. The function <code>eBayes()</code>,
when applied to the output of <code>lmFit()</code>, performs the pooled
estimation of standard errors that results in the moderated t-statistics
and resulting p-values.</p>
<div class="codewrapper sourceCode" id="cb49">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"limma"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'limma'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following object is masked from 'package:BiocGenerics':

    plotMA</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb52">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">design_age</span> <span class="op">&lt;-</span> <span class="fu">model.matrix</span><span class="op">(</span><span class="va">lm_age_methyl1</span><span class="op">)</span> <span class="co"># model matrix</span></span>
<span><span class="va">fit_age</span> <span class="op">&lt;-</span> <span class="fu">lmFit</span><span class="op">(</span><span class="va">methyl_mat</span>, design <span class="op">=</span> <span class="va">design_age</span><span class="op">)</span></span>
<span><span class="va">fit_age</span> <span class="op">&lt;-</span> <span class="fu">eBayes</span><span class="op">(</span><span class="va">fit_age</span><span class="op">)</span></span></code></pre>
</div>
<p>To obtain the results of the linear models, we can use the
<code>topTable()</code> function. By default, this returns results for
the first coefficient in the model. As we saw above when using
<code>lm()</code>, and when we defined <code>design_age</code> above,
the first coefficient relates to the intercept term, which we are not
particularly interested in here; therefore we specify
<code>coef = 2</code>. Further, <code>topTable()</code> by default only
returns the top 10 results. To see all of the results in the data, we
specify <code>number = nrow(fit_age)</code> to ensure that it returns a
row for every row of the input matrix.</p>
<div class="codewrapper sourceCode" id="cb53">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">toptab_age</span> <span class="op">&lt;-</span> <span class="fu">topTable</span><span class="op">(</span><span class="va">fit_age</span>, coef <span class="op">=</span> <span class="fl">2</span>, number <span class="op">=</span> <span class="fu">nrow</span><span class="op">(</span><span class="va">fit_age</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">head</span><span class="op">(</span><span class="va">toptab_age</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>                 logFC    AveExpr         t      P.Value   adj.P.Val        B
cg08446924 -0.02571353 -0.4185868 -6.039068 5.595675e-07 0.002797837 5.131574
cg06493994  0.01550941 -2.1057877  5.593988 2.239813e-06 0.005599533 3.747986
cg17661642  0.02266668 -2.0527722  5.358739 4.658336e-06 0.006048733 3.019698
cg05168977  0.02276336 -2.2918472  5.346500 4.838987e-06 0.006048733 2.981904
cg24549277  0.01975577 -1.7466088  4.939242 1.708355e-05 0.011508818 1.731821
cg04436528 -0.01943612  0.7033503 -4.917179 1.828563e-05 0.011508818 1.664608</code></pre>
</div>
<p>The output of <code>topTable</code> includes the coefficient, here
termed a log fold change <code>logFC</code>, the average level
(<code>aveExpr</code>), the t-statistic <code>t</code>, the p-value
(<code>P.Value</code>), and the <em>adjusted</em> p-value
(<code>adj.P.Val</code>). We’ll cover what an adjusted p-value is very
shortly. The table also includes <code>B</code>, which represents the
log-odds that a feature is signficantly different, which we won’t cover
here, but which will generally be a 1-1 transformation of the p-value.
The coefficient estimates here are termed <code>logFC</code> for legacy
reasons relating to how microarray experiments were traditionally
performed. There are more details on this topic in many places, for
example <a href="https://kasperdanielhansen.github.io/genbioconductor/html/limma.html" class="external-link">this
tutorial by Kasper D. Hansen</a></p>
<p>Now we have estimates of effect sizes and p-values for the
association between methylation level at each locus and age for our 37
samples. It’s useful to create a plot of effect size estimates (model
coefficients) against p-values for each of these linear models, to
visualise the magnitude of effects and the statistical significance of
each. These plots are often called “volcano plots”, because they
resemble an eruption.</p>
<div class="codewrapper sourceCode" id="cb55">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">plot</span><span class="op">(</span><span class="va">toptab_age</span><span class="op">$</span><span class="va">logFC</span>, <span class="op">-</span><span class="fu">log10</span><span class="op">(</span><span class="va">toptab_age</span><span class="op">$</span><span class="va">P.Value</span><span class="op">)</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Effect size"</span>, ylab <span class="op">=</span> <span class="fu">bquote</span><span class="op">(</span><span class="op">-</span><span class="va">log</span><span class="op">[</span><span class="fl">10</span><span class="op">]</span><span class="op">(</span><span class="va">p</span><span class="op">-</span><span class="va">value</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    pch <span class="op">=</span> <span class="fl">19</span></span>
<span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/02-high-dimensional-regression-rendered-limmavolc1-1.png" alt="A plot of -log10(p) against effect size estimates for a regression of age against methylation using limma." class="figure mx-auto d-block"><figcaption>
Plotting p-values against effect sizes using limma; the results are
similar to a standard linear model.
</figcaption></figure><p>In this figure, every point represents a feature of interest. The
x-axis represents the effect size observed for that feature in a linear
model, while the y-axis is the <span class="math inline">\(-\\log\_{10}(\\text{p-value})\)</span>, where
larger values indicate increasing statistical evidence of a non-zero
effect size. A positive effect size represents increasing methylation
with increasing age, and a negative effect size represents decreasing
methylation with increasing age. Points higher on the y-axis represent
features for which we think the results we observed would be very
unlikely under the null hypothesis.</p>
<p>Since we want to identify features that have different methylation
levels in different age groups, in an ideal case there would be clear
separation between “null” and “non-null” features. However, usually we
observe results as we do here: there is a continuum of effect sizes and
p-values, with no clear separation between these two classes of
features. While statistical methods exist to derive insights from
continuous measures like these, it is often convenient to obtain a list
of features which we are confident have non-zero effect sizes. This is
made more difficult by the number of tests we perform.</p>
<div id="challenge-3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-3" class="callout-inner">
<h3 class="callout-title">Challenge 3</h3>
<div class="callout-content">
<p>The effect size estimates are very small, and yet many of the
p-values are well below a usual significance level of p &lt; 0.05. Why
is this?</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>Because age has a much larger range than methylation levels, the unit
change in methylation level even for a strong relationship is very
small!</p>
<p>As we mentioned, the p-value is a function of both the effect size
estimate and the uncertainty (standard error) of that estimate. Because
the uncertainty in our estimates is much smaller than the estimates
themselves, the p-values are also small.</p>
<p>If we predicted age using methylation level, it is likely we would
see much larger coefficients, though broadly similar p-values!</p>
</div>
</div>
</div>
</div>
<p>It is worthwhile considering what exactly the effect of the
<em>moderation</em> or information sharing that
<strong><code>limma</code></strong> performs has on our results. To do
this, let us compare the effect sizes estimates and p-values from the
two approaches.</p>
<figure style="text-align: center"><img src="../fig/02-high-dimensional-regression-rendered-plot-limma-lm-effect-1.png" alt="A scatter plot of the effect size using limmma vs. those using lm. The plot also shows a straight line through all points showing that the effect sizes are the same." class="figure mx-auto d-block"><figcaption>
Plot of effect sizes using limma vs. those using lm.
</figcaption></figure><p>These are exactly identical! This is because
<strong><code>limma</code></strong> does not perform any sharing of
information when estimating effect sizes. This is in contrast to similar
packages that apply shrinkage to the effect size estimates, like
<strong><code>DESeq2</code></strong>. These often use information
sharing to shrink or moderate the effect size estimates, in the case of
<strong><code>DESeq2</code></strong> by again sharing information
between features about sample-to-sample variability. In contrast, let us
look at the p-values from <strong><code>limma</code></strong> and R’s
built-in <code>lm()</code> function:</p>
<figure style="text-align: center"><img src="../fig/02-high-dimensional-regression-rendered-plot-limma-lm-pval-1.png" alt="A scatter plot of the p-values using limma vs. those using lm. A straight line is also displayed, showing that the p-values for limma tend to be smaller than those using lm towards the left of the plot and higher towards the right of the plot." class="figure mx-auto d-block"><figcaption>
Plot of p-values using limma vs. those using lm.
</figcaption></figure><p>We can see that for the vast majority of features, the results are
broadly similar. There seems to be a minor general tendency for
<strong><code>limma</code></strong> to produce smaller p-values, but for
several features, the p-values from limma are considerably larger than
the p-values from <code>lm()</code>. This is because the information
sharing tends to shrink large standard error estimates downwards and
small estimates upwards. When the degree of statistical significance is
due to an abnormally small standard error rather than a large effect,
this effect results in this prominent reduction in statistical
significance, which has been shown to perform well in case studies. The
degree of shrinkage generally depends on the amount of pooled
information and the strength of the evidence independent of pooling. For
example, with very few samples and many features, information sharing
has a larger effect, because there are a lot of genes that can be used
to provide pooled estimates, and the evidence from the data that this is
weighed against is relatively sparse. In contrast, when there are many
samples and few features, there is not much opportunity to generate
pooled estimates, and the evidence of the data can easily outweigh the
pooling.</p>
<p>Shrinkage methods like these ones can be complex to implement and
understand, but it is useful to develop an intuition about why these
approaches may be more precise and sensitive than the naive approach of
fitting a model to each feature separately.</p>
<div id="challenge-4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-4" class="callout-inner">
<h3 class="callout-title">Challenge 4</h3>
<div class="callout-content">
<ol style="list-style-type: decimal">
<li>Try to run the same kind of linear model with smoking status as
covariate instead of age, and making a volcano plot. <em>Note: smoking
status is stored as</em> <code>methylation$smoker</code>.</li>
<li>We saw in the example in the lesson that this information sharing
can lead to larger p-values. Why might this be preferable?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4"> Show me the solution </h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" aria-labelledby="headingSolution4" data-bs-parent="#accordionSolution4">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li>The following code runs the same type of model with smoking
status:</li>
</ol>
<div class="codewrapper sourceCode" id="cb56">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">design_smoke</span> <span class="op">&lt;-</span> <span class="fu">model.matrix</span><span class="op">(</span><span class="op">~</span><span class="va">methylation</span><span class="op">$</span><span class="va">smoker</span><span class="op">)</span></span>
<span><span class="va">fit_smoke</span> <span class="op">&lt;-</span> <span class="fu">lmFit</span><span class="op">(</span><span class="va">methyl_mat</span>, design <span class="op">=</span> <span class="va">design_smoke</span><span class="op">)</span></span>
<span><span class="va">fit_smoke</span> <span class="op">&lt;-</span> <span class="fu">eBayes</span><span class="op">(</span><span class="va">fit_smoke</span><span class="op">)</span></span>
<span><span class="va">toptab_smoke</span> <span class="op">&lt;-</span> <span class="fu">topTable</span><span class="op">(</span><span class="va">fit_smoke</span>, coef <span class="op">=</span> <span class="fl">2</span>, number <span class="op">=</span> <span class="fu">nrow</span><span class="op">(</span><span class="va">fit_smoke</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">toptab_smoke</span><span class="op">$</span><span class="va">logFC</span>, <span class="op">-</span><span class="fu">log10</span><span class="op">(</span><span class="va">toptab_smoke</span><span class="op">$</span><span class="va">P.Value</span><span class="op">)</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Effect size"</span>, ylab <span class="op">=</span> <span class="fu">bquote</span><span class="op">(</span><span class="op">-</span><span class="va">log</span><span class="op">[</span><span class="fl">10</span><span class="op">]</span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    pch <span class="op">=</span> <span class="fl">19</span></span>
<span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/02-high-dimensional-regression-rendered-limmavolc2-1.png" alt="A plot of -log10(p) against effect size estimates for a regression of smoking status against methylation using limma." class="figure mx-auto d-block"><figcaption>
A plot of significance against effect size for a regression of smoking
against methylation.
</figcaption></figure><ol start="2" style="list-style-type: decimal">
<li>Being a bit more conservative when identifying features can help to
avoid false discoveries. Furthermore, when rejecting the null hypothesis
is based more on a small standard error resulting from abnormally low
levels of variability for a given feature, we might want to be a bit
more conservative in our expectations.</li>
</ol>
</div>
</div>
</div>
</div>
<div id="shrinkage" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="shrinkage" class="callout-inner">
<h3 class="callout-title">Shrinkage</h3>
<div class="callout-content">
<p>Shrinkage is an intuitive term for an effect of information sharing,
and is something observed in a broad range of statistical models. Often,
shrinkage is induced by a <em>multilevel</em> modelling approach or by
<em>Bayesian</em> methods.</p>
<p>The general idea is that these models incorporate information about
the structure of the data into account when fitting the parameters. We
can share information between features because of our knowledge about
the data structure; this generally requires careful consideration about
how the data were generated and the relationships within.</p>
<p>An example people often use is estimating the effect of attendance on
grades in several schools. We can assume that this effect is similar in
different schools (but maybe not identical), so we can <em>share
information</em> about the effect size between schools and shrink our
estimates towards a common value.</p>
<p>For example in <strong><code>DESeq2</code></strong>, the authors used
the observation that genes with similar expression counts in RNAseq data
have similar <em>dispersion</em>, and a better estimate of these
dispersion parameters makes estimates of fold changes much more stable.
Similarly, in <strong><code>limma</code></strong> the authors made the
assumption that in the absence of biological effects, we can often
expect the technical variation in the measurement of the expression of
each of the genes to be broadly similar. Again, better estimates of
variability allow us to prioritise genes in a more reliable way.</p>
<p>There are many good resources to learn about this type of approach,
including:</p>
<ul>
<li><a href="https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/" class="external-link">a
blog post by TJ Mahr</a></li>
<li><a href="https://gumroad.com/l/empirical-bayes" class="external-link">a book by David
Robinson</a></li>
<li><a href="https://www.stat.columbia.edu/~gelman/arm/" class="external-link">a (relatively
technical) book by Gelman and Hill</a></li>
</ul>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="the-problem-of-multiple-tests">The problem of multiple tests<a class="anchor" aria-label="anchor" href="#the-problem-of-multiple-tests"></a>
</h2>
<hr class="half-width">
<p>With such a large number of features, it would be useful to decide
which features are “interesting” or “significant” for further study.
However, if we were to apply a normal significance threshold of 0.05, it
would be likely we end up with a lot of false positives. This is because
a p-value threshold like this represents a <span class="math inline">\(\\frac{1}{20}\)</span> chance that we observe
results as extreme or more extreme under the null hypothesis (that there
is no assocation between age and methylation level). If we carry out
many more than 20 such tests, we can expect to see situations where,
despite the null hypothesis being true, we observe observe signifiant
p-values due to random chance. To demonstrate this, it is useful to see
what happens if we permute (scramble) the age values and run the same
test again:</p>
<div class="codewrapper sourceCode" id="cb57">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">set.seed</span><span class="op">(</span><span class="fl">123</span><span class="op">)</span> </span>
<span><span class="va">age_perm</span> <span class="op">&lt;-</span> <span class="va">age</span><span class="op">[</span><span class="fu">sample</span><span class="op">(</span><span class="fu">ncol</span><span class="op">(</span><span class="va">methyl_mat</span><span class="op">)</span>, <span class="fu">ncol</span><span class="op">(</span><span class="va">methyl_mat</span><span class="op">)</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">design_age_perm</span> <span class="op">&lt;-</span> <span class="fu">model.matrix</span><span class="op">(</span><span class="op">~</span><span class="va">age_perm</span><span class="op">)</span></span>
<span></span>
<span><span class="va">fit_age_perm</span> <span class="op">&lt;-</span> <span class="fu">lmFit</span><span class="op">(</span><span class="va">methyl_mat</span>, design <span class="op">=</span> <span class="va">design_age_perm</span><span class="op">)</span></span>
<span><span class="va">fit_age_perm</span> <span class="op">&lt;-</span> <span class="fu">eBayes</span><span class="op">(</span><span class="va">fit_age_perm</span><span class="op">)</span></span>
<span><span class="va">toptab_age_perm</span> <span class="op">&lt;-</span> <span class="fu">topTable</span><span class="op">(</span><span class="va">fit_age_perm</span>, coef <span class="op">=</span> <span class="fl">2</span>, number <span class="op">=</span> <span class="fu">nrow</span><span class="op">(</span><span class="va">fit_age_perm</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">toptab_age_perm</span><span class="op">$</span><span class="va">logFC</span>, <span class="op">-</span><span class="fu">log10</span><span class="op">(</span><span class="va">toptab_age_perm</span><span class="op">$</span><span class="va">P.Value</span><span class="op">)</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Effect size"</span>, ylab <span class="op">=</span> <span class="fu">bquote</span><span class="op">(</span><span class="op">-</span><span class="va">log</span><span class="op">[</span><span class="fl">10</span><span class="op">]</span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    pch <span class="op">=</span> <span class="fl">19</span></span>
<span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>h <span class="op">=</span> <span class="op">-</span><span class="fu">log10</span><span class="op">(</span><span class="fl">0.05</span><span class="op">)</span>, lty <span class="op">=</span> <span class="st">"dashed"</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/02-high-dimensional-regression-rendered-volcplotfake-1.png" alt="Plot of -log10(p) against effect size estimates for a regression of a made-up feature against methylation level for each feature in the data. A dashed line represents a 0.05 significance level." class="figure mx-auto d-block"><figcaption>
Plotting p-values against effect sizes for a randomised outcome shows we
still observe ‘significant’ results.
</figcaption></figure><p>Since we have generated a random sequence of ages, we have no reason
to suspect that there is a true association between methylation levels
and this sequence of random numbers. However, you can see that the
p-value for many features is still lower than a traditional significance
level of <span class="math inline">\(p=0.05\)</span>. In fact, here 226
features are significant at p &lt; 0.05. If we were to use this fixed
threshold in a real experiment, it is likely that we would identify many
features as associated with age, when the results we are observing are
simply due to chance.</p>
<div id="challenge-5" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-5" class="callout-inner">
<h3 class="callout-title">Challenge 5</h3>
<div class="callout-content">
<ol style="list-style-type: decimal">
<li>If we run 5000 tests, even if there are no true differences, how
many of them (on average) will be statistically significant at a
threshold of <span class="math inline">\(p \&lt; 0.05\)</span>?</li>
<li>Why would we want to be conservative in labelling features as
significantly different? By conservative, we mean to err towards
labelling true differences as “not significant” rather than vice
versa.</li>
<li>How could we account for a varying number of tests to ensure
“significant” changes are truly different?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5"> Show me the solution </h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" aria-labelledby="headingSolution5" data-bs-parent="#accordionSolution5">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li>By default we expect <span class="math inline">\(5000 \\times 0.05 =
250\)</span> features to be statistically significant under the null
hypothesis, because p-values should always be uniformly distributed
under the null hypothesis.</li>
<li>Features that we label as “significantly different” will often be
reported in manuscripts. We may also spend time and money investigating
them further, computationally or in the lab. Therefore, spurious results
have a real cost for ourselves and for others.</li>
<li>One approach to controlling for the number of tests is to divide our
significance threshold by the number of tests performed. This is termed
“Bonferroni correction” and we’ll discuss this further now.</li>
</ol>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="adjusting-for-multiple-tests">Adjusting for multiple tests<a class="anchor" aria-label="anchor" href="#adjusting-for-multiple-tests"></a>
</h2>
<hr class="half-width">
<p>When performing many statistical tests to categorise features, we are
effectively classifying features as “non-significant” or “significant”,
that latter meaning those for which we reject the null hypothesis. We
also generally hope that there is a subset of features for which the
null hypothesis is truly false, as well as many for which the null truly
does hold. We hope that for all features for which the null hypothesis
is true, we accept it, and for all features for which the null
hypothesis is not true, we reject it. As we showed in the example with
permuted age, with a large number of tests it is inevitable that we will
get some of these wrong.</p>
<p>We can think of these features as being “truly different” or “not
truly different”<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Using this idea, we can see that each
categorisation we make falls into four categories:</p>
<table class="table">
<colgroup>
<col width="43%">
<col width="38%">
<col width="17%">
</colgroup>
<thead><tr class="header">
<th align="right">True outcome</th>
<th align="right">Label as different</th>
<th align="right">Label as not different</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">Truly different</td>
<td align="right">True positive</td>
<td align="right">False negative</td>
</tr>
<tr class="even">
<td align="right">Truly not different</td>
<td align="right">False positive</td>
<td align="right">True negative</td>
</tr>
</tbody>
</table>
<p>If the null hypothesis was true for every feature, then as we perform
more and more tests we’d tend to correctly categorise most results as
negative. However, since p-values are uniformly distributed under the
null, at a significance level of 5%, 5% of all results will be
“significant” even though we would expect to see these results, given
the null hypothesis is true, simply by chance. These would fall under
the label “false positives” in the table above, and are also termed
“false discoveries.”</p>
<p>There are two common ways of controlling these false discoveries. The
first is to say, when we’re doing <span class="math inline">\(n\)</span>
tests, that we want to have the same certainty of making one false
discovery with <span class="math inline">\(n\)</span> tests as we have
if we’re only doing one test. This is “Bonferroni” correction,<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> which
divides the significance level by the number of tests performed, <span class="math inline">\(n\)</span>. Equivalently, we can use the
non-transformed p-value threshold but multiply our p-values by the
number of tests. This is often very conservative, especially with a lot
of features!</p>
<div class="codewrapper sourceCode" id="cb58">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">p_raw</span> <span class="op">&lt;-</span> <span class="va">toptab_age</span><span class="op">$</span><span class="va">P.Value</span></span>
<span><span class="va">p_fwer</span> <span class="op">&lt;-</span> <span class="fu">p.adjust</span><span class="op">(</span><span class="va">p_raw</span>, method <span class="op">=</span> <span class="st">"bonferroni"</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">p_raw</span>, <span class="va">p_fwer</span>, pch <span class="op">=</span> <span class="fl">16</span>, log<span class="op">=</span><span class="st">"xy"</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>v <span class="op">=</span> <span class="fl">0.05</span>, lty <span class="op">=</span> <span class="st">"dashed"</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>h <span class="op">=</span> <span class="fl">0.05</span>, lty <span class="op">=</span> <span class="st">"dashed"</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/02-high-dimensional-regression-rendered-p-fwer-1.png" alt="Plot of Bonferroni-adjusted p-values (y) against unadjusted p-values (x). A dashed black line represents the identity (where x=y), while dashed red lines represent 0.05 significance thresholds." class="figure mx-auto d-block"><figcaption>
Bonferroni correction often produces very large p-values, especially
with low sample sizes.
</figcaption></figure><p>You can see that the p-values are exactly one for the vast majority
of tests we performed! This is not ideal sometimes, because
unfortunately we usually don’t have very large sample sizes in health
sciences.</p>
<p>The second main way of controlling for multiple tests is to control
the <em>false discovery rate (FDR)</em>.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> This is the proportion
of false positives, or false discoveries, we’d expect to get each time
if we repeated the experiment over and over.</p>
<ol style="list-style-type: decimal">
<li>Rank the p-values</li>
<li>Assign each a rank (1 is smallest)</li>
<li>Calculate the critical value <span class="math display">\[
  q = \left(\frac{i}{m}\right)Q
  \]</span>, where <span class="math inline">\(i\)</span> is rank, <span class="math inline">\(m\)</span> is the number of tests, and <span class="math inline">\(Q\)</span> is the false discovery rate we want to
target.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>
</li>
<li>Find the largest p-value less than the critical value. All smaller
than this are significant.</li>
</ol>
<table class="table">
<colgroup>
<col width="53%">
<col width="46%">
</colgroup>
<thead><tr class="header">
<th align="left">FWER</th>
<th align="left">FDR</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">+ Controls probability of identifying a false
positive</td>
<td align="left">+ Controls rate of false discoveries</td>
</tr>
<tr class="even">
<td align="left">+ Strict error rate control</td>
<td align="left">+ Allows error control with less stringency</td>
</tr>
<tr class="odd">
<td align="left">- Very conservative</td>
<td align="left">- Does not control probability of making errors</td>
</tr>
<tr class="even">
<td align="left">- Requires larger statistical power</td>
<td align="left">- May result in false discoveries</td>
</tr>
</tbody>
</table>
<div id="challenge-6" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-6" class="callout-inner">
<h3 class="callout-title">Challenge 6</h3>
<div class="callout-content">
<ol style="list-style-type: decimal">
<li>At a significance level of 0.05, with 100 tests performed, what is
the Bonferroni significance threshold?</li>
<li>In a gene expression experiment, after FDR correction with an
FDR-adjusted p-value threshold of 0.05, we observe 500 significant
genes. What proportion of these genes are truly different?</li>
<li>Try running FDR correction on the <code>p_raw</code> vector.
<em>Hint: check <code>help("p.adjust")</code> to see what the method is
called</em>.<br>
Compare these values to the raw p-values and the Bonferroni
p-values.</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution6" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution6" aria-expanded="false" aria-controls="collapseSolution6">
  <h4 class="accordion-header" id="headingSolution6"> Show me the solution </h4>
</button>
<div id="collapseSolution6" class="accordion-collapse collapse" aria-labelledby="headingSolution6" data-bs-parent="#accordionSolution6">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li><p>The Bonferroni threshold for this significance threshold is <span class="math display">\[
  \frac{0.05}{100} = 0.0005
  \]</span></p></li>
<li><p>We can’t say what proportion of these genes are truly different.
However, if we repeated this experiment and statistical test over and
over, on average 5% of the results from each run would be false
discoveries.</p></li>
<li><p>The following code runs FDR correction and compares it to
non-corrected values and to Bonferroni:</p></li>
</ol>
<div class="codewrapper sourceCode" id="cb59">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">p_fdr</span> <span class="op">&lt;-</span> <span class="fu">p.adjust</span><span class="op">(</span><span class="va">p_raw</span>, method <span class="op">=</span> <span class="st">"BH"</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">p_raw</span>, <span class="va">p_fdr</span>, pch <span class="op">=</span> <span class="fl">16</span>, log<span class="op">=</span><span class="st">"xy"</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>v <span class="op">=</span> <span class="fl">0.05</span>, lty <span class="op">=</span> <span class="st">"dashed"</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>h <span class="op">=</span> <span class="fl">0.05</span>, lty <span class="op">=</span> <span class="st">"dashed"</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/02-high-dimensional-regression-rendered-p-fdr-1.png" alt="Plot of Benjamini-Hochberg-adjusted p-values (y) against unadjusted p-values (x). A dashed black line represents the identity (where x=y), while dashed red lines represent 0.05 significance thresholds." class="figure mx-auto d-block"><figcaption>
Benjamini-Hochberg correction is less conservative than Bonferroni
</figcaption></figure><div class="codewrapper sourceCode" id="cb60">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">plot</span><span class="op">(</span><span class="va">p_fwer</span>, <span class="va">p_fdr</span>, pch <span class="op">=</span> <span class="fl">16</span>, log<span class="op">=</span><span class="st">"xy"</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>v <span class="op">=</span> <span class="fl">0.05</span>, lty <span class="op">=</span> <span class="st">"dashed"</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>h <span class="op">=</span> <span class="fl">0.05</span>, lty <span class="op">=</span> <span class="st">"dashed"</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/02-high-dimensional-regression-rendered-plot-fdr-fwer-1.png" alt="Plot of Benjamini-Hochberg-adjusted p-values (y) against Bonferroni-adjusted p-values (x). A dashed black line represents the identity (where x=y), while dashed red lines represent 0.05 significance thresholds." style="display: block; margin: auto;" class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
</div>
<div id="feature-selection" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="feature-selection" class="callout-inner">
<h3 class="callout-title">Feature selection</h3>
<div class="callout-content">
<p>In this episode, we have focussed on regression in a setting where
there are more features than observations. This approach is relevant if
we are interested in the association of each feature with some outcome
or if we want to screen for features that have a strong association with
an outcome. If, however, we are interested in predicting an outcome or
if we want to know which features explain the variation in the outcome,
we may want to restrict ourselves to a subset of relevant features. One
way of doing this is called <em>regularisation</em>, and this is the
topic of the next episode. An alternative is called <em>feature
selection</em>. This is covered in the subsequent (optional)
episode.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="further-reading">Further reading<a class="anchor" aria-label="anchor" href="#further-reading"></a>
</h2>
<hr class="half-width">
<ul>
<li><a href="https://kasperdanielhansen.github.io/genbioconductor/html/limma.html" class="external-link"><strong><code>limma</code></strong>
tutorial by Kasper D. Hansen</a></li>
<li>
<a href="https://www.bioconductor.org/packages/release/bioc/vignettes/limma/inst/doc/usersguide.pdf" class="external-link"><strong><code>limma</code></strong>
user manual</a>.</li>
<li>
<a href="https://bioconductor.org/packages/release/bioc/vignettes/variancePartition/inst/doc/dream.html" class="external-link">The
<strong><code>VariancePartition</code></strong> package</a> has similar
functionality to <strong><code>limma</code></strong> but allows the
inclusion of random effects.</li>
</ul></section><section><h2 class="section-heading" id="footnotes">Footnotes<a class="anchor" aria-label="anchor" href="#footnotes"></a>
</h2>
<hr class="half-width">
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>Performing linear regression in a high-dimensional setting requires
us to perform hypothesis testing in a way that low-dimensional
regression may not.</li>
<li>Sharing information between features can increase power and reduce
false positives.</li>
<li>When running a lot of null hypothesis tests for high-dimensional
data, multiple testing correction allows retain power and avoid making
costly false discoveries.</li>
<li>Multiple testing methods can be more conservative or more liberal,
depending on our goals.</li>
</ul>
</div>
</div>
</div>
</section><div class="footnotes footnotes-end-of-document">
<hr>
<ol>
<li id="fn1"><p>“True difference” is a hard category to rigidly define.
As we’ve seen, with a lot of data, we can detect tiny differences, and
with little data, we can’t detect large differences. However, both can
be argued to be “true”.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Bonferroni correction is also termed “family-wise” error
rate control.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>This is often called “Benjamini-Hochberg” adjustment.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>People often perform extra controls on FDR-adjusted
p-values, ensuring that ranks don’t change and the critical value is
never smaller than the original p-value.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div></section><section id="aio-03-regression-regularisation"><p>Content from <a href="03-regression-regularisation.html">Regularised regression</a></p>
<hr>
<p>Last updated on 2025-09-02 |

        <a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/edit/main/episodes/03-regression-regularisation.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 170 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is regularisation?</li>
<li>How does regularisation work?</li>
<li>How can we select the level of regularisation for a model?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand the benefits of regularised models.</li>
<li>Understand how different types of regularisation work.</li>
<li>Apply and critically analyse regularised regression models.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<hr class="half-width">
<p>This episode is about <strong>regularisation</strong>, also called
<strong>regularised regression</strong> or <strong>penalised
regression</strong>. This approach can be used for prediction and for
feature selection and it is particularly useful when dealing with
high-dimensional data.</p>
<p>One reason that we need special statistical tools for
high-dimensional data is that standard linear models cannot handle
high-dimensional data sets – one cannot fit a linear model where there
are more features (predictor variables) than there are observations
(data points). In the previous lesson, we dealt with this problem by
fitting individual models for each feature and sharing information among
these models. Now we will take a look at an alternative approach that
can be used to fit models with more features than observations by
stabilising coefficient estimates. This approach is called
regularisation. Compared to many other methods, regularisation is also
often very fast and can therefore be extremely useful in practice.</p>
<p>First, let us check out what happens if we try to fit a linear model
to high-dimensional data! We start by reading in the <a href="https://carpentries-incubator.github.io/high-dimensional-stats-r/data/index.html" class="external-link"><code>methylation</code></a>
data from the last lesson:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"SummarizedExperiment"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: MatrixGenerics</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: matrixStats</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'MatrixGenerics'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:matrixStats':

    colAlls, colAnyNAs, colAnys, colAvgsPerRowSet, colCollapse,
    colCounts, colCummaxs, colCummins, colCumprods, colCumsums,
    colDiffs, colIQRDiffs, colIQRs, colLogSumExps, colMadDiffs,
    colMads, colMaxs, colMeans2, colMedians, colMins, colOrderStats,
    colProds, colQuantiles, colRanges, colRanks, colSdDiffs, colSds,
    colSums2, colTabulates, colVarDiffs, colVars, colWeightedMads,
    colWeightedMeans, colWeightedMedians, colWeightedSds,
    colWeightedVars, rowAlls, rowAnyNAs, rowAnys, rowAvgsPerColSet,
    rowCollapse, rowCounts, rowCummaxs, rowCummins, rowCumprods,
    rowCumsums, rowDiffs, rowIQRDiffs, rowIQRs, rowLogSumExps,
    rowMadDiffs, rowMads, rowMaxs, rowMeans2, rowMedians, rowMins,
    rowOrderStats, rowProds, rowQuantiles, rowRanges, rowRanks,
    rowSdDiffs, rowSds, rowSums2, rowTabulates, rowVarDiffs, rowVars,
    rowWeightedMads, rowWeightedMeans, rowWeightedMedians,
    rowWeightedSds, rowWeightedVars</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: GenomicRanges</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: stats4</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: BiocGenerics</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'BiocGenerics'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:stats':

    IQR, mad, sd, var, xtabs</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:base':

    anyDuplicated, aperm, append, as.data.frame, basename, cbind,
    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,
    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,
    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,
    Position, rank, rbind, Reduce, rownames, sapply, saveRDS, setdiff,
    table, tapply, union, unique, unsplit, which.max, which.min</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: S4Vectors</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'S4Vectors'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following object is masked from 'package:utils':

    findMatches</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:base':

    expand.grid, I, unname</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: IRanges</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: GenomeInfoDb</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: Biobase</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Welcome to Bioconductor

    Vignettes contain introductory material; view with
    'browseVignettes()'. To cite Bioconductor, see
    'citation("Biobase")', and for packages 'citation("pkgname")'.</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'Biobase'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following object is masked from 'package:MatrixGenerics':

    rowMedians</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:matrixStats':

    anyMissing, rowMedians</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: replacing previous import 'S4Arrays::makeNindexFromArrayViewport' by
'DelayedArray::makeNindexFromArrayViewport' when loading 'SummarizedExperiment'</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">methylation</span> <span class="op">&lt;-</span> <span class="fu">readRDS</span><span class="op">(</span><span class="st">"data/methylation.rds"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## here, we transpose the matrix to have features as rows and samples as columns</span></span>
<span><span class="va">methyl_mat</span> <span class="op">&lt;-</span> <span class="fu">t</span><span class="op">(</span><span class="fu">assay</span><span class="op">(</span><span class="va">methylation</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">age</span> <span class="op">&lt;-</span> <span class="va">methylation</span><span class="op">$</span><span class="va">Age</span></span></code></pre>
</div>
<p>Then, we try to fit a model with outcome age and all 5,000 features
in this dataset as predictors (average methylation levels, M-values,
across different sites in the genome).</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># by using methyl_mat in the formula below, R will run a multivariate regression</span></span>
<span><span class="co"># model in which each of the columns in methyl_mat is used as a predictor. </span></span>
<span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu">lm</span><span class="op">(</span><span class="va">age</span> <span class="op">~</span> <span class="va">methyl_mat</span><span class="op">)</span></span>
<span><span class="fu">summary</span><span class="op">(</span><span class="va">fit</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Call:
lm(formula = age ~ methyl_mat)

Residuals:
ALL 37 residuals are 0: no residual degrees of freedom!

Coefficients: (4964 not defined because of singularities)
                          Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)               2640.474        NaN     NaN      NaN
methyl_matcg00075967      -108.216        NaN     NaN      NaN
methyl_matcg00374717      -139.637        NaN     NaN      NaN
methyl_matcg00864867        33.102        NaN     NaN      NaN
methyl_matcg00945507        72.250        NaN     NaN      NaN
 [ reached 'max' / getOption("max.print") -- omitted 4996 rows ]

Residual standard error: NaN on 0 degrees of freedom
Multiple R-squared:      1,	Adjusted R-squared:    NaN
F-statistic:   NaN on 36 and 0 DF,  p-value: NA</code></pre>
</div>
<p>You can see that we’re able to get some effect size estimates, but
they seem very high! The summary also says that we were unable to
estimate effect sizes for 4,964 features because of “singularities”. We
clarify what singularities are in the note below but this means that R
couldn’t find a way to perform the calculations necessary to fit the
model. Large effect sizes and singularities are common when naively
fitting linear regression models with a large number of features (i.e.,
to high-dimensional data), often since the model cannot distinguish
between the effects of many, correlated features or when we have more
features than observations.</p>
<div id="singularities" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="singularities" class="callout-inner">
<h3 class="callout-title">Singularities</h3>
<div class="callout-content">
<p>The message that <code>lm()</code> produced is not necessarily the
most intuitive. What are “singularities” and why are they an issue? A
singular matrix is one that cannot be <a href="https://en.wikipedia.org/wiki/Invertible_matrix" class="external-link">inverted</a>. R
uses inverse operations to fit linear models (find the coefficients)
using:</p>
<p><span class="math display">\[
(X^TX)^{-1}X^Ty,
\]</span></p>
<p>where <span class="math inline">\(X\)</span> is a matrix of predictor
features and <span class="math inline">\(y\)</span> is the outcome
vector. Thus, if the matrix <span class="math inline">\(X^TX\)</span>
cannot be inverted to give <span class="math inline">\((X^TX)^{-1}\)</span>, R cannot fit the model and
returns the error that there are singularities.</p>
<p>Why might R be unable to calculate <span class="math inline">\((X^TX)^{-1}\)</span> and return the error that
there are singularities? Well, when the <a href="https://en.wikipedia.org/wiki/Determinant" class="external-link">determinant</a> of the
matrix is zero, we are unable to find its inverse. The determinant of
the matrix is zero when there are more features than observations or
often when the features are highly correlated.</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">xtx</span> <span class="op">&lt;-</span> <span class="fu">t</span><span class="op">(</span><span class="va">methyl_mat</span><span class="op">)</span> <span class="op">%*%</span> <span class="va">methyl_mat</span></span>
<span><span class="fu">det</span><span class="op">(</span><span class="va">xtx</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 0</code></pre>
</div>
</div>
</div>
</div>
<div id="correlated-features-common-in-high-dimensional-data" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="correlated-features-common-in-high-dimensional-data" class="callout-inner">
<h3 class="callout-title">Correlated features – common in
high-dimensional data</h3>
<div class="callout-content">
<p>In high-dimensional datasets, there are often multiple features that
contain redundant information (correlated features). If we visualise the
level of correlation between sites in the <code>methylation</code>
dataset, we can see that many of the features represent the same
information - there are many off-diagonal cells, which are deep red or
blue. For example, the following heatmap visualises the correlations for
the first 500 features in the <code>methylation</code> dataset (we
selected 500 features only as it can be hard to visualise patterns when
there are too many features!).</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"pheatmap"</span><span class="op">)</span></span>
<span><span class="va">small</span> <span class="op">&lt;-</span> <span class="va">methyl_mat</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">500</span><span class="op">]</span></span>
<span><span class="va">cor_mat</span> <span class="op">&lt;-</span> <span class="fu">cor</span><span class="op">(</span><span class="va">small</span><span class="op">)</span></span>
<span><span class="fu">pheatmap</span><span class="op">(</span><span class="va">cor_mat</span>,</span>
<span>         main <span class="op">=</span> <span class="st">"Feature-feature correlation in methylation data"</span>,</span>
<span>         legend_title <span class="op">=</span> <span class="st">"Pearson correlation"</span>,</span>
<span>         cluster_rows <span class="op">=</span> <span class="cn">FALSE</span>, </span>
<span>         cluster_cols <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>         show_rownames <span class="op">=</span> <span class="cn">FALSE</span>, </span>
<span>         show_colnames <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-corr-mat-meth-1.png" alt="A symmetrical heatmap where rows and columns are features in a DNA methylation dataset. Colour corresponds to correlation, with red being large positive correlations and blue being large negative correlations. There are large blocks of deep red and blue throughout the plot." class="figure mx-auto d-block"><figcaption>
Heatmap of pairwise feature-feature correlations between CpG sites in
DNA methylation data
</figcaption></figure>
</div>
</div>
</div>
<div id="challenge-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-1" class="callout-inner">
<h3 class="callout-title">Challenge 1</h3>
<div class="callout-content">
<p>Consider or discuss in groups:</p>
<ol style="list-style-type: decimal">
<li>Why would we observe correlated features in high-dimensional
biological data?</li>
<li>Why might correlated features be a problem when fitting linear
models?</li>
<li>What issue might correlated features present when selecting features
to include in a model one at a time?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li>Many of the features in biological data represent very similar
information biologically. For example, sets of genes that form complexes
are often expressed in very similar quantities. Similarly, methylation
levels at nearby sites are often very highly correlated.</li>
<li>Correlated features can make inference unstable or even impossible
mathematically.</li>
<li>When we are selecting features one at a time we want to pick the
most predictive feature each time. When a lot of features are very
similar but encode slightly different information, which of the
correlated features we select to include can have a huge impact on the
later stages of model selection!</li>
</ol>
</div>
</div>
</div>
</div>
<p>Regularisation can help us to deal with correlated features, as well
as effectively reduce the number of features in our model. Before we
describe regularisation, let’s recap what’s going on when we fit a
linear model.</p>
</section><section><h2 class="section-heading" id="coefficient-estimates-of-a-linear-model">Coefficient estimates of a linear model<a class="anchor" aria-label="anchor" href="#coefficient-estimates-of-a-linear-model"></a>
</h2>
<hr class="half-width">
<p>When we fit a linear model, we’re finding the line through our data
that minimises the sum of the squared residuals. We can think of that as
finding the slope and intercept that minimises the square of the length
of the dashed lines. In this case, the red line in the left panel is the
line that accomplishes this objective, and the red dot in the right
panel is the point that represents this line in terms of its slope and
intercept among many different possible models, where the background
colour represents how well different combinations of slope and intercept
accomplish this objective.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: viridisLite</code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-regplot-1.png" alt="For each observation, the left panel shows the residuals with respect to the optimal line (the one that minimises the sum of square errors). These are calculated as the difference between the value predicted by the line and the observed outcome. Right panel shows the sum of squared residuals across all possible linear regression models (as defined by different values of the regression coefficients)." class="figure mx-auto d-block"><figcaption>
Illustrative example demonstrated how regression coefficients are
inferred under a linear model framework.
</figcaption></figure><p>Mathematically, we can write the sum of squared residuals as</p>
<p><span class="math display">\[
\sum_{i=1}^N ( y_i-x'_i\beta)^2
\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is a vector of
(unknown) covariate effects which we want to learn by fitting a
regression model: the <span class="math inline">\(j\)</span>-th element
of <span class="math inline">\(\beta\)</span>, which we denote as <span class="math inline">\(\beta_j\)</span> quantifies the effect of the
<span class="math inline">\(j\)</span>-th covariate. For each individual
<span class="math inline">\(i\)</span>, <span class="math inline">\(x_i\)</span> is a vector of <span class="math inline">\(j\)</span> covariate values and <span class="math inline">\(y_i\)</span> is the true observed value for the
outcome. The notation <span class="math inline">\(x'_i\beta\)</span>
indicates matrix multiplication. In this case, the result is equivalent
to multiplying each element of <span class="math inline">\(x_i\)</span>
by its corresponding element in <span class="math inline">\(\beta\)</span> and then calculating the sum across
all of those values. The result of this product (often denoted by <span class="math inline">\(\hat{y}_i\)</span>) is the predicted value of the
outcome generated by the model. As such, <span class="math inline">\(y_i-x'_i\beta\)</span> can be interpreted as
the prediction error, also referred to as model residual. To quantify
the total error across all individuals, we sum the square residuals
<span class="math inline">\(( y_i-x'_i\beta)^2\)</span> across all
the individuals in our data.</p>
<p>Finding the value of <span class="math inline">\(\beta\)</span> that
minimises the sum above is the line of best fit through our data when
considering this goal of minimising the sum of squared error. However,
it is not the only possible line we could use! For example, we might
want to err on the side of caution when estimating effect sizes
(coefficients). That is, we might want to avoid estimating very large
effect sizes.</p>
<div id="challenge-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-2" class="callout-inner">
<h3 class="callout-title">Challenge 2</h3>
<div class="callout-content">
<p>Discuss in groups:</p>
<ol style="list-style-type: decimal">
<li>What are we minimising when we fit a linear model?</li>
<li>Why are we minimising this objective? What assumptions are we making
about our data when we do so?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li>When we fit a linear model we are minimising the squared error. In
fact, the standard linear model estimator is often known as “ordinary
least squares”. The “ordinary” really means “original” here, to
distinguish between this method, which dates back to ~1800, and some
more “recent” (think 1940s…) methods.</li>
<li>Squared error is useful because it ignores the <em>sign</em> of the
residuals (whether they are positive or negative). It also penalises
large errors much more than small errors. On top of all this, it also
makes the solution very easy to compute mathematically. Least squares
assumes that, when we account for the change in the mean of the outcome
based on changes in the income, the data are normally distributed. That
is, the <em>residuals</em> of the model, or the error left over after we
account for any linear relationships in the data, are normally
distributed, and have a fixed variance.</li>
</ol>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="model-selection-using-training-and-test-sets">Model selection using training and test sets<a class="anchor" aria-label="anchor" href="#model-selection-using-training-and-test-sets"></a>
</h2>
<hr class="half-width">
<p>Sets of models are often compared using statistics such as adjusted
<span class="math inline">\(R^2\)</span>, AIC or BIC. These show us how
well the model is learning the data used in fitting that same model <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. However,
these statistics do not really tell us how well the model will
generalise to new data. This is an important thing to consider – if our
model doesn’t generalise to new data, then there’s a chance that it’s
just picking up on a technical or batch effect in our data, or simply
some noise that happens to fit the outcome we’re modelling. This is
especially important when our goal is prediction – it’s not much good if
we can only predict well for samples where the outcome is already known,
after all!</p>
<p>To get an idea of how well our model generalises, we can split the
data into two - a “training” and a “test” set. We use the “training”
data to fit the model, and then see its performance on the “test”
data.</p>
<figure style="text-align: center"><img src="../fig/validation.png" alt="Schematic representation of how a dataset can be divided into a training (the portion of the data used to fit a model) and a test set (the portion of the data used to assess external generalisability)." width="500px" class="figure mx-auto d-block"><figcaption>
Schematic representation of how a dataset can be divided into a training
and a test set.
</figcaption></figure><p>One thing that often happens in this context is that large
coefficient values minimise the training error, but they don’t minimise
the test error on unseen data. First, we’ll go through an example of
what exactly this means.</p>
<p>To compare the training and test errors for a model of methylation
features and age, we’ll split the data into training and test sets, fit
a linear model and calculate the errors. First, let’s calculate the
training error. Let’s start by splitting the data into training and test
sets:</p>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">methylation</span> <span class="op">&lt;-</span> <span class="fu">readRDS</span><span class="op">(</span><span class="st">"/data/methylation.rds"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning in gzfile(file, "rb"): cannot open compressed file
'/data/methylation.rds', probable reason 'No such file or directory'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in gzfile(file, "rb"): cannot open the connection</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb34">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"SummarizedExperiment"</span><span class="op">)</span></span>
<span><span class="va">age</span> <span class="op">&lt;-</span> <span class="va">methylation</span><span class="op">$</span><span class="va">Age</span></span>
<span><span class="va">methyl_mat</span> <span class="op">&lt;-</span> <span class="fu">t</span><span class="op">(</span><span class="fu">assay</span><span class="op">(</span><span class="va">methylation</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<p>We will then subset the data to a set of CpG markers that are known
to be associated with age from a previous study by Horvath et al.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>, described
in <a href="https://carpentries-incubator.github.io/high-dimensional-stats-r/data/index.html" class="external-link">data</a>.</p>
<div class="codewrapper sourceCode" id="cb35">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cpg_markers</span> <span class="op">&lt;-</span> <span class="fu">c</span><span class="op">(</span><span class="st">"cg16241714"</span>, <span class="st">"cg14424579"</span>, <span class="st">"cg22736354"</span>, <span class="st">"cg02479575"</span>, <span class="st">"cg00864867"</span>, </span>
<span>    <span class="st">"cg25505610"</span>, <span class="st">"cg06493994"</span>, <span class="st">"cg04528819"</span>, <span class="st">"cg26297688"</span>, <span class="st">"cg20692569"</span>, </span>
<span>    <span class="st">"cg04084157"</span>, <span class="st">"cg22920873"</span>, <span class="st">"cg10281002"</span>, <span class="st">"cg21378206"</span>, <span class="st">"cg26005082"</span>, </span>
<span>    <span class="st">"cg12946225"</span>, <span class="st">"cg25771195"</span>, <span class="st">"cg26845300"</span>, <span class="st">"cg06144905"</span>, <span class="st">"cg27377450"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">horvath_mat</span> <span class="op">&lt;-</span> <span class="va">methyl_mat</span><span class="op">[</span>, <span class="va">cpg_markers</span><span class="op">]</span></span>
<span></span>
<span><span class="co">## Generate an index to split the data</span></span>
<span><span class="fu">set.seed</span><span class="op">(</span><span class="fl">42</span><span class="op">)</span></span>
<span><span class="va">train_ind</span> <span class="op">&lt;-</span> <span class="fu">sample</span><span class="op">(</span><span class="fu">nrow</span><span class="op">(</span><span class="va">methyl_mat</span><span class="op">)</span>, <span class="fl">25</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Split the data </span></span>
<span><span class="va">train_mat</span> <span class="op">&lt;-</span> <span class="va">horvath_mat</span><span class="op">[</span><span class="va">train_ind</span>, <span class="op">]</span></span>
<span><span class="va">train_age</span> <span class="op">&lt;-</span> <span class="va">age</span><span class="op">[</span><span class="va">train_ind</span><span class="op">]</span></span>
<span><span class="va">test_mat</span> <span class="op">&lt;-</span> <span class="va">horvath_mat</span><span class="op">[</span><span class="op">-</span><span class="va">train_ind</span>, <span class="op">]</span></span>
<span><span class="va">test_age</span> <span class="op">&lt;-</span> <span class="va">age</span><span class="op">[</span><span class="op">-</span><span class="va">train_ind</span><span class="op">]</span></span></code></pre>
</div>
<p>Now let’s fit a linear model to our training data and calculate the
training error. Here we use the mean of the squared difference between
our predictions and the observed data, or “mean squared error”
(MSE).</p>
<div class="codewrapper sourceCode" id="cb36">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Fit a linear model</span></span>
<span><span class="co"># as.data.frame() converts train_mat into a data.frame</span></span>
<span><span class="co"># Using the `.` syntax above together with a `data` argument will lead to</span></span>
<span><span class="co"># the same result as using `train_age ~ train_mat`: R will fit a multivariate </span></span>
<span><span class="co"># regression model in which each of the columns in `train_mat` is used as </span></span>
<span><span class="co"># a predictor. We opted to use the `.` syntax because it will help us to </span></span>
<span><span class="co"># obtain model predictions using the `predict()` function. </span></span>
<span></span>
<span><span class="va">fit_horvath</span> <span class="op">&lt;-</span> <span class="fu">lm</span><span class="op">(</span><span class="va">train_age</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="fu">as.data.frame</span><span class="op">(</span><span class="va">train_mat</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Function to calculate the (mean squared) error</span></span>
<span><span class="va">mse</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">true</span>, <span class="va">prediction</span><span class="op">)</span> <span class="op">{</span> </span>
<span>    <span class="fu">mean</span><span class="op">(</span><span class="op">(</span><span class="va">true</span> <span class="op">-</span> <span class="va">prediction</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> </span>
<span><span class="op">}</span> </span>
<span></span>
<span><span class="co">## Calculate the training error </span></span>
<span><span class="va">err_lm_train</span> <span class="op">&lt;-</span> <span class="fu">mse</span><span class="op">(</span><span class="va">train_age</span>, <span class="fu">fitted</span><span class="op">(</span><span class="va">fit_horvath</span><span class="op">)</span><span class="op">)</span> </span>
<span><span class="va">err_lm_train</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 1.319628</code></pre>
</div>
<p>The training error appears very low here – on average we’re only off
by about a year!</p>
<div id="challenge-3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-3" class="callout-inner">
<h3 class="callout-title">Challenge 3</h3>
<div class="callout-content">
<p>For the fitted model above, calculate the mean squared error for the
test set.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>First, let’s find the predicted values for the ‘unseen’ test
data:</p>
<div class="codewrapper sourceCode" id="cb38">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pred_lm</span> <span class="op">&lt;-</span> <span class="fu">predict</span><span class="op">(</span><span class="va">fit_horvath</span>, newdata <span class="op">=</span> <span class="fu">as.data.frame</span><span class="op">(</span><span class="va">test_mat</span><span class="op">)</span><span class="op">)</span> </span></code></pre>
</div>
<p>The mean squared error for the test set is the mean of the squared
error between the predicted values and true test data.</p>
<div class="codewrapper sourceCode" id="cb39">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">err_lm</span> <span class="op">&lt;-</span> <span class="fu">mse</span><span class="op">(</span><span class="va">test_age</span>, <span class="va">pred_lm</span><span class="op">)</span></span>
<span><span class="va">err_lm</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 223.3571</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>Unfortunately, the test error is a lot higher than the training
error. If we plot true age against predicted age for the samples in the
test set, we can gain more insight into the performance of the model on
the test set. Ideally, the predicted values should be close to the test
data.</p>
<div class="codewrapper sourceCode" id="cb41">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">par</span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">test_age</span>, <span class="va">pred_lm</span>, pch <span class="op">=</span> <span class="fl">19</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>coef <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-test-plot-lm-1.png" alt="A scatter plot of observed age versus predicted age for individuals in the test set. Each dot represents one individual. Dashed line is used as a reference to indicate how perfect predictions would look (observed = predicted). In this case we observe high prediction error in the test set." class="figure mx-auto d-block"><figcaption>
A scatter plot of observed age versus predicted age for individuals in
the test set. Each dot represents one individual. Dashed line is used as
a reference to indicate how perfect predictions would look (observed =
predicted).
</figcaption></figure><p>This figure shows the predicted ages obtained from a linear model fit
plotted against the true ages, which we kept in the test dataset. If the
prediction were good, the dots should follow a line. Regularisation can
help us to make the model more generalisable, improving predictions for
the test dataset (or any other dataset that is not used when fitting our
model).</p>
</section><section><h2 class="section-heading" id="regularisation">Regularisation<a class="anchor" aria-label="anchor" href="#regularisation"></a>
</h2>
<hr class="half-width">
<p>Regularisation can be used to reduce correlation between predictors,
the number of features, and improve generalisability by restricting
model parameter estimates. Essentially, we add another condition to the
problem we’re solving with linear regression that controls the total
size of the coefficients that come out and shrinks many coefficients to
zero (or near zero).</p>
<p>For example, we might say that the point representing the slope and
intercept must fall within a certain distance of the origin, <span class="math inline">\((0, 0)\)</span>. For the 2-parameter model (slope
and intercept), we could visualise this constraint as a circle with a
given radius. We want to find the “best” solution (in terms of
minimising the residuals) that also falls within a circle of a given
radius (in this case, 2).</p>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-ridgeplot-1.png" alt="For each observation, the left panel shows the residuals with respect to the optimal lines obtained with and without regularisation. Right panel shows the sum of squared residuals across all possible linear regression models. Regularisation moves the line away from the optimal (in terms of minimising the sum of squared residuals). " class="figure mx-auto d-block"><figcaption>
Illustrative example demonstrated how regression coefficients are
inferred under a linear model framework, with (blue line) and without
(red line) regularisation. A ridge penalty is used in this example
</figcaption></figure></section><section><h2 class="section-heading" id="ridge-regression">Ridge regression<a class="anchor" aria-label="anchor" href="#ridge-regression"></a>
</h2>
<hr class="half-width">
<p>There are multiple ways to define the distance that our solution must
fall in. The one we’ve plotted above controls the squared sum of the
coefficients, <span class="math inline">\(\beta\)</span>. This is also
sometimes called the <span class="math inline">\(L^2\)</span> norm. This
is defined as</p>
<p><span class="math display">\[
\left\lVert \beta\right\lVert_2 = \sqrt{\sum_{j=1}^p \beta_j^2}
\]</span></p>
<p>To control this, we specify that the solution for the equation above
also has to have an <span class="math inline">\(L^2\)</span> norm
smaller than a certain amount. Or, equivalently, we try to minimise a
function that includes our <span class="math inline">\(L^2\)</span> norm
scaled by a factor that is usually written <span class="math inline">\(\lambda\)</span>.</p>
<p><span class="math display">\[
\sum_{i=1}^N \biggl( y_i - x'_i\beta\biggr)^2  + \lambda \left\lVert
\beta \right\lVert_2 ^2
\]</span></p>
<p>This type of regularisation is called <em>ridge regression</em>.
Another way of thinking about this is that when finding the best model,
we’re weighing up a balance of the ordinary least squares objective and
a “penalty” term that punishes models with large coefficients. The
balance between the penalty and the ordinary least squares objective is
controlled by <span class="math inline">\(\lambda\)</span> - when <span class="math inline">\(\lambda\)</span> is large, we want to penalise
large coefficients. In other words, we care a lot about the size of the
coefficients and we want to reduce the complexity of our model. When
<span class="math inline">\(\lambda\)</span> is small, we don’t really
care a lot about shrinking our coefficients and we opt for a more
complex model. When it’s zero, we’re back to just using ordinary least
squares. We see how a penalty term, <span class="math inline">\(\lambda\)</span>, might be chosen later in this
episode.</p>
<p>For now, to see how regularisation might improve a model, let’s fit a
model using the same set of 20 features (stored as
<code>cpg_markers</code>) selected earlier in this episode (these are a
subset of the features identified by Horvarth et al), using both
regularised and ordinary least squares. To fit regularised regression
models, we will use the <strong><code>glmnet</code></strong>
package.</p>
<div class="codewrapper sourceCode" id="cb42">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"glmnet"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: Matrix</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'Matrix'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following object is masked from 'package:S4Vectors':

    expand</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loaded glmnet 4.1-10</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb47">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## glmnet() performs scaling by default, supply un-scaled data:</span></span>
<span><span class="va">horvath_mat</span> <span class="op">&lt;-</span> <span class="va">methyl_mat</span><span class="op">[</span>, <span class="va">cpg_markers</span><span class="op">]</span> <span class="co"># select the same 20 sites as before</span></span>
<span><span class="va">train_mat</span> <span class="op">&lt;-</span> <span class="va">horvath_mat</span><span class="op">[</span><span class="va">train_ind</span>, <span class="op">]</span> <span class="co"># use the same individuals as selected before</span></span>
<span><span class="va">test_mat</span> <span class="op">&lt;-</span> <span class="va">horvath_mat</span><span class="op">[</span><span class="op">-</span><span class="va">train_ind</span>, <span class="op">]</span></span>
<span></span>
<span><span class="va">ridge_fit</span> <span class="op">&lt;-</span> <span class="fu">glmnet</span><span class="op">(</span>x <span class="op">=</span> <span class="va">train_mat</span>, y <span class="op">=</span> <span class="va">train_age</span>, alpha <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">ridge_fit</span>, xvar <span class="op">=</span> <span class="st">"lambda"</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>h <span class="op">=</span> <span class="fl">0</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-plot-ridge-1.png" alt="A line plot of coefficient estimates against log lambda for a ridge regression model. Lines are depicted in different colours, with coefficients generally having large values on the left of the plot (small log lambda) and moving smoothly and gradually towards zero to the right of the plot (large log lambda). Some coefficients appear to increase and then decrease in magnitude as lambda increases, or switch signs." class="figure mx-auto d-block"><figcaption>
A line plot of coefficient estimates against log lambda for a ridge
regression model.
</figcaption></figure><p>This plot shows how the estimated coefficients for each CpG site
change as we increase the penalty, <span class="math inline">\(\lambda\)</span>. That is, as we decrease the size
of the region that solutions can fall into, the values of the
coefficients that we get back tend to decrease. In this case,
coefficients tend towards zero but generally don’t reach it until the
penalty gets very large. We can see that initially, some parameter
estimates are really, really large, and these tend to shrink fairly
rapidly.</p>
<p>We can also notice that some parameters “flip signs”; that is, they
start off positive and become negative as lambda grows. This is a sign
of collinearity, or correlated predictors. As we reduce the importance
of one feature, we can “make up for” the loss in accuracy from that one
feature by adding a bit of weight to another feature that represents
similar information.</p>
<p>Since we split the data into test and training data, we can prove
that ridge regression predicts the test data better than the model with
no regularisation. Let’s generate our predictions under the ridge
regression model and calculate the mean squared error in the test
set:</p>
<div class="codewrapper sourceCode" id="cb48">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Obtain a matrix of predictions from the ridge model,</span></span>
<span><span class="co"># where each column corresponds to a different lambda value</span></span>
<span><span class="va">pred_ridge</span> <span class="op">&lt;-</span> <span class="fu">predict</span><span class="op">(</span><span class="va">ridge_fit</span>, newx <span class="op">=</span> <span class="va">test_mat</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calculate MSE for every column of the prediction matrix against the vector of true ages</span></span>
<span><span class="va">err_ridge</span> <span class="op">&lt;-</span> <span class="fu">apply</span><span class="op">(</span><span class="va">pred_ridge</span>, <span class="fl">2</span>, <span class="kw">function</span><span class="op">(</span><span class="va">col</span><span class="op">)</span> <span class="fu">mse</span><span class="op">(</span><span class="va">test_age</span>, <span class="va">col</span><span class="op">)</span><span class="op">)</span> </span>
<span><span class="va">min_err_ridge</span> <span class="op">&lt;-</span> <span class="fu">min</span><span class="op">(</span><span class="va">err_ridge</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Identify the lambda value that results in the lowest MSE (ie, the "best" lambda value)</span></span>
<span><span class="va">which_min_err</span> <span class="op">&lt;-</span> <span class="fu">which.min</span><span class="op">(</span><span class="va">err_ridge</span><span class="op">)</span></span>
<span><span class="va">pred_min_ridge</span> <span class="op">&lt;-</span> <span class="va">pred_ridge</span><span class="op">[</span>, <span class="va">which_min_err</span><span class="op">]</span></span>
<span></span>
<span><span class="co">## Return errors</span></span>
<span><span class="va">min_err_ridge</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 46.76802</code></pre>
</div>
<p>This is much lower than the test error for the model without
regularisation:</p>
<div class="codewrapper sourceCode" id="cb50">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">err_lm</span>  </span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 223.3571</code></pre>
</div>
<p>We can see where on the continuum of lambdas we’ve picked a model by
plotting the coefficient paths again. In this case, we’ve picked a model
with fairly modest coefficient shrinking.</p>
<div class="codewrapper sourceCode" id="cb52">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">chosen_lambda</span> <span class="op">&lt;-</span> <span class="va">ridge_fit</span><span class="op">$</span><span class="va">lambda</span><span class="op">[</span><span class="fu">which.min</span><span class="op">(</span><span class="va">err_ridge</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">ridge_fit</span>, xvar <span class="op">=</span> <span class="st">"lambda"</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>v <span class="op">=</span> <span class="fu">log</span><span class="op">(</span><span class="va">chosen_lambda</span><span class="op">)</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-chooselambda-1.png" alt="A line plot of coefficient estimates against log lambda for a ridge regression model. A dashed vertical line depicts the optimal lambda value towards the left of the plot. Lines are depicted in different colours, with coefficients generally having large values on the left of the plot (small log lambda) and moving smoothly and gradually towards zero to the right of the plot (large log lambda). Some coefficients appear to increase and then decrease in magnitude as lambda increases, or switch signs." class="figure mx-auto d-block"><figcaption>
A line plot of coefficient estimates against log lambda for a ridge
regression model, showing the optimal value based on the minimal test
error.
</figcaption></figure><div id="challenge-4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-4" class="callout-inner">
<h3 class="callout-title">Challenge 4</h3>
<div class="callout-content">
<ol style="list-style-type: decimal">
<li>Which performs better, ridge or OLS?</li>
<li>Plot predicted ages for each method against the true ages. How do
the predictions look for both methods? Why might ridge be performing
better?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4"> Show me the solution </h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" aria-labelledby="headingSolution4" data-bs-parent="#accordionSolution4">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li>Ridge regression performs significantly better on unseen data,
despite being “worse” on the training data.</li>
</ol>
<div class="codewrapper sourceCode" id="cb53">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">min_err_ridge</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 46.76802</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb55">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">err_lm</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 223.3571</code></pre>
</div>
<ol start="2" style="list-style-type: decimal">
<li>The ridge ones are much less spread out with far fewer extreme
predictions.</li>
</ol>
<div class="codewrapper sourceCode" id="cb57">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">all</span> <span class="op">&lt;-</span> <span class="fu">c</span><span class="op">(</span><span class="va">pred_lm</span>, <span class="va">test_age</span>, <span class="va">pred_min_ridge</span><span class="op">)</span></span>
<span><span class="va">lims</span> <span class="op">&lt;-</span> <span class="fu">range</span><span class="op">(</span><span class="va">all</span><span class="op">)</span></span>
<span><span class="fu">par</span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">test_age</span>, <span class="va">pred_lm</span>,</span>
<span>    xlim <span class="op">=</span> <span class="va">lims</span>, ylim <span class="op">=</span> <span class="va">lims</span>,</span>
<span>    pch <span class="op">=</span> <span class="fl">19</span></span>
<span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>coef <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">test_age</span>, <span class="va">pred_min_ridge</span>,</span>
<span>    xlim <span class="op">=</span> <span class="va">lims</span>, ylim <span class="op">=</span> <span class="va">lims</span>,</span>
<span>    pch <span class="op">=</span> <span class="fl">19</span></span>
<span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>coef <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-plot-ridge-prediction-1.png" alt="Two plots showing OLS predictions and ridge regression predictions of age (y) against true age (x). A dashed line shows the line y=x. In the OLS plot, predictions are quite extreme, while in the ridge regression plot, they are generally more conservative." class="figure mx-auto d-block"><figcaption>
Two plots showing OLS predictions (left) and ridge regression
predictions (right) of age (y) against true age (x).
</figcaption></figure>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="lasso-regression">LASSO regression<a class="anchor" aria-label="anchor" href="#lasso-regression"></a>
</h2>
<hr class="half-width">
<p><em>LASSO</em> is another type of regularisation. In this case we use
the <span class="math inline">\(L^1\)</span> norm, or the sum of the
absolute values of the coefficients.</p>
<p><span class="math display">\[
\left\lVert \beta \right\lVert_1 = \sum_{j=1}^p |\beta_j|
\]</span></p>
<p>This tends to produce sparse models; that is to say, it tends to
remove features from the model that aren’t necessary to produce accurate
predictions. This is because the region we’re restricting the
coefficients to has sharp edges. So, when we increase the penalty
(reduce the norm), it’s more likely that the best solution that falls in
this region will be at the corner of this diagonal (i.e., one or more
coefficient is exactly zero).</p>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-shrink-lasso-1.png" alt="For each observation, the left panel shows the residuals with respect to the optimal lines obtained with and without regularisation. Right panel shows the sum of squared residuals across all possible linear regression models. Regularisation moves the line away from the optimal (in terms of minimising the sum of squared residuals)" class="figure mx-auto d-block"><figcaption>
Illustrative example demonstrated how regression coefficients are
inferred under a linear model framework, with (blue line) and without
(red line) regularisation. A LASSO penalty is used in this example.
</figcaption></figure><div id="challenge-5" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-5" class="callout-inner">
<h3 class="callout-title">Challenge 5</h3>
<div class="callout-content">
<ol style="list-style-type: decimal">
<li>Use <code>glmnet()</code> to fit a LASSO model (hint: set
<code>alpha = 1</code>).</li>
<li>Plot the model object. Remember that for ridge regression, we set
<code>xvar = "lambda"</code>. What if you don’t set this? What’s the
relationship between the two plots?</li>
<li>How do the coefficient paths differ to the ridge case?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5"> Show me the solution </h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" aria-labelledby="headingSolution5" data-bs-parent="#accordionSolution5">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li>Fitting a LASSO model is very similar to a ridge model, we just need
to change the <code>alpha</code> setting.</li>
</ol>
<div class="codewrapper sourceCode" id="cb58">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit_lasso</span> <span class="op">&lt;-</span> <span class="fu">glmnet</span><span class="op">(</span>x <span class="op">=</span> <span class="va">methyl_mat</span>, y <span class="op">=</span> <span class="va">age</span>, alpha <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre>
</div>
<ol start="2" style="list-style-type: decimal">
<li>When <code>xvar = "lambda"</code>, the x-axis represents increasing
model sparsity from left to right, because increasing lambda increases
the penalty added to the coefficients. When we instead plot the L1 norm
on the x-axis, increasing L1 norm means that we are allowing our
coefficients to take on increasingly large values.</li>
</ol>
<div class="codewrapper sourceCode" id="cb59">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">par</span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">fit_lasso</span>, xvar <span class="op">=</span> <span class="st">"lambda"</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">fit_lasso</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-plotlas-1.png" alt="Two line plots side-by-side, showing coefficient values from a LASSO model against log lambda (left) and L1 norm (right). The coefficients, generally, suddenly become zero as log lambda increases or, equivalently, L1 norm decreases. However, some coefficients increase in size before decreasing as log lamdba increases." class="figure mx-auto d-block"><figcaption>
Line plots showing coefficient values from a LASSO model against log
lambda (left) and L1 norm (right).
</figcaption></figure><ol start="3" style="list-style-type: decimal">
<li>When using LASSO, the paths tend to go to exactly zero much more as
the penalty ($ \lambda $) increases. In the ridge case, the paths tend
towards zero but less commonly reach exactly zero.</li>
</ol>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="cross-validation-to-find-the-best-value-of-lambda">Cross-validation to find the best value of <span class="math inline">\(\lambda\)</span>
<a class="anchor" aria-label="anchor" href="#cross-validation-to-find-the-best-value-of-lambda"></a>
</h2>
<hr class="half-width">
<p>Ideally, we want <span class="math inline">\(\lambda\)</span> to be
large enough to reduce the complexity of our model, thus reducing the
number of and correlations between the features, and improving
generalisability. However, we don’t want the value of <span class="math inline">\(\lambda\)</span> to be so large that we lose a lot
of the valuable information in the features.</p>
<p>Various methods can be used to balance this trade-off and thus select
the “best” value for <span class="math inline">\(\lambda\)</span>. One
method splits the data into <span class="math inline">\(K\)</span>
chunks. We then use <span class="math inline">\(K-1\)</span> of these as
the training set, and the remaining <span class="math inline">\(1\)</span> chunk as the test set. We can repeat
this until we’ve rotated through all <span class="math inline">\(K\)</span> chunks, giving us a good estimate of
how well each of the lambda values work in our data. This is called
cross-validation, and doing this repeated test/train split gives us a
better estimate of how generalisable our model is.</p>
<figure style="text-align: center"><img src="../fig/cross_validation.png" alt="The data is divided into $K$ chunks. For each cross-validation iteration, one data chunk is used as the test set. The remaining $K-1$ chunks are combined into a training set." width="695" class="figure mx-auto d-block"><figcaption>
Schematic representiation of a <span class="math inline">\(K\)</span>-fold cross-validation procedure.
</figcaption></figure><p>We can use this new idea to choose a lambda value by finding the
lambda that minimises the error across each of the test and training
splits. In R:</p>
<div class="codewrapper sourceCode" id="cb60">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># fit lasso model with cross-validation across a range of lambda values</span></span>
<span><span class="va">lasso</span> <span class="op">&lt;-</span> <span class="fu">cv.glmnet</span><span class="op">(</span><span class="va">methyl_mat</span>, <span class="va">age</span>, alpha <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">lasso</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-lasso-cv-1.png" alt="Alt" class="figure mx-auto d-block"><figcaption>
Cross-validated mean squared error for different values of lambda under
a LASSO penalty.
</figcaption></figure><div class="codewrapper sourceCode" id="cb61">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Extract the coefficients from the model with the lowest mean squared error from cross-validation</span></span>
<span><span class="va">coefl</span> <span class="op">&lt;-</span> <span class="fu">coef</span><span class="op">(</span><span class="va">lasso</span>, <span class="va">lasso</span><span class="op">$</span><span class="va">lambda.min</span><span class="op">)</span></span>
<span><span class="co"># select only non-zero coefficients</span></span>
<span><span class="va">selection</span> <span class="op">&lt;-</span> <span class="fu">which</span><span class="op">(</span><span class="va">coefl</span> <span class="op">!=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="co"># and convert to a normal matrix</span></span>
<span><span class="va">selected_coefs</span> <span class="op">&lt;-</span> <span class="fu">as.matrix</span><span class="op">(</span><span class="va">coefl</span><span class="op">)</span><span class="op">[</span><span class="va">selection</span>, <span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">selected_coefs</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>  (Intercept)    cg02388150    cg06493994    cg22449114    cg22736354
-8.4133296328  0.6966503392  0.1615535465  6.4255580409 12.0507794749
   cg03330058    cg09809672    cg11299964    cg19761273    cg26162695
-0.0002362055 -0.7487594618 -2.0399663416 -5.2538055304 -0.4486970332
   cg09502015    cg24771684    cg08446924    cg13215762    cg24549277
 1.0787003366  4.5743800395 -0.5960137381  0.1481402638  0.6290915767
   cg12304482    cg13131095    cg17962089    cg13842639    cg04080666
-1.0167896196  2.8860222552  6.3065284096  0.1590465147  2.4889065761
   cg06147194    cg03669936    cg14230040    cg19848924    cg23964682
-0.6838637838 -0.0352696698  0.1280760909 -0.0006938337  1.3378854603
   cg13578928    cg02745847    cg17410295    cg17459023    cg06223736
-0.8601170264  2.2346315955 -2.3008028295  0.0370389967  1.6158734083
   cg06717750    cg20138604    cg12851161    cg20972027    cg23878422
 2.3401693309  0.0084327521 -3.3033355652  0.2442751682  1.1059030593
   cg16612298    cg03762081    cg14428146    cg16908369    cg16271524
 0.0050053190 -6.5228858163  0.3167227488  0.2302773154 -1.3787104336
   cg22071651    cg04262805    cg24969251    cg11233105    cg03156032
 0.3480551279  1.1841804186  8.3024629942  0.6130598151 -1.1121959544 </code></pre>
</div>
<p>We can see that cross-validation has selected a value of <span class="math inline">\(\lambda\)</span> resulting in 44 features and the
intercept.</p>
</section><section><h2 class="section-heading" id="elastic-nets-blending-ridge-regression-and-the-lasso">Elastic nets: blending ridge regression and the LASSO<a class="anchor" aria-label="anchor" href="#elastic-nets-blending-ridge-regression-and-the-lasso"></a>
</h2>
<hr class="half-width">
<p>So far, we’ve used ridge regression (where <code>alpha = 0</code>)
and LASSO regression, (where <code>alpha = 1</code>). What if
<code>alpha</code> is set to a value between zero and one? Well, this
actually lets us blend the properties of ridge and LASSO regression.
This allows us to have the nice properties of the LASSO, where
uninformative variables are dropped automatically, and the nice
properties of ridge regression, where our coefficient estimates are a
bit more conservative, and as a result our predictions are a bit
better.</p>
<p>Formally, the objective function of elastic net regression is to
optimise the following function:</p>
<p><span class="math display">\[
\left(\sum_{i=1}^N y_i - x'_i\beta\right)
+ \lambda \left(
\alpha \left\lVert \beta \right\lVert_1 +
(1-\alpha)  \left\lVert \beta \right\lVert_2
\right)
\]</span></p>
<p>You can see that if <code>alpha = 1</code>, then the L1 norm term is
multiplied by one, and the L2 norm is multiplied by zero. This means we
have pure LASSO regression. Conversely, if <code>alpha = 0</code>, the
L2 norm term is multiplied by one, and the L1 norm is multiplied by
zero, meaning we have pure ridge regression. Anything in between gives
us something in between.</p>
<p>The contour plots we looked at previously to visualise what this
penalty looks like for different values of <code>alpha</code>.</p>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-elastic-contour-1.png" alt="For lower values of alpha, the penalty resembles ridge regression. For higher values of alpha, the penalty resembles LASSO regression." class="figure mx-auto d-block"><figcaption>
For an elastic net, the panels show the effect of the regularisation
across different values of alpha
</figcaption></figure><div id="challenge-6" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-6" class="callout-inner">
<h3 class="callout-title">Challenge 6</h3>
<div class="callout-content">
<ol style="list-style-type: decimal">
<li>Fit an elastic net model (hint: alpha = 0.5) without
cross-validation and plot the model object.</li>
<li>Fit an elastic net model with cross-validation and plot the error.
Compare with LASSO.</li>
<li>Select the lambda within one standard error of the minimum
cross-validation error (hint: <code>lambda.1se</code>). Compare the
coefficients with the LASSO model.</li>
<li>Discuss: how could we pick an <code>alpha</code> in the range (0,
1)? Could we justify choosing one <em>a priori</em>?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution6" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution6" aria-expanded="false" aria-controls="collapseSolution6">
  <h4 class="accordion-header" id="headingSolution6"> Show me the solution </h4>
</button>
<div id="collapseSolution6" class="accordion-collapse collapse" aria-labelledby="headingSolution6" data-bs-parent="#accordionSolution6">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li>Fitting an elastic net model is just like fitting a LASSO model. You
can see that coefficients tend to go exactly to zero, but the paths are
a bit less extreme than with pure LASSO; similar to ridge.</li>
</ol>
<div class="codewrapper sourceCode" id="cb63">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">elastic</span> <span class="op">&lt;-</span> <span class="fu">glmnet</span><span class="op">(</span><span class="va">methyl_mat</span><span class="op">[</span>, <span class="op">-</span><span class="fl">1</span><span class="op">]</span>, <span class="va">age</span>, alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">elastic</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-elastic-1.png" alt="A line plot showing coefficient values from an elastic net model against L1 norm. The coefficients, generally, suddenly become zero as L1 norm decreases. However, some coefficients increase in size before decreasing as L1 norm decreases." class="figure mx-auto d-block"><figcaption>
Line plot showing coefficient values from an elastic net model against
L1 norm.
</figcaption></figure><ol start="2" style="list-style-type: decimal">
<li>The process of model selection is similar for elastic net models as
for LASSO models.</li>
</ol>
<div class="codewrapper sourceCode" id="cb64">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">elastic_cv</span> <span class="op">&lt;-</span> <span class="fu">cv.glmnet</span><span class="op">(</span><span class="va">methyl_mat</span><span class="op">[</span>, <span class="op">-</span><span class="fl">1</span><span class="op">]</span>, <span class="va">age</span>, alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">elastic_cv</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-elastic-cv-1.png" alt="A plot of the cross-validation mean squared error of an elastic net model against log lambda." class="figure mx-auto d-block"><figcaption>
A plot of the cross-validation mean squared error of an elastic net
model against log lambda.
</figcaption></figure><ol start="3" style="list-style-type: decimal">
<li>You can see that the coefficients from these two methods are broadly
similar, but the elastic net coefficients are a bit more conservative.
Further, more coefficients are exactly zero in the LASSO model.</li>
</ol>
<div class="codewrapper sourceCode" id="cb65">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">coefe</span> <span class="op">&lt;-</span> <span class="fu">coef</span><span class="op">(</span><span class="va">elastic_cv</span>, <span class="va">elastic_cv</span><span class="op">$</span><span class="va">lambda.1se</span><span class="op">)</span></span>
<span><span class="fu">sum</span><span class="op">(</span><span class="va">coefe</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 4973</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb67">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">sum</span><span class="op">(</span><span class="va">coefl</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 4956</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb69">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">plot</span><span class="op">(</span></span>
<span>    <span class="va">coefl</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span>, <span class="va">coefe</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span>,</span>
<span>    pch <span class="op">=</span> <span class="fl">16</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"LASSO coefficients"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Elastic net coefficients"</span></span>
<span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in xy.coords(x, y, xlabel, ylabel, log): 'x' and 'y' lengths differ</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb71">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">abline</span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet</code></pre>
</div>
<ol start="4" style="list-style-type: decimal">
<li>You could pick an arbitrary value of <code>alpha</code>, because
arguably pure ridge regression or pure LASSO regression are also
arbitrary model choices. To be rigorous and to get the best-performing
model and the best inference about predictors, it’s usually best to find
the best combination of <code>alpha</code> and <code>lambda</code> using
a grid search approach in cross-validation. However, this can be very
computationally demanding.</li>
</ol>
</div>
</div>
</div>
</div>
<div id="the-bias-variance-tradeoff" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="the-bias-variance-tradeoff" class="callout-inner">
<h3 class="callout-title">The bias-variance tradeoff</h3>
<div class="callout-content">
<p>When we make predictions in statistics, there are two sources of
error that primarily influence the (in)accuracy of our predictions.
these are <em>bias</em> and <em>variance</em>.</p>
<p>The total expected error in our predictions is given by the following
equation:</p>
<p><span class="math display">\[
E(y - \hat{y}) = \text{Bias}^2 + \text{Variance} + \sigma^2
\]</span></p>
<p>Here, <span class="math inline">\(\sigma^2\)</span> represents the
irreducible error, that we can never overcome. Bias results from
erroneous assumptions in the model used for predictions. Fundamentally,
bias means that our model is mis-specified in some way, and fails to
capture some components of the data-generating process (which is true of
all models). If we have failed to account for a confounding factor that
leads to very inaccurate predictions in a subgroup of our population,
then our model has high bias.</p>
<p>Variance results from sensitivity to particular properties of the
input data. For example, if a tiny change to the input data would result
in a huge change to our predictions, then our model has high
variance.</p>
<p>Linear regression is an unbiased model under certain conditions. In
fact, the <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem" class="external-link">Gauss-Markov
theorem</a> shows that under the right conditions, OLS is the best
possible type of unbiased linear model.</p>
<p>Introducing penalties to means that our model is no longer unbiased,
meaning that the coefficients estimated from our data will
systematically deviate from the ground truth. Why would we do this? As
we saw, the total error is a function of bias and variance. By accepting
a small amount of bias, it’s possible to achieve huge reductions in the
total expected error.</p>
<p>This bias-variance tradeoff is also why people often favour elastic
net regression over pure LASSO regression.</p>
</div>
</div>
</div>
<div id="other-types-of-outcomes" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="other-types-of-outcomes" class="callout-inner">
<h3 class="callout-title">Other types of outcomes</h3>
<div class="callout-content">
<p>You may have noticed that <code>glmnet</code> is written as
<code>glm</code>, not <code>lm</code>. This means we can actually model
a variety of different outcomes using this regularisation approach. For
example, we can model binary variables using logistic regression, as
shown below. The type of outcome can be specified using the
<code>family</code> argument, which specifies the family of the outcome
variable.</p>
<p>In fact, <code>glmnet</code> is somewhat cheeky as it also allows you
to model survival using Cox proportional hazards models, which aren’t
GLMs, strictly speaking.</p>
<p>For example, in the current dataset we can model smoking status as a
binary variable in logistic regression by setting
<code>family = "binomial"</code>.</p>
<p>The <a href="https://glmnet.stanford.edu/articles/glmnet.html" class="external-link">package
documentation</a> explains this in more detail.</p>
<div class="codewrapper sourceCode" id="cb73">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">smoking</span> <span class="op">&lt;-</span> <span class="fu">as.numeric</span><span class="op">(</span><span class="fu">factor</span><span class="op">(</span><span class="va">methylation</span><span class="op">$</span><span class="va">smoker</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span></span>
<span><span class="co"># binary outcome</span></span>
<span><span class="fu">table</span><span class="op">(</span><span class="va">smoking</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>smoking
 0  1
30  7 </code></pre>
</div>
<div class="codewrapper sourceCode" id="cb75">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu">cv.glmnet</span><span class="op">(</span>x <span class="op">=</span> <span class="va">methyl_mat</span>, nfolds <span class="op">=</span> <span class="fl">5</span>, y <span class="op">=</span> <span class="va">smoking</span>, family <span class="op">=</span> <span class="st">"binomial"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning in lognet(x, is.sparse, y, weights, offset, alpha, nobs, nvars, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(x, is.sparse, y, weights, offset, alpha, nobs, nvars, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(x, is.sparse, y, weights, offset, alpha, nobs, nvars, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(x, is.sparse, y, weights, offset, alpha, nobs, nvars, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(x, is.sparse, y, weights, offset, alpha, nobs, nvars, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(x, is.sparse, y, weights, offset, alpha, nobs, nvars, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb77">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">coef</span> <span class="op">&lt;-</span> <span class="fu">coef</span><span class="op">(</span><span class="va">fit</span>, s <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">lambda.min</span><span class="op">)</span></span>
<span><span class="va">coef</span> <span class="op">&lt;-</span> <span class="fu">as.matrix</span><span class="op">(</span><span class="va">coef</span><span class="op">)</span></span>
<span><span class="va">coef</span><span class="op">[</span><span class="fu">which</span><span class="op">(</span><span class="va">coef</span> <span class="op">!=</span> <span class="fl">0</span><span class="op">)</span>, <span class="fl">1</span><span class="op">]</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] -1.455287</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb79">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">plot</span><span class="op">(</span><span class="va">fit</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-binomial-1.png" alt="A plot of the cross-validation binomial deviance of a logistic regression elastic net model against log lambda." class="figure mx-auto d-block"><figcaption>
A plot of the cross-validation binomial deviance of a logistic
regression elastic net model against log lambda.
</figcaption></figure><p>In this case, the results aren’t very interesting! We select an
intercept-only model. However, as highlighted by the warnings above, we
should not trust this result too much as the data was too small to
obtain reliable results! We only included it here to provide the code
that <em>could</em> be used to perform penalised regression for binary
outcomes (i.e. penalised logistic regression).</p>
</div>
</div>
</div>
<div id="tidymodels" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="tidymodels" class="callout-inner">
<h3 class="callout-title">tidymodels</h3>
<div class="callout-content">
<p>A lot of the packages for fitting predictive models like regularised
regression have different user interfaces. To do predictive modelling,
it’s important to consider things like choosing a good performance
metric and how to run normalisation. It’s also useful to compare
different model “engines”.</p>
<p>To this end, the <strong><code>tidymodels</code></strong> R framework
exists. We’re not doing a course on advanced topics in predictive
modelling so we are not covering this framework in detail. However, the
code below would be useful to perform repeated cross-validation. More
information about <strong><code>tidymodels</code></strong>, including
installation instructions, can be found <a href="https://www.tidymodels.org/" class="external-link">on the tidymodels website</a>.</p>
<div class="codewrapper sourceCode" id="cb80">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"tidymodels"</span><span class="op">)</span></span>
<span><span class="va">all_data</span> <span class="op">&lt;-</span> <span class="fu">as.data.frame</span><span class="op">(</span><span class="fu">cbind</span><span class="op">(</span>age <span class="op">=</span> <span class="va">age</span>, <span class="va">methyl_mat</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">split_data</span> <span class="op">&lt;-</span> <span class="fu">initial_split</span><span class="op">(</span><span class="va">all_data</span><span class="op">)</span></span>
<span></span>
<span><span class="va">norm_recipe</span> <span class="op">&lt;-</span> <span class="fu">recipe</span><span class="op">(</span><span class="fu">training</span><span class="op">(</span><span class="va">split_data</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="co">## everything other than age is a predictor</span></span>
<span>    <span class="fu">update_role</span><span class="op">(</span><span class="fu">everything</span><span class="op">(</span><span class="op">)</span>, new_role <span class="op">=</span> <span class="st">"predictor"</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">update_role</span><span class="op">(</span><span class="va">age</span>, new_role <span class="op">=</span> <span class="st">"outcome"</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="co">## center and scale all the predictors</span></span>
<span>    <span class="fu">step_center</span><span class="op">(</span><span class="fu">all_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">step_scale</span><span class="op">(</span><span class="fu">all_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> </span>
<span></span>
<span><span class="co">## set the "engine" to be a linear model with tunable alpha and lambda</span></span>
<span><span class="va">glmnet_model</span> <span class="op">&lt;-</span> <span class="fu">linear_reg</span><span class="op">(</span>penalty <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span><span class="op">)</span>, mixture <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">set_engine</span><span class="op">(</span><span class="st">"glmnet"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## define a workflow, with normalisation recipe into glmnet engine</span></span>
<span><span class="va">workflow</span> <span class="op">&lt;-</span> <span class="fu">workflow</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">add_recipe</span><span class="op">(</span><span class="va">norm_recipe</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">add_model</span><span class="op">(</span><span class="va">glmnet_model</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## 5-fold cross-validation repeated 5 times</span></span>
<span><span class="va">folds</span> <span class="op">&lt;-</span> <span class="fu">vfold_cv</span><span class="op">(</span><span class="fu">training</span><span class="op">(</span><span class="va">split_data</span><span class="op">)</span>, v <span class="op">=</span> <span class="fl">5</span>, repeats <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## define a grid of lambda and alpha parameters to search</span></span>
<span><span class="va">glmn_set</span> <span class="op">&lt;-</span> <span class="fu">parameters</span><span class="op">(</span></span>
<span>    <span class="fu">penalty</span><span class="op">(</span>range <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="op">-</span><span class="fl">5</span>, <span class="fl">1</span><span class="op">)</span>, trans <span class="op">=</span> <span class="fu">log10_trans</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    <span class="fu">mixture</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">glmn_grid</span> <span class="op">&lt;-</span> <span class="fu">grid_regular</span><span class="op">(</span><span class="va">glmn_set</span><span class="op">)</span></span>
<span><span class="va">ctrl</span> <span class="op">&lt;-</span> <span class="fu">control_grid</span><span class="op">(</span>save_pred <span class="op">=</span> <span class="cn">TRUE</span>, verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## use the metric "rmse" (root mean squared error) to grid search for the</span></span>
<span><span class="co">## best model</span></span>
<span><span class="va">results</span> <span class="op">&lt;-</span> <span class="va">workflow</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">tune_grid</span><span class="op">(</span></span>
<span>        resamples <span class="op">=</span> <span class="va">folds</span>,</span>
<span>        metrics <span class="op">=</span> <span class="fu">metric_set</span><span class="op">(</span><span class="va">rmse</span><span class="op">)</span>,</span>
<span>        control <span class="op">=</span> <span class="va">ctrl</span></span>
<span>    <span class="op">)</span></span>
<span><span class="co">## select the best model based on RMSE</span></span>
<span><span class="va">best_mod</span> <span class="op">&lt;-</span> <span class="va">results</span> <span class="op">%&gt;%</span> <span class="fu">select_best</span><span class="op">(</span><span class="st">"rmse"</span><span class="op">)</span></span>
<span><span class="va">best_mod</span></span>
<span><span class="co">## finalise the workflow and fit it with all of the training data</span></span>
<span><span class="va">final_workflow</span> <span class="op">&lt;-</span> <span class="fu">finalize_workflow</span><span class="op">(</span><span class="va">workflow</span>, <span class="va">best_mod</span><span class="op">)</span></span>
<span><span class="va">final_workflow</span></span>
<span><span class="va">final_model</span> <span class="op">&lt;-</span> <span class="va">final_workflow</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">fit</span><span class="op">(</span>data <span class="op">=</span> <span class="fu">training</span><span class="op">(</span><span class="va">split_data</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## plot predicted age against true age for test data</span></span>
<span><span class="fu">plot</span><span class="op">(</span></span>
<span>    <span class="fu">testing</span><span class="op">(</span><span class="va">split_data</span><span class="op">)</span><span class="op">$</span><span class="va">age</span>,</span>
<span>    <span class="fu">predict</span><span class="op">(</span><span class="va">final_model</span>, new_data <span class="op">=</span> <span class="fu">testing</span><span class="op">(</span><span class="va">split_data</span><span class="op">)</span><span class="op">)</span><span class="op">$</span><span class="va">.pred</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"True age"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Predicted age"</span>,</span>
<span>    pch <span class="op">=</span> <span class="fl">16</span>,</span>
<span>    log <span class="op">=</span> <span class="st">"xy"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></span></code></pre>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="further-reading">Further reading<a class="anchor" aria-label="anchor" href="#further-reading"></a>
</h2>
<hr class="half-width">
<ul>
<li>
<a href="https://www.statlearning.com/" class="external-link">An introduction to
statistical learning</a>.</li>
<li>
<a href="https://web.stanford.edu/~hastie/ElemStatLearn/" class="external-link">Elements
of statistical learning</a>.</li>
<li>
<a href="https://glmnet.stanford.edu/articles/glmnet.html" class="external-link">glmnet
vignette</a>.</li>
<li>
<a href="https://www.tidymodels.org/" class="external-link">tidymodels</a>.</li>
</ul></section><section><h2 class="section-heading" id="footnotes">Footnotes<a class="anchor" aria-label="anchor" href="#footnotes"></a>
</h2>
<hr class="half-width">
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>Regularisation is a way to fit a model, get better estimates of
effect sizes, and perform variable selection simultaneously.</li>
<li>Test and training splits, or cross-validation, are a useful way to
select models or hyperparameters.</li>
<li>Regularisation can give us a more predictive set of variables, and
by restricting the magnitude of coefficients, can give us a better (and
more stable) estimate of our outcome.</li>
<li>Regularisation is often <em>very</em> fast! Compared to other
methods for variable selection, it is very efficient. This makes it
easier to practice rigorous variable selection.</li>
</ul>
</div>
</div>
</div>
</section><div class="footnotes footnotes-end-of-document">
<hr>
<ol>
<li id="fn1"><p>Model selection including <span class="math inline">\(R^2\)</span>, AIC and BIC are covered in the
additional feature selection for regression episode of this course.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0014821" class="external-link">Epigenetic
Predictor of Age, Bocklandt et al. (2011)</a><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div></section><section id="aio-04-principal-component-analysis"><p>Content from <a href="04-principal-component-analysis.html">Principal component analysis</a></p>
<hr>
<p>Last updated on 2025-09-02 |

        <a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/edit/main/episodes/04-principal-component-analysis.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 130 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is principal component analysis (PCA) and when can it be
used?</li>
<li>How can we perform a PCA in R?</li>
<li>How many principal components are needed to explain a significant
amount of variation in the data?</li>
<li>How to interpret the output of PCA using loadings and principal
components?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Identify situations where PCA can be used to answer research
questions using high-dimensional data.</li>
<li>Perform a PCA on high-dimensional data.</li>
<li>Select the appropriate number of principal components.</li>
<li>Interpret the output of PCA.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<hr class="half-width">
<p>If a dataset contains many variables (<span class="math inline">\(p\)</span>), it is likely that some of these
variables will be highly correlated. Variables may even be so highly
correlated that they represent the same overall effect. Such datasets
are challenging to analyse for several reasons, with the main problem
being how to reduce dimensionality in the dataset while retaining the
important features.</p>
<p>In this episode we will explore <em>principal component analysis</em>
(PCA) as a popular method of analysing high-dimensional data. PCA is a
statistical method which allows large datasets of correlated variables
to be summarised into smaller numbers of uncorrelated principal
components that explain most of the variability in the original dataset.
As an example, PCA might reduce several variables representing aspects
of patient health (blood pressure, heart rate, respiratory rate) into a
single feature capturing an overarching “patient health” effect. This is
useful from an exploratory point of view, discovering how variables
might be associated and combined. The resulting principal component
could also be used as an effect in further analysis (e.g. linear
regression).</p>
<div id="challenge-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-1" class="callout-inner">
<h3 class="callout-title">Challenge 1</h3>
<div class="callout-content">
<p>Descriptions of three datasets and research questions are given
below. For which of these might PCA be considered a useful tool for
analysing data so that the research questions may be addressed?</p>
<ol style="list-style-type: decimal">
<li>An epidemiologist has data collected from different patients
admitted to hospital with infectious respiratory disease. They would
like to determine whether length of stay in hospital differs in patients
with different respiratory diseases.</li>
<li>A scientist has assayed gene expression levels in 1000 cancer
patients and has data from probes targeting different genes in tumour
samples from patients. She would like to create new variables
representing relative abundance of different groups of genes to i) find
out if genes form subgroups based on biological function and ii) use
these new variables in a linear regression examining how gene expression
varies with disease severity.</li>
<li>Both of the above.</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>In the first case, a regression model would be more suitable; perhaps
a survival model. In the second example, PCA can help to identify
modules of correlated features that explain a large amount of variation
within the data.</p>
<p>Therefore the answer here is 2.</p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="principal-component-analysis">Principal component analysis<a class="anchor" aria-label="anchor" href="#principal-component-analysis"></a>
</h2>
<hr class="half-width">
<p>PCA transforms a dataset of continuous variables into a new set of
uncorrelated variables called “principal components”. The first
principal component derived explains the largest amount of the
variability in the underlying dataset. The second principal component
derived explains the second largest amount of variability in the dataset
and so on. Once the dataset has been transformed into principal
components, we can extract a subset of the principal components in order
of the variance they explain (starting with the first principal
component that by definition explains the most variability, and then the
second), giving new variables that explain a lot of the variability in
the original dataset. Thus, PCA helps us to produce a lower dimensional
dataset while keeping most of the information in the original
dataset.</p>
<p>To see what these new principal components may look like, the figure
below shows the log-transformed prostate specific antigen
(<code>lpsa</code>) versus cancer volume (<code>lcavol</code>) data from
the <a href="https://carpentries-incubator.github.io/high-dimensional-stats-r/data/index.html" class="external-link"><code>prostate</code></a>
data set introduced in Episode 1. Principal components are a collection
of new, artificial data points, one for each individual observation
called <em>scores</em>. The red line on the plot represents the line
passing through the scores (points) of the first principal component for
each observation. The angle that the first principal component line
passes through the data points at is set to the direction with the
highest variability. The plotted first principal components can
therefore be thought of reflecting the effect in the data that has the
highest variability. The second principal component explains the next
highest amount of variability in the data and is represented by the line
perpendicular to the first (the green line). The second principal
component can be thought of as capturing the overall effect in the data
that has the second-highest variability.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: ggrepel</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'PCAtools'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:stats':

    biplot, screeplot</code></pre>
</div>
<figure style="text-align: center"><img src="../fig/04-principal-component-analysis-rendered-pros-pcs-1.png" alt="Side-by-side scatter plots of the prostate data. The left figure displays a scatter plot of the log prostate specific antigen versus the log cancer volume. The first principal component is shown by a red line and the second principal component is shown by a green line. The right figure displays the same scatter plot rotated so that the first principal component is horizontal and the second principal component is shown perpendicular to this." class="figure mx-auto d-block"><figcaption>
Scatter plots of the prostate data with the first principal component in
red and second principal component in green.
</figcaption></figure><p>The animation below illustrates how principal components are
calculated from data. You can imagine that the black line is a rod and
each red dashed line is a spring. The energy of each spring is
proportional to its squared length. The direction of the first principal
component is the one that minimises the total energy of all of the
springs. In the animation below, the springs pull the rod, finding the
direction of the first principal component when they reach equilibrium.
We then use the length of the springs from the rod as the first
principal component. This is explained in more detail on <a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues" class="external-link">this
Q&amp;A website</a>.</p>
<figure style="text-align: center"><img src="../fig/pendulum.gif" alt="Animated scatter plot displaying the iterative process by which principal components are found. The data points are shown in blue and the principal component is shown by a solid black line. The distances of the points from the line are shown by red dashed lines. The animation initially starts with the principal component far away from the direction of variability in the points and, as time goes on, eventually finds the direction of variability exhibiting a springing motion." class="figure mx-auto d-block"><figcaption>
Animation showing the iterative process by which principal components
are found. Data points are shown in blue, distances from the line as red
dashed lines and the principal component is shown by a black line.
</figcaption></figure><div id="mathematical-description-of-pca" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="mathematical-description-of-pca" class="callout-inner">
<h3 class="callout-title">Mathematical description of PCA</h3>
<div class="callout-content">
<p>Mathematically, each principal component is a linear combination of
the variables in the dataset. That is, the first principal component
values or <em>scores</em>, <span class="math inline">\(Z_1\)</span>, are
a linear combination of variables in the dataset, <span class="math inline">\(X_1...X_p\)</span>, given by</p>
<p><span class="math display">\[ Z_1 = a_{11}X_1 + a_{21}X_2
+....+a_{p1}X_p, \]</span></p>
<p>where <span class="math inline">\(a_{11}...a_{p1}\)</span> represent
principal component <em>loadings</em>.</p>
</div>
</div>
</div>
<p>In summary, the principal components’ values are called
<em>scores</em>. The loadings can be thought of as the degree to which
each original variable contributes to the principal component scores. In
this episode, we will see how to perform PCA to summarise the
information in high-dimensional datasets.</p>
</section><section><h2 class="section-heading" id="how-do-we-perform-a-pca">How do we perform a PCA?<a class="anchor" aria-label="anchor" href="#how-do-we-perform-a-pca"></a>
</h2>
<hr class="half-width">
<p>To illustrate how to perform PCA initially, we revisit the
low-dimensional <code>prostate</code> dataset. The data come from a
study which examined the correlation between the level of prostate
specific antigen and a number of clinical measures in 97 men who were
about to receive a radical prostatectomy. The data have 97 rows and 9
columns.</p>
<p>The variables in the dataset (columns) are:</p>
<ul>
<li>
<code>lcavol</code> (log-transformed cancer volume),</li>
<li>
<code>lweight</code> (log-transformed prostate weight),</li>
<li>
<code>lbph</code> (log-transformed amount of benign prostate
enlargement),</li>
<li>
<code>svi</code> (seminal vesicle invasion),</li>
<li>
<code>lcp</code> (log-transformed capsular penetration; amount of
spread of cancer in outer walls of prostate),</li>
<li>
<code>gleason</code> (Gleason score; grade of cancer cells),</li>
<li>
<code>pgg45</code> (percentage Gleason scores 4 or 5),</li>
<li>
<code>lpsa</code> (log-transformed prostate specific antigen; level
of PSA in blood).</li>
<li>
<code>age</code> (patient age in years).</li>
</ul>
<p>We will perform PCA on the five continuous clinical variables in our
dataset so that we can create fewer variables representing clinical
markers of cancer progression.</p>
<p>First, we will examine the <code>prostate</code> dataset (originally
part of the <strong><code>lasso2</code></strong> package):</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">prostate</span> <span class="op">&lt;-</span> <span class="fu">readRDS</span><span class="op">(</span><span class="st">"data/prostate.rds"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">head</span><span class="op">(</span><span class="va">prostate</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>      lcavol  lweight age      lbph svi       lcp gleason pgg45       lpsa
1 -0.5798185 2.769459  50 -1.386294   0 -1.386294       6     0 -0.4307829
2 -0.9942523 3.319626  58 -1.386294   0 -1.386294       6     0 -0.1625189
3 -0.5108256 2.691243  74 -1.386294   0 -1.386294       7    20 -0.1625189
4 -1.2039728 3.282789  58 -1.386294   0 -1.386294       6     0 -0.1625189
5  0.7514161 3.432373  62 -1.386294   0 -1.386294       6     0  0.3715636
6 -1.0498221 3.228826  50 -1.386294   0 -1.386294       6     0  0.7654678</code></pre>
</div>
<p>Note that each row of the dataset represents a single patient.</p>
<p>We will create a subset of the data including only the clinical
variables we want to use in the PCA.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pros2</span> <span class="op">&lt;-</span> <span class="va">prostate</span><span class="op">[</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"lcavol"</span>, <span class="st">"lweight"</span>, <span class="st">"lbph"</span>, <span class="st">"lcp"</span>, <span class="st">"lpsa"</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="fu">head</span><span class="op">(</span><span class="va">pros2</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>      lcavol  lweight      lbph       lcp       lpsa
1 -0.5798185 2.769459 -1.386294 -1.386294 -0.4307829
2 -0.9942523 3.319626 -1.386294 -1.386294 -0.1625189
3 -0.5108256 2.691243 -1.386294 -1.386294 -0.1625189
4 -1.2039728 3.282789 -1.386294 -1.386294 -0.1625189
5  0.7514161 3.432373 -1.386294 -1.386294  0.3715636
6 -1.0498221 3.228826 -1.386294 -1.386294  0.7654678</code></pre>
</div>
<div class="section level3">
<h3 id="do-we-need-to-scale-the-data">Do we need to scale the data?<a class="anchor" aria-label="anchor" href="#do-we-need-to-scale-the-data"></a>
</h3>
<p>PCA derives principal components based on the variance they explain
in the data. Therefore, we may need to apply some pre-processing to
scale variables in our dataset if we want to ensure that each variable
is considered equally by the PCA. Scaling is essential if we want to
avoid the PCA ignoring variables that may be important to our analysis
just because they take low values and have low variance. We do not need
to scale if we want variables with low variance to carry less weight in
the PCA.</p>
<p>In this example, we want each variable to be treated equally by the
PCA since variables with lower values may be just as informative as
variables with higher values. Let’s therefore investigate the variables
in our dataset to see if we need to scale our variables first:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">apply</span><span class="op">(</span><span class="va">pros2</span>, <span class="fl">2</span>, <span class="va">var</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>  lcavol  lweight     lbph      lcp     lpsa
1.389157 0.246642 2.104840 1.955102 1.332476 </code></pre>
</div>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">par</span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">hist</span><span class="op">(</span><span class="va">pros2</span><span class="op">$</span><span class="va">lweight</span><span class="op">)</span></span>
<span><span class="fu">hist</span><span class="op">(</span><span class="va">pros2</span><span class="op">$</span><span class="va">lbph</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/04-principal-component-analysis-rendered-var-hist-1.png" alt="Side-by-side histograms of two variables from the dataset, lweight on the left and lbph on the right. The histogram for the lweight data ranges from around 2 to 6 on the x axis, while the histogram for the lbph ranges from around -2 to 3 on the x axis." class="figure mx-auto d-block"><figcaption>
Histograms of two variables from the prostate data set.
</figcaption></figure><p>Note that variance is greatest for <code>lbph</code> and lowest for
<code>lweight</code>. Since we want each of the variables to be treated
equally in our PCA, but there are large differences in the variances of
the variables, we need to scale each of the variables before including
them in a PCA to ensure that differences in variances do not drive the
calculation of principal components. In this example, we standardise all
five variables to have a mean of 0 and a standard deviation of 1.</p>
<div id="challenge-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-2" class="callout-inner">
<h3 class="callout-title">Challenge 2</h3>
<div class="callout-content">
<ol style="list-style-type: decimal">
<li>Why might it be necessary to standardise variables before performing
a PCA?<br>
Some ideas:</li>
</ol>
<ul>
<li>To make the results of the PCA interesting.</li>
<li>If you want to ensure that variables with different ranges of values
contribute equally to analysis.</li>
<li>To allow the feature matrix to be calculated faster, especially in
cases where there are a lot of input variables.</li>
<li>To allow both continuous and categorical variables to be included in
the PCA.</li>
<li>All of the above.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Can you think of datasets where it might not be necessary to
standardise variables?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li>Scaling the data isn’t guaranteed to make the results more
interesting. It also won’t affect how quickly the output will be
calculated, whether continuous and categorical variables are present or
not.</li>
</ol>
<p>We use scaling to ensure that features with different ranges of
values have equal weighting in the resulting principal components (point
2).</p>
<ol start="2" style="list-style-type: decimal">
<li>You may not want to standardise datasets which contain continuous
variables all measured on the same scale (e.g. gene expression data or
RNA sequencing data). In this case, variables with very little
sample-to-sample variability may represent only random noise, and
standardising the data would give these extra weight in the PCA.</li>
</ol>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="performing-pca">Performing PCA<a class="anchor" aria-label="anchor" href="#performing-pca"></a>
</h3>
<p>Throughout this episode, we will carry out PCA using the Bioconductor
package <strong><code>PCAtools</code></strong>. This package provides
several functions that are useful for exploring data via PCA and
producing useful figures and analysis tools. The package is made for the
somewhat unusual Bioconductor style of data tables (observations in
columns, features in rows). When using Bioconductor data sets and
<strong><code>PCAtools</code></strong>, it is thus not necessary to
transpose the data. You can use the help files in the package
documentation to find out about the <code>pca()</code> function (type
<code>help("pca")</code> or <code>?pca</code> in R). Although we focus
on PCA implementation using this package, there are several other
options for performing PCA, including a base R function,
<code>prcomp()</code>. Equivalent functions and output labels for the
common implementations are summarised below.</p>
<table class="table">
<thead><tr class="header">
<th>library::command()</th>
<th>Principal component scores</th>
<th>Loadings</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>stats::prcomp()</td>
<td>$x</td>
<td>$rotation</td>
</tr>
<tr class="even">
<td>stats::princomp()</td>
<td>$scores</td>
<td>$loadings</td>
</tr>
<tr class="odd">
<td>PCAtools::pca()</td>
<td>$rotated</td>
<td>$loadings</td>
</tr>
</tbody>
</table>
<p>Let’s use the <strong><code>PCAtools</code></strong> package to
perform PCA on the scaled <code>prostate</code> data. First, let’s load
the <strong><code>PCAtools</code></strong> package.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"PCAtools"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: ggplot2</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: ggrepel</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'PCAtools'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:stats':

    biplot, screeplot</code></pre>
</div>
<p>The input data (<code>pros2</code>) is in the form of a matrix and we
need to take the transpose of this matrix and convert it to a data frame
for use within the <strong><code>PCAtools</code></strong> package. We
set the <code>scale = TRUE</code> argument to standardise the variables
to have a mean 0 and standard deviation of 1 as above.</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pros2t</span> <span class="op">&lt;-</span> <span class="fu">data.frame</span><span class="op">(</span><span class="fu">t</span><span class="op">(</span><span class="va">pros2</span><span class="op">)</span><span class="op">)</span>  <span class="co"># create transposed data frame</span></span>
<span><span class="fu">colnames</span><span class="op">(</span><span class="va">pros2t</span><span class="op">)</span><span class="op">&lt;-</span><span class="fu">seq</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu">dim</span><span class="op">(</span><span class="va">pros2t</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="co"># rename columns for use within PCAtools</span></span>
<span><span class="va">pmetadata</span><span class="op">=</span> <span class="fu">data.frame</span><span class="op">(</span><span class="st">"M"</span> <span class="op">=</span> <span class="fu">rep</span><span class="op">(</span><span class="fl">1</span>, <span class="fu">dim</span><span class="op">(</span><span class="va">pros2t</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>, row.names <span class="op">=</span> <span class="fu">colnames</span><span class="op">(</span><span class="va">pros2t</span><span class="op">)</span><span class="op">)</span> <span class="co"># create fake metadata for use with PCA tools</span></span>
<span></span>
<span><span class="co">## implement PCA</span></span>
<span><span class="va">pca.pros</span> <span class="op">&lt;-</span> <span class="fu">pca</span><span class="op">(</span><span class="va">pros2t</span>, scale <span class="op">=</span> <span class="cn">TRUE</span>, center <span class="op">=</span> <span class="cn">TRUE</span>, metadata <span class="op">=</span> <span class="va">pmetadata</span><span class="op">)</span></span>
<span><span class="fu">summary</span><span class="op">(</span><span class="va">pca.pros</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>           Length Class      Mode
rotated     5     data.frame list
loadings    5     data.frame list
variance    5     -none-     numeric
sdev        5     -none-     numeric
metadata    1     data.frame list
xvars       5     -none-     character
yvars      97     -none-     character
components  5     -none-     character</code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="how-many-principal-components-do-we-need">How many principal components do we need?<a class="anchor" aria-label="anchor" href="#how-many-principal-components-do-we-need"></a>
</h2>
<hr class="half-width">
<p>We have calculated one principal component for each variable in the
original dataset. How do we choose how many of these are necessary to
represent the true variation in the data, without having extra
components that are unnecessary?</p>
<p>Let’s look at the relative importance of (percentage of the variance
explained by) the components:</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">prop.var</span> <span class="op">&lt;-</span> <span class="fu">round</span><span class="op">(</span><span class="va">pca.pros</span><span class="op">$</span><span class="va">variance</span>, digits <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span><span class="va">prop.var</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>    PC1     PC2     PC3     PC4     PC5
48.9767 27.3063 11.1094  8.0974  4.5101 </code></pre>
</div>
<p>This returns the proportion of variance in the data explained by each
of the principal components. In this example, the first principal
component, PC1, explains approximately 49% of variance in the data, PC2
27% of variance, PC3 a further 11%, PC4 approximately 8% and PC5 around
5%.</p>
<p>Let’s visualise this. A plot of the amount of variance accounted for
by each principal component is also called a scree plot. Often, scree
plots show a characteristic pattern where initially, the variance drops
rapidly with each additional principal component. But then there is an
“elbow” after which the variance decreases more slowly. The total
variance explained up to the elbow point is sometimes interpreted as
structural variance that is relevant and should be retained versus noise
which may be discarded after the elbow.</p>
<p>We can create a scree plot using the
<strong><code>PCAtools</code></strong> function
<code>screeplot()</code>:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">screeplot</span><span class="op">(</span><span class="va">pca.pros</span>, axisLabSize <span class="op">=</span> <span class="fl">5</span>, titleLabSize <span class="op">=</span> <span class="fl">8</span>, </span>
<span>          drawCumulativeSumLine <span class="op">=</span> <span class="cn">FALSE</span>, drawCumulativeSumPoints <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_line</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu">length</span><span class="op">(</span><span class="va">pca.pros</span><span class="op">$</span><span class="va">components</span><span class="op">)</span>, y <span class="op">=</span> <span class="fu">as.numeric</span><span class="op">(</span><span class="va">pca.pros</span><span class="op">$</span><span class="va">variance</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> </span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/04-principal-component-analysis-rendered-varexp-1.png" alt="A scree plot showing the percentage of variance explained by each principal component versus the principal component number. The points are joined by lines to indicate where the elbow of the scree plot occurs." class="figure mx-auto d-block"><figcaption>
Scree plot showing the percentage of the variance explained by the
principal components calculated from the prostate data.
</figcaption></figure><p>The scree plot shows that the first principal component explains most
of the variance in the data (&gt;50%) and each subsequent principal
component explains less and less of the total variance. The first two
principal components explain &gt;70% of variance in the data.</p>
<p>Selecting a subset of principal components results in loss of
information from the original dataset but selecting principal components
to summarise the <em>majority</em> of the information in the original
dataset is useful from a practical dimension reduction perspective.
There are no clear guidelines on how many principal components should be
included in PCA. We often look at the ‘elbow’ on the scree plot as an
indicator that the addition of principal components does not drastically
contribute to explaining the remaining variance. In this case, the
‘elbow’ of the scree plot appears to be at the third principal
component. We may therefore conclude that three principal components are
sufficient to explain the majority of the variability in the original
dataset. We may also choose another criterion, such as an arbitrary
cut-off for the proportion of the variance explained compared to other
principal components. Essentially, the criterion used to select
principal components should be determined based on what is deemed a
sufficient level of information retention for a specific dataset and
question.</p>
</section><section><h2 class="section-heading" id="loadings-and-principal-component-scores">Loadings and principal component scores<a class="anchor" aria-label="anchor" href="#loadings-and-principal-component-scores"></a>
</h2>
<hr class="half-width">
<p>Most PCA functions will produce two main output matrices: the
<em>principal component scores</em> and the <em>loadings</em>. The
matrix of principal component scores has as many rows as there were
observations in the input matrix. These scores are what is usually
visualised or used for down-stream analyses. The matrix of loadings
(also called rotation matrix) has as many rows as there are features in
the original data. It contains information about how the (usually
centered and scaled) original data relate to the principal component
scores.</p>
<p>We can print the loadings for the
<strong><code>PCAtools</code></strong> implementation using</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pca.pros</span><span class="op">$</span><span class="va">loadings</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>              PC1         PC2         PC3         PC4         PC5
lcavol  0.5616465 -0.23664270  0.01486043  0.22708502 -0.75945046
lweight 0.2985223  0.60174151 -0.66320198 -0.32126853 -0.07577123
lbph    0.1681278  0.69638466  0.69313753  0.04517286 -0.06558369
lcp     0.4962203 -0.31092357  0.26309227 -0.72394666  0.25253840
lpsa    0.5665123 -0.01680231 -0.10141557  0.56487128  0.59111493</code></pre>
</div>
<p>The principal component scores are obtained by carrying out matrix
multiplication of the (usually centered and scaled) original data times
the loadings. The following callout demonstrates this.</p>
<div id="computing-a-pca-by-hand" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="computing-a-pca-by-hand" class="callout-inner">
<h3 class="callout-title">Computing a PCA “by hand”</h3>
<div class="callout-content">
<p>The rotation matrix obtained in a PCA is identical to the
eigenvectors of the covariance matrix of the data. Multiplying these
with the (centered and scaled) data yields the principal component
scores:</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pros2.scaled</span> <span class="op">&lt;-</span> <span class="fu">scale</span><span class="op">(</span><span class="va">pros2</span><span class="op">)</span> <span class="co"># centre and scale the prostate data</span></span>
<span><span class="va">pros2.cov</span> <span class="op">&lt;-</span> <span class="fu">cov</span><span class="op">(</span><span class="va">pros2.scaled</span><span class="op">)</span>   <span class="co"># generate covariance matrix</span></span>
<span><span class="va">pros2.cov</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>           lcavol   lweight         lbph          lcp      lpsa
lcavol  1.0000000 0.1941283  0.027349703  0.675310484 0.7344603
lweight 0.1941283 1.0000000  0.434934636  0.100237795 0.3541204
lbph    0.0273497 0.4349346  1.000000000 -0.006999431 0.1798094
lcp     0.6753105 0.1002378 -0.006999431  1.000000000 0.5488132
lpsa    0.7344603 0.3541204  0.179809410  0.548813169 1.0000000</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pros2.eigen</span> <span class="op">&lt;-</span> <span class="fu">eigen</span><span class="op">(</span><span class="va">pros2.cov</span><span class="op">)</span> <span class="co"># preform eigen decomposition</span></span>
<span><span class="va">pros2.eigen</span> <span class="co"># The slot $vectors = rotation of the PCA</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>eigen() decomposition
$values
[1] 2.4488355 1.3653171 0.5554705 0.4048702 0.2255067

$vectors
           [,1]        [,2]        [,3]        [,4]        [,5]
[1,] -0.5616465  0.23664270  0.01486043  0.22708502  0.75945046
[2,] -0.2985223 -0.60174151 -0.66320198 -0.32126853  0.07577123
[3,] -0.1681278 -0.69638466  0.69313753  0.04517286  0.06558369
[4,] -0.4962203  0.31092357  0.26309227 -0.72394666 -0.25253840
[5,] -0.5665123  0.01680231 -0.10141557  0.56487128 -0.59111493</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># generate principal component scores by by hand, using matrix multiplication</span></span>
<span><span class="va">my.pros2.pcs</span> <span class="op">&lt;-</span> <span class="va">pros2.scaled</span> <span class="op">%*%</span> <span class="va">pros2.eigen</span><span class="op">$</span><span class="va">vectors</span></span>
<span><span class="co"># compare results</span></span>
<span><span class="fu">par</span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu">c</span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">pca.pros</span><span class="op">$</span><span class="va">rotated</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span>, main<span class="op">=</span><span class="st">"PCAtools"</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span>, v<span class="op">=</span><span class="fl">0</span>, lty<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">my.pros2.pcs</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span>, main<span class="op">=</span><span class="st">"\"By hand\""</span>, xlab<span class="op">=</span><span class="st">"PC1"</span>, ylab<span class="op">=</span><span class="st">"PC2"</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span>, v<span class="op">=</span><span class="fl">0</span>, lty<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/04-principal-component-analysis-rendered-pca-by-hand-1.png" alt="Side-by-side scatter plots of the second versus first principal components calculated by prcomp() (left) and by hand (right). The left scatter plot is the same as the right scatter plot but with swapped axes." class="figure mx-auto d-block"><figcaption>
Scatter plots of the second versus first principal components calculated
by prcomp() (left) and by hand (right).
</figcaption></figure><div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">par</span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu">c</span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># Note that the axis orientations may be swapped but the relative positions of the dots should be the same in both plots.</span></span></code></pre>
</div>
</div>
</div>
</div>
<p>One way to visualise how principal components relate to the original
variables is by creating a biplot. Biplots usually show two principal
components plotted against each other. Observations are sometimes
labelled with numbers. The contribution of each original variable to the
principal components displayed is then shown by arrows (generated from
those two columns of the rotation matrix that correspond to the
principal components shown). See <code>help("PCAtools::biplot")</code>
for arguments and their meaning. For instance, <code>lab</code> or
<code>colBy</code> may be useful. Note that there are several biplot
implementations in different R libraries. It is thus a good idea to
specify the desired package when calling <code>biplot()</code>. A biplot
of the first two principal components can be generated as follows:</p>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">PCAtools</span><span class="fu">::</span><span class="fu">biplot</span><span class="op">(</span><span class="va">pca.pros</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/04-principal-component-analysis-rendered-stats-biplot-1.png" alt="Scatter plot of the second principal component versus the first principal component. Observations as points, numerically labelled." class="figure mx-auto d-block"><figcaption>
Scatter plot of the first two principal components with observations
numerically labelled.
</figcaption></figure><p>This biplot shows the position of each patient on a 2-dimensional
plot where loadings can be observed via the red arrows associated with
each of the variables. The variables <code>lpsa</code>,
<code>lcavol</code> and <code>lcp</code> are associated with positive
values on PC1 while positive values on PC2 are associated with the
variables <code>lbph</code> and <code>lweight</code>. The length of the
arrows indicates how much each variable contributes to the calculation
of each principal component.</p>
<p>The left and bottom axes show normalised principal component scores.
The axes on the top and right of the plot are used to interpret the
loadings, where loadings are scaled by the standard deviation of the
principal components (<code>pca.pros$sdev</code>) times the square root
the number of observations.</p>
</section><section><h2 class="section-heading" id="advantages-and-disadvantages-of-pca">Advantages and disadvantages of PCA<a class="anchor" aria-label="anchor" href="#advantages-and-disadvantages-of-pca"></a>
</h2>
<hr class="half-width">
<p>Advantages:</p>
<ul>
<li>It is a relatively easy to use and popular method.</li>
<li>Various software/packages are available to run a PCA.</li>
<li>The calculations used in a PCA are simple to understand compared to
other methods for dimension reduction.</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>It assumes that variables in a dataset are correlated.</li>
<li>It is sensitive to the scale at which input variables are measured.
If input variables are measured at different scales, the variables with
large variance relative to the scale of measurement will have greater
impact on the principal components relative to variables with smaller
variance. In many cases, this is not desirable.</li>
<li>It is not robust against outliers, meaning that very large or small
data points can have a large effect on the output of the PCA.</li>
<li>PCA assumes a linear relationship between variables which is not
always a realistic assumption.</li>
<li>It can be difficult to interpret the meaning of the principal
components, especially when including them in further analyses
(e.g. inclusion in a linear regression).</li>
</ul></section><section><h2 class="section-heading" id="using-pca-to-analyse-gene-expression-data">Using PCA to analyse gene expression data<a class="anchor" aria-label="anchor" href="#using-pca-to-analyse-gene-expression-data"></a>
</h2>
<hr class="half-width">
<p>The <a href="https://carpentries-incubator.github.io/high-dimensional-stats-r/data/index.html" class="external-link">dataset</a>
we will be analysing in this lesson includes two subsets of data:</p>
<ul>
<li>a matrix of gene expression data showing microarray results for
different probes used to examine gene expression profiles in 91
different breast cancer patient samples.</li>
<li>metadata associated with the gene expression results detailing
information from patients from whom samples were taken.</li>
</ul>
<p>In this section, we will work through Challenges and you will apply
your own PCA to the high-dimensional gene expression data using what we
have learnt so far.</p>
<p>We will first load the microarray breast cancer gene expression data
and associated metadata, downloaded from the <a href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2990" class="external-link">Gene
Expression Omnibus</a>.</p>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"SummarizedExperiment"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: MatrixGenerics</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: matrixStats</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'MatrixGenerics'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:matrixStats':

    colAlls, colAnyNAs, colAnys, colAvgsPerRowSet, colCollapse,
    colCounts, colCummaxs, colCummins, colCumprods, colCumsums,
    colDiffs, colIQRDiffs, colIQRs, colLogSumExps, colMadDiffs,
    colMads, colMaxs, colMeans2, colMedians, colMins, colOrderStats,
    colProds, colQuantiles, colRanges, colRanks, colSdDiffs, colSds,
    colSums2, colTabulates, colVarDiffs, colVars, colWeightedMads,
    colWeightedMeans, colWeightedMedians, colWeightedSds,
    colWeightedVars, rowAlls, rowAnyNAs, rowAnys, rowAvgsPerColSet,
    rowCollapse, rowCounts, rowCummaxs, rowCummins, rowCumprods,
    rowCumsums, rowDiffs, rowIQRDiffs, rowIQRs, rowLogSumExps,
    rowMadDiffs, rowMads, rowMaxs, rowMeans2, rowMedians, rowMins,
    rowOrderStats, rowProds, rowQuantiles, rowRanges, rowRanks,
    rowSdDiffs, rowSds, rowSums2, rowTabulates, rowVarDiffs, rowVars,
    rowWeightedMads, rowWeightedMeans, rowWeightedMedians,
    rowWeightedSds, rowWeightedVars</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: GenomicRanges</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: stats4</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: BiocGenerics</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'BiocGenerics'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:stats':

    IQR, mad, sd, var, xtabs</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:base':

    anyDuplicated, aperm, append, as.data.frame, basename, cbind,
    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,
    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,
    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,
    Position, rank, rbind, Reduce, rownames, sapply, saveRDS, setdiff,
    table, tapply, union, unique, unsplit, which.max, which.min</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: S4Vectors</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'S4Vectors'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following object is masked from 'package:utils':

    findMatches</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:base':

    expand.grid, I, unname</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: IRanges</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: GenomeInfoDb</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: Biobase</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Welcome to Bioconductor

    Vignettes contain introductory material; view with
    'browseVignettes()'. To cite Bioconductor, see
    'citation("Biobase")', and for packages 'citation("pkgname")'.</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'Biobase'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following object is masked from 'package:MatrixGenerics':

    rowMedians</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:matrixStats':

    anyMissing, rowMedians</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: replacing previous import 'S4Arrays::makeNindexFromArrayViewport' by
'DelayedArray::makeNindexFromArrayViewport' when loading 'SummarizedExperiment'</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb54">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cancer</span> <span class="op">&lt;-</span> <span class="fu">readRDS</span><span class="op">(</span><span class="st">"data/cancer_expression.rds"</span><span class="op">)</span></span>
<span><span class="va">mat</span> <span class="op">&lt;-</span> <span class="fu">assay</span><span class="op">(</span><span class="va">cancer</span><span class="op">)</span></span>
<span><span class="va">metadata</span> <span class="op">&lt;-</span> <span class="fu">colData</span><span class="op">(</span><span class="va">cancer</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb55">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">head</span><span class="op">(</span><span class="va">mat</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb56">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">head</span><span class="op">(</span><span class="va">metadata</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb57">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Check that column names and row names match, if they do should return TRUE</span></span>
<span><span class="fu">all</span><span class="op">(</span><span class="fu">colnames</span><span class="op">(</span><span class="va">mat</span><span class="op">)</span> <span class="op">==</span> <span class="fu">rownames</span><span class="op">(</span><span class="va">metadata</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] TRUE</code></pre>
</div>
<p>The <code>mat</code> variable contains a matrix of gene expression
profiles for each sample. Rows represent gene expression measurements
and columns represent samples. The <code>metadata</code> variable
contains the metadata associated with the gene expression data including
the name of the study from which data originate, the age of the patient
from which the sample was taken, whether or not an oestrogen receptor
was involved in their cancer and the grade and size of the cancer for
each sample (represented by rows).</p>
<p>Microarray data are difficult to analyse for several reasons.
Firstly, formulating a research question using microarray data can be
difficult, especially if not much is known a priori about which genes
code for particular phenotypes of interest. Secondly, they are typically
high-dimensional and therefore are subject to the same difficulties
associated with analysing high-dimensional data. Finally, exploratory
analysis, which can be used to help formulate research questions and
display relationships, is difficult using microarray data due to the
number of potentially interesting response variables (i.e. expression
data from probes targeting different genes).</p>
<p>If researchers hypothesise that groups of genes (e.g. biological
pathways) may be associated with different phenotypic characteristics of
cancers (e.g. histologic grade, tumour size), using statistical methods
that reduce the number of columns in the microarray matrix to a smaller
number of dimensions representing groups of genes would help visualise
the data and address research questions regarding the effect different
groups of genes have on disease progression.</p>
<p>Using <strong><code>PCAtools</code></strong>, we will perform PCA on
the cancer gene expression data, plot the amount of variation in the
data explained by each principal component and plot the most important
principal components against each other as well as understanding what
each principal component represents.</p>
<div id="challenge-3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-3" class="callout-inner">
<h3 class="callout-title">Challenge 3</h3>
<div class="callout-content">
<p>Apply a PCA to the cancer gene expression data using the
<code>pca()</code> function from
<strong><code>PCAtools</code></strong>.</p>
<p>Let us assume we only care about the principal components accounting
for the top 80% of the variance in the dataset. Use the
<code>removeVar</code> argument in <code>pca()</code> to remove the
principal components accounting for the bottom 20%.</p>
<p>As in the example using <code>prostate</code> data above, examine the
first 5 rows and columns of rotated data and loadings from your PCA.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb59">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pc</span> <span class="op">&lt;-</span> <span class="fu">pca</span><span class="op">(</span><span class="va">mat</span>, metadata <span class="op">=</span> <span class="va">metadata</span><span class="op">)</span> <span class="co"># implement a PCA</span></span>
<span><span class="co"># We can check the scree plot to see that many principal components explain a very small amount of</span></span>
<span><span class="co"># the total variance in the data</span></span>
<span></span>
<span><span class="co"># Let's remove the principal components with lowest 20% of the variance</span></span>
<span><span class="va">pc</span> <span class="op">&lt;-</span> <span class="fu">pca</span><span class="op">(</span><span class="va">mat</span>, metadata <span class="op">=</span> <span class="va">metadata</span>, removeVar <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>-- removing the lower 20% of variables based on variance</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb61">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Explore other arguments provided in pca</span></span>
<span><span class="va">pc</span><span class="op">$</span><span class="va">rotated</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span> <span class="co"># obtain the first 5 principal component scores for the first 5 observations</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>               PC1        PC2        PC3        PC4        PC5
GSM65752 -29.79105  43.866788   3.255903 -40.663138 15.3427597
GSM65753 -37.33911 -15.244788  -4.948201  -6.182795  9.4725870
GSM65755 -29.41462   7.846858 -22.880525 -16.149669 22.3821009
GSM65757 -33.35286   1.343573 -22.579568   2.200329 15.0082786
GSM65758 -40.51897  -8.491125   5.288498  14.007364  0.8739772</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb63">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pc</span><span class="op">$</span><span class="va">loadings</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span> <span class="co"># obtain the first 5 principal component loadings for the first 5 features</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>                    PC1          PC2          PC3        PC4          PC5
206378_at -0.0024680993 -0.053253543 -0.004068209 0.04068635  0.015078376
205916_at -0.0051557973  0.001315022 -0.009836545 0.03992371  0.038552048
206799_at  0.0005684075 -0.050657061 -0.009515725 0.02610233  0.006208078
205242_at  0.0130742288  0.028876408  0.007655420 0.04449641 -0.001061205
206509_at  0.0019031245 -0.054698479 -0.004667356 0.01566468  0.001306807</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb65">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">which.max</span><span class="op">(</span><span class="va">pc</span><span class="op">$</span><span class="va">loadings</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="co"># index of the maximal loading for the first principal component</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 49</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb67">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># principal component loadings for the feature</span></span>
<span><span class="co"># with the maximal loading on the first principal component:</span></span>
<span><span class="va">pc</span><span class="op">$</span><span class="va">loadings</span><span class="op">[</span><span class="fu">which.max</span><span class="op">(</span><span class="va">pc</span><span class="op">$</span><span class="va">loadings</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span><span class="op">)</span>, <span class="op">]</span> </span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>                   PC1          PC2         PC3          PC4          PC5
215281_x_at 0.03752947 -0.007369379 0.006243377 -0.008242589 -0.004783206
                   PC6          PC7         PC8         PC9          PC10
215281_x_at 0.01194012 -0.002822407 -0.01216792 0.001137451 -0.0009056616
                   PC11          PC12       PC13         PC14          PC15
215281_x_at -0.00196034 -0.0001676705 0.00699201 -0.002897995 -0.0005044658
                     PC16        PC17         PC18        PC19         PC20
215281_x_at -0.0004547916 0.002277035 -0.006199078 0.002708574 -0.006217326
                  PC21        PC22        PC23        PC24        PC25
215281_x_at 0.00516745 0.007625912 0.003434534 0.005460017 0.001477415
                   PC26         PC27          PC28         PC29       PC30
215281_x_at 0.002350428 0.0007183107 -0.0006195515 0.0006349803 0.00413627
                    PC31        PC32         PC33         PC34         PC35
215281_x_at 0.0001322301 0.003182956 -0.002123462 -0.001042769 -0.001729869
                    PC36        PC37        PC38          PC39        PC40
215281_x_at -0.006556369 0.005766949 0.002537993 -0.0002846248 -0.00018195
                     PC41        PC42         PC43          PC44         PC45
215281_x_at -0.0007970789 0.003888626 -0.008210075 -0.0009570174 0.0007998935
                     PC46         PC47        PC48        PC49         PC50
215281_x_at -0.0006931441 -0.005717836 0.005189649 0.002591188 0.0007810259
                   PC51        PC52         PC53         PC54        PC55
215281_x_at 0.006610815 0.005371134 -0.001704796 -0.002286475 0.001365417
                   PC56         PC57        PC58         PC59         PC60
215281_x_at 0.003529892 0.0003375981 0.009895923 -0.001564423 -0.006989092
                   PC61        PC62         PC63          PC64        PC65
215281_x_at 0.000971273 0.001345406 -0.003575415 -0.0005588113 0.006516669
                    PC66        PC67       PC68         PC69        PC70
215281_x_at -0.008770186 0.006699641 0.01284606 -0.005041574 0.007845653
                   PC71        PC72         PC73         PC74        PC75
215281_x_at 0.003964697 -0.01104367 -0.001506485 -0.001583824 0.003798343
                   PC76         PC77         PC78         PC79          PC80
215281_x_at 0.004817252 -0.001290033 -0.004402926 -0.003440367 -0.0001646198
                   PC81        PC82          PC83         PC84        PC85
215281_x_at 0.003923775 0.003179556 -0.0004388192 9.664648e-05 0.003501335
                   PC86        PC87          PC88         PC89         PC90
215281_x_at -0.00112973 0.006489667 -0.0005039785 -0.004296355 -0.002751513
                   PC91
215281_x_at 0.009785973</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb69">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">which.max</span><span class="op">(</span><span class="va">pc</span><span class="op">$</span><span class="va">loadings</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="co"># index of the maximal loading for the second principal component</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 27</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb71">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># principal component loadings for the feature</span></span>
<span><span class="co"># with the maximal loading on the second principal component:</span></span>
<span><span class="va">pc</span><span class="op">$</span><span class="va">loadings</span><span class="op">[</span><span class="fu">which.max</span><span class="op">(</span><span class="va">pc</span><span class="op">$</span><span class="va">loadings</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span>, <span class="op">]</span> </span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>                   PC1        PC2          PC3        PC4          PC5
211122_s_at 0.01649085 0.05090275 -0.003378728 0.05178144 -0.003742393
                    PC6         PC7          PC8        PC9        PC10
211122_s_at -0.00543753 -0.03522848 -0.006333521 0.01575401 0.004732546
                   PC11        PC12        PC13        PC14       PC15
211122_s_at 0.004687599 -0.01349892 0.005207937 -0.01731898 0.02323893
                   PC16       PC17        PC18       PC19        PC20
211122_s_at -0.02069509 0.01477432 0.005658529 0.02667751 -0.01333503
                    PC21        PC22       PC23         PC24        PC25
211122_s_at -0.003254036 0.003572342 0.01416779 -0.005511838 -0.02582847
                  PC26        PC27       PC28        PC29       PC30      PC31
211122_s_at 0.03405417 -0.01797345 0.01826328 0.005123959 0.01300763 0.0127127
                   PC32       PC33       PC34        PC35        PC36
211122_s_at 0.002477672 0.01933214 0.03017661 -0.01935071 -0.01960912
                   PC37        PC38        PC39        PC40       PC41
211122_s_at 0.004411188 -0.01263612 -0.02019279 -0.01441513 -0.0310399
                   PC42         PC43        PC44        PC45        PC46
211122_s_at -0.02540426 0.0007949801 -0.00200195 -0.01748543 0.006881834
                   PC47         PC48        PC49         PC50        PC51
211122_s_at 0.006690698 -0.004000732 -0.02747926 -0.006963189 -0.02232332
                     PC52        PC53        PC54        PC55       PC56
211122_s_at -0.0003089115 -0.01604491 0.005649511 -0.02629501 0.02332997
                   PC57        PC58        PC59        PC60         PC61
211122_s_at -0.01248022 -0.01563245 0.005369433 0.009445262 -0.005209349
                  PC62       PC63       PC64        PC65        PC66
211122_s_at 0.01787645 0.01629425 0.02457665 -0.02384242 0.002814479
                    PC67        PC68         PC69         PC70       PC71
211122_s_at 0.0004584731 0.007939733 -0.009554166 -0.003967123 0.01825668
                   PC72        PC73        PC74        PC75        PC76
211122_s_at -0.00580374 -0.02236727 0.001295688 -0.02264723 0.006855855
                   PC77         PC78       PC79         PC80        PC81
211122_s_at 0.004995447 -0.008404118 0.00442875 -0.001027912 0.006104406
                   PC82        PC83         PC84       PC85       PC86
211122_s_at -0.01988441 0.009667348 -0.008248781 0.01198369 0.01221713
                    PC87        PC88        PC89        PC90       PC91
211122_s_at -0.003864842 -0.02876816 -0.01771452 -0.02164973 0.01124148</code></pre>
</div>
<p>The function <code>pca()</code> is used to perform PCA, and uses as
input a matrix (<code>mat</code>) containing continuous numerical data
in which rows are data variables and columns are samples, and
<code>metadata</code> associated with the matrix in which rows represent
samples and columns represent data variables. It has options to centre
or scale the input data before a PCA is performed, although in this case
gene expression data do not need to be transformed prior to PCA being
carried out as variables are measured on a similar scale (values are
comparable between rows). The output of the <code>pca()</code> function
includes a lot of information such as loading values for each variable
(<code>loadings</code>), principal component scores
(<code>rotated</code>) and the amount of variance in the data explained
by each principal component.</p>
<p>Rotated data shows principal component scores for each sample and
each principal component. Loadings the contribution each variable makes
to each principal component.</p>
</div>
</div>
</div>
</div>
<div id="scaling-variables-for-pca" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="scaling-variables-for-pca" class="callout-inner">
<h3 class="callout-title">Scaling variables for PCA</h3>
<div class="callout-content">
<p>When running <code>pca()</code> above, we kept the default setting,
<code>scale=FALSE</code>. That means genes with higher variation in
their expression levels should have higher loadings, which is what we
are interested in. Whether or not to scale variables for PCA will depend
on your data and research question.</p>
<p>Note that this is different from normalising gene expression data.
Gene expression data have to be normalised before donwstream analyses
can be carried out. This is to reduce technical and other potentially
confounding factors. We assume that the expression data we use had been
normalised previously.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="choosing-how-many-components-are-important-to-explain-the-variance-in-the-data">Choosing how many components are important to explain the variance
in the data<a class="anchor" aria-label="anchor" href="#choosing-how-many-components-are-important-to-explain-the-variance-in-the-data"></a>
</h2>
<hr class="half-width">
<p>As in the example using the <code>prostate</code> dataset, we can use
a scree plot to compare the proportion of variance in the data explained
by each principal component. This allows us to understand how much
information in the microarray dataset is lost by projecting the
observations onto the first few principal components and whether these
principal components represent a reasonable amount of the variation. The
proportion of variance explained should sum to one.</p>
<div id="challenge-4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-4" class="callout-inner">
<h3 class="callout-title">Challenge 4</h3>
<div class="callout-content">
<p>This time using the <code>screeplot()</code> function in
<strong><code>PCAtools</code></strong>, create a scree plot to show
proportion of variance explained by the first 20 principal component
(hint: <code>components = 1:20</code>). Explain the output of the scree
plot in terms of proportion of the variance in the data explained by
each principal component.</p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4"> Show me the solution </h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" aria-labelledby="headingSolution4" data-bs-parent="#accordionSolution4">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb73">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pc</span> <span class="op">&lt;-</span> <span class="fu">pca</span><span class="op">(</span><span class="va">mat</span>, metadata <span class="op">=</span> <span class="va">metadata</span><span class="op">)</span></span>
<span><span class="fu">screeplot</span><span class="op">(</span><span class="va">pc</span>, components <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">20</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">ylim</span><span class="op">(</span><span class="fl">0</span>, <span class="fl">80</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Scale for y is already present.
Adding another scale for y, which will replace the existing scale.</code></pre>
</div>
<figure style="text-align: center"><img src="../fig/04-principal-component-analysis-rendered-scree-ex-1.png" alt="A bar and line plot showing the variance explained by principal components (PCs) of gene expression data. Blue bars depict the variance explained by each PC, while a red line depicts the cumulative variance explained by the PCs. The first principal component explains roughly 30% of the variance, while succeeding PCs explain less than 10%." class="figure mx-auto d-block"><figcaption>
A scree plot of the gene expression data.
</figcaption></figure><p>The first principal component explains around 33% of the variance in
the microarray data, the first 4 principal components explain around
50%, and 20 principal components explain around 75%. Many principal
components explain very little variation. A first ‘elbow’ appears to be
around 4-5 principal components, indicating that this may be a suitable
number of principal components. However, these principal components
cumulatively explain only 51-55% of the variance in the dataset.
Although the fact we are able to summarise most of the information in
the complex dataset in 4-5 principal components may be a useful result,
we may opt to retain more principal components (for example, 20) to
capture more of the variability in the dataset depending on research
question. A second ‘elbow’ around 12 principal components may provide a
good middleground. Note that first principal component (PC1) explains
more variation than other principal components (which is always the case
in PCA). The scree plot shows that the first principal component only
explains ~33% of the total variation in the microarray data and many
principal components explain very little variation. The red line shows
the cumulative percentage of explained variation with increasing
principal components.</p>
<p>Note that in this case 18 principal components are needed to explain
over 75% of variation in the data. This is not an unusual result for
complex biological datasets including genetic information as clear
relationships between groups are sometimes difficult to observe in the
data. The scree plot shows that using a PCA we have reduced 91
predictors to 18 in order to explain a significant amount of variation
in the data. See additional arguments in scree plot function for
improving the appearance of the plot.</p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="investigating-the-principal-components">Investigating the principal components<a class="anchor" aria-label="anchor" href="#investigating-the-principal-components"></a>
</h2>
<hr class="half-width">
<p>Once the most important principal components have been identified
using <code>screeplot()</code> and explored, these can be explored in
more detail by plotting principal components against each other and
highlighting points based on variables in the metadata. This will allow
any potential clustering of points according to demographic or
phenotypic variables to be seen.</p>
<p>We can use biplots to look for patterns in the output from the PCA.
Note that there are two functions called <code>biplot()</code>, one in
the package <strong><code>PCAtools</code></strong> and one in
<strong><code>stats</code></strong>. Both functions produce biplots but
their scales are different!</p>
<div id="challenge-5" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-5" class="callout-inner">
<h3 class="callout-title">Challenge 5</h3>
<div class="callout-content">
<p>Create a biplot of the first two principal components from your PCA
using <code>biplot()</code> function in
<strong><code>PCAtools</code></strong>. See
<code>help("PCAtools::biplot")</code> for arguments and their meaning.
For instance, <code>lab</code> or <code>colby</code> may be useful.</p>
<p>Examine whether the data appear to form clusters. Explain your
results.</p>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5"> Show me the solution </h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" aria-labelledby="headingSolution5" data-bs-parent="#accordionSolution5">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb75">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">PCAtools</span><span class="fu">::</span><span class="fu">biplot</span><span class="op">(</span><span class="va">pc</span>, lab <span class="op">=</span> <span class="cn">NULL</span>, colby <span class="op">=</span> <span class="st">'Grade'</span>, legendPosition <span class="op">=</span> <span class="st">'top'</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/04-principal-component-analysis-rendered-biplot-ex-1.png" alt="A biplot of PC2 against PC1 in the gene expression data, coloured by Grade. The points on the scatter plot separate clearly on PC1, but there is no clear grouping of samples based on Grade across these two groups." class="figure mx-auto d-block"><figcaption>
A biplot of PC2 against PC1 in the gene expression data, coloured by
Grade.
</figcaption></figure><p>The biplot shows the position of patient samples relative to PC1 and
PC2 in a 2-dimensional plot. Note that two groups are apparent along the
PC1 axis according to expressions of different genes while no separation
can be seem along the PC2 axis. Labels of patient samples are
automatically added in the biplot. Labels for each sample are added by
default, but can be removed if there is too much overlap in names. Note
that <strong><code>PCAtools</code></strong> does not scale biplot in the
same way as biplot using the <strong><code>stats</code></strong>
package.</p>
</div>
</div>
</div>
</div>
<p>Let’s consider this biplot in more detail, and also display the
loadings:</p>
<div id="challenge-6" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-6" class="callout-inner">
<h3 class="callout-title">Challenge 6</h3>
<div class="callout-content">
<p>Use <code>colby</code> and <code>lab</code> arguments in
<code>biplot()</code> to explore whether these two groups may cluster by
patient age or by whether or not the sample expresses the oestrogen
receptor gene (ER+ or ER-).</p>
<p>Note: You may see a warning about
<strong><code>ggrepel</code></strong>. This happens when there are many
labels but little space for plotting. This is not usually a serious
problem - not all labels will be shown.</p>
</div>
</div>
</div>
<div id="accordionSolution6" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution6" aria-expanded="false" aria-controls="collapseSolution6">
  <h4 class="accordion-header" id="headingSolution6"> Show me the solution </h4>
</button>
<div id="collapseSolution6" class="accordion-collapse collapse" aria-labelledby="headingSolution6" data-bs-parent="#accordionSolution6">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb76">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="fu">PCAtools</span><span class="fu">::</span><span class="fu">biplot</span><span class="op">(</span><span class="va">pc</span>,</span>
<span>    lab <span class="op">=</span> <span class="fu">paste0</span><span class="op">(</span><span class="va">pc</span><span class="op">$</span><span class="va">metadata</span><span class="op">$</span><span class="va">Age</span>,<span class="st">'years'</span><span class="op">)</span>,</span>
<span>    colby <span class="op">=</span> <span class="st">'ER'</span>,</span>
<span>    legendPosition <span class="op">=</span> <span class="st">'right'</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: ggrepel: 23 unlabeled data points (too many overlaps). Consider
increasing max.overlaps</code></pre>
</div>
<figure style="text-align: center"><img src="../fig/04-principal-component-analysis-rendered-pca-biplot-ex2-1.png" alt="A biplot of PC2 against PC1 in the gene expression data, coloured by ER status. The points on the scatter plot separate clearly on PC1, but there is no clear grouping of samples based on ER across these two groups, although there are more ER- samples in the rightmost cluster. Patient ages are overlaid as text near the points, but there is again no clear pattern." class="figure mx-auto d-block"><figcaption>
A biplot of PC2 against PC1 in the gene expression data, coloured by ER
status with patient ages overlaid.
</figcaption></figure><p>It appears that one cluster has more ER+ samples than the other
group.</p>
</div>
</div>
</div>
</div>
<p>So far, we have only looked at a biplot of PC1 versus PC2 which only
gives part of the picture. The <code>pairplots()</code> function in
<strong><code>PCAtools</code></strong> can be used to create multiple
biplots including different principal components.</p>
<div class="codewrapper sourceCode" id="cb78">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">pairsplot</span><span class="op">(</span><span class="va">pc</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.
Coordinate system already present. Adding new coordinate system, which will
replace the existing one.</code></pre>
</div>
<figure style="text-align: center"><img src="../fig/04-principal-component-analysis-rendered-pairsplot-1.png" alt="A triangular grid of scatter plots. The grid is the upper right triangle of a square, where each entry of the grid corresponds to a plot of one principal component against another. For example, the plot in the upper left corner of the plot corresponds to principal component one plotted against principal component 2, and the plot to the right of this plots principal component 1 against principal component 3. Points correspond to samples, and are coloured arbitrarily from light blue to dark blue." class="figure mx-auto d-block"><figcaption>
Multiple biplots produced by pairsplot().
</figcaption></figure><p>The plots show two apparent clusters involving the first principal
component only. No other clusters are found involving other principal
components. Each dot is coloured differently along a gradient of blues.
This can potentially help identifying the same observation/individual in
several panels. Here too, the argument <code>colby</code> allows you to
set custom colours.</p>
<p>Finally, it can sometimes be of interest to compare how certain
variables contribute to different principal components. This can be
visualised with <code>plotloadings()</code> from the
<strong><code>PCAtools</code></strong> package. The function checks the
range of loadings for each principal component specified (default: first
five principal components). It then selects the features in the top and
bottom 5% of these ranges and displays their loadings. This behaviour
can be adjusted with the <code>rangeRetain</code> argument, which has
0.1 as the default value (i.e. 5% on each end of the range). NB, if
there are too many labels to be plotted, you will see a warning. This is
not a serious problem.</p>
<p><code>{r loadingsplots, fig.cap=c("Plot of the features in the top and bottom 5 % of the loadings range for the first principal component.", "Plot of the features in the top and bottom 5 % of the loadings range for the second principal component.", fig.cap = "Plot of the features in the top and bottom 5 % of the loadings range for the either the first or second principal components."), fig.alt=c("A plot with component loading versus principal component index. The plot shows the loadings of features at the top and bottom 5 % of the loadings range and only for the first principal component. The component loading values for four features are shown. The features with the highest loadings are shown at the top of the plot and consist of three features, each with a blue dot and feature label. The feature with the lowest loading is shown at the bottom of the plot and delineated by a yellow dot with a feature label.",      "A plot with component loadings versus principal component index. The plot shows the loadings of features at the top and bottom 5 % of the loadings range and only for the second principal component. The component loading values for 9 features are shown. The features with the highest loadings are shown at the top of the plot and consist of 6 features, each with a blue dot and feature label. The 3 features at the lower range are at the bottom of the plot and delineated by a yellow dot with a feature label.",      fig.alt = "A plot with component loading versus principal component index. The plot shows the loadings of features at the top and bottom 5 % of the loadings range for either principal component. The component loading values for several features are shown with loading score-delineated colours ranging from dark blue for the highest loadings and dark yellow for the lowest loadings. Each point has a feature label. The loadings of features points for the first principal component are concentrated towards the top, while the loadings of features points for the second principal components are concentrated at the top, middle and bottom of the range.")} plotloadings(pc, c("PC1"), rangeRetain = 0.1) plotloadings(pc, c("PC2"), rangeRetain = 0.1) plotloadings(pc, c("PC1", "PC2"), rangeRetain = 0.1)</code></p>
<p>You can see how the third code line produces more dots, some of which
do not have extreme loadings. This is because all loadings selected for
any principal component are shown for all other principal components.
For instance, it is plausible that features which have high loadings on
PC1 may have lower ones on PC2.</p>
</section><section><h2 class="section-heading" id="using-pca-output-in-further-analysis">Using PCA output in further analysis<a class="anchor" aria-label="anchor" href="#using-pca-output-in-further-analysis"></a>
</h2>
<hr class="half-width">
<p>The output of PCA can be used to interpret data or can be used in
further analyses. For example, the PCA outputs new variables (principal
components) which represent several variables in the original dataset.
These new variables are useful for further exploring data, for example,
comparing principal component scores between groups or including the new
variables in linear regressions. Because the principal components are
uncorrelated (and independent) they can be included together in a single
linear regression.</p>
<div id="principal-component-regression" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="principal-component-regression" class="callout-inner">
<h3 class="callout-title">Principal component regression</h3>
<div class="callout-content">
<p>PCA is often used to reduce large numbers of correlated variables
into fewer uncorrelated variables that can then be included in linear
regression or other models. This technique is called principal component
regression (PCR) and it allows researchers to examine the effect of
several correlated explanatory variables on a single response variable
in cases where a high degree of correlation initially prevents them from
being included in the same model. Principal component regression (PCR)
is just one example of how principal components can be used in further
analysis of data. When carrying out PCR, the variable of interest
(response/dependent variable) is regressed against the principal
components calculated using PCA, rather than against each individual
explanatory variable from the original dataset. As there as many
principal components created from PCA as there are variables in the
dataset, we must select which principal components to include in PCR.
This can be done by examining the amount of variation in the data
explained by each principal component (see above).</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="further-reading">Further reading<a class="anchor" aria-label="anchor" href="#further-reading"></a>
</h2>
<hr class="half-width">
<ul>
<li>James, G., Witten, D., Hastie, T. &amp; Tibshirani, R. (2013) An
Introduction to Statistical Learning with Applications in R. Chapter 6.3
(Dimension Reduction Methods), Chapter 10 (Unsupervised Learning).</li>
<li>
<a href="https://dx.doi.org/10.1098/rsta.2015.0202" class="external-link">Jolliffe, I.T.
&amp; Cadima, J. (2016) Principal component analysis: a review and
recent developments. Phil. Trans. R. Soc A 374.</a>.</li>
<li><a href="https://doi.org/10.1098/rsta.2009.0159" class="external-link">Johnstone, I.M.
&amp; Titterington, D.M. (2009) Statistical challenges of
high-dimensional data. Phil. Trans. R. Soc A 367.</a></li>
<li>
<a href="https://www.analyticsvidhya.com/blog/2016/03/pca-practical-guide-principal-component-analysis-python/" class="external-link">PCA:
A Practical Guide to Principal Component Analysis, Analytics
Vidhya</a>.</li>
<li>
<a href="https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c" class="external-link">A
One-Stop Shop for Principal Component Analysis, Towards Data
Science</a>.</li>
</ul>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>A principal component analysis is a statistical approach used to
reduce dimensionality in high-dimensional datasets (i.e. where <span class="math inline">\(p\)</span> is equal or greater than <span class="math inline">\(n\)</span>).</li>
<li>PCA may be used to create a low-dimensional set of features from a
larger set of variables. Examples of when a PCA may be useful include
reducing high-dimensional datasets to fewer variables for use in a
linear regression and for identifying groups with similar features.</li>
<li>PCA is a useful dimensionality reduction technique used in the
analysis of complex biological datasets (e.g. high throughput data or
genetics data).</li>
<li>The first principal component represents the dimension along which
there is maximum variation in the data. Subsequent principal components
represent dimensions with progressively less variation.</li>
<li>Scree plots and biplots may be used to show: 1. how much variation
in the data is explained by each principal component and 2. how data
points cluster according to principal component scores and which
variables are associated with these scores.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-05-factor-analysis"><p>Content from <a href="05-factor-analysis.html">Factor analysis</a></p>
<hr>
<p>Last updated on 2025-09-02 |

        <a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/edit/main/episodes/05-factor-analysis.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 40 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is factor analysis and when can it be used?</li>
<li>What are communality and uniqueness in factor analysis?</li>
<li>How to decide on the number of factors to use?</li>
<li>How to interpret the output of factor analysis?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Perform a factor analysis on high-dimensional data.</li>
<li>Select an appropriate number of factors.</li>
<li>Interpret the output of factor analysis.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<hr class="half-width">
<p>Biologists often encounter high-dimensional datasets from which they
wish to extract underlying features – they need to carry out
dimensionality reduction. The last episode dealt with one method to
achieve this, called principal component analysis (PCA), which expressed
new dimension-reduced components as linear combinations of the original
features in the dataset. Principal components can therefore be difficult
to interpret. Here, we introduce a related but more interpretable method
called factor analysis (FA), which constructs new components, called
<em>factors</em>, that explicitly represent underlying <em>(latent)</em>
constructs in our data. Like PCA, FA uses linear combinations, but uses
latent constructs instead. FA is therefore often more interpretable and
useful when we would like to extract meaning from our dimension-reduced
set of variables.</p>
<p>There are two types of FA, called exploratory and confirmatory factor
analysis (EFA and CFA). Both EFA and CFA aim to reproduce the observed
relationships among a group of features with a smaller set of latent
variables. EFA is used in a descriptive (exploratory) manner to uncover
which measured variables are reasonable indicators of the various latent
dimensions. In contrast, CFA is conducted in an <em>a priori</em>,
hypothesis-testing manner that requires strong empirical or theoretical
foundations. We will mainly focus on EFA here, which is used to group
features into a specified number of latent factors.</p>
<p>Unlike with PCA, researchers using FA have to specify the number of
latent variables (factors) at the point of running the analysis.
Researchers may use exploratory data analysis methods (including PCA) to
provide an initial estimate of how many factors adequately explain the
variation observed in a dataset. In practice, a range of different
values is usually tested.</p>
<div class="section level3">
<h3 id="motivating-example-student-scores">Motivating example: student scores<a class="anchor" aria-label="anchor" href="#motivating-example-student-scores"></a>
</h3>
<p>One scenario for using FA would be whether student scores in
different subjects can be summarised by certain subject categories. Take
a look at the hypothetical dataset below. If we were to run and EFA on
this, we might find that the scores can be summarised well by two
factors, which we can then interpret. We have labelled these
hypothetical factors “mathematical ability” and “writing ability”.</p>
<figure style="text-align: center"><img src="../fig/table_for_fa.png" alt="A table displaying data of student scores across several subjects. Each row displays the scores across different subjects for a given individual. The plot is annotated at the top with a curly bracket labelled Factor 1: mathematical ability and encompasses the data for the student scores is Arithmetic, Algebra, Geometry, and Statistics. Similarly, the subjects Creative Writing, Literature, Spelling/Grammar are encompassed by a different curly bracket with label Factor 2: writing ability." width="5448" class="figure mx-auto d-block"><figcaption>
Student scores data across several subjects with hypothesised factors.
</figcaption></figure><p>So, EFA is designed to identify a specified number of unobservable
factors from observable features contained in the original dataset. This
is slightly different from PCA, which does not do this directly. Just to
recap, PCA creates as many principal components as there are features in
the dataset, each component representing a different linear combination
of features. The principal components are ordered by the amount of
variance they account for.</p>
</div>
</section><section><h2 class="section-heading" id="prostate-cancer-patient-data">Prostate cancer patient data<a class="anchor" aria-label="anchor" href="#prostate-cancer-patient-data"></a>
</h2>
<hr class="half-width">
<p>We revisit the [<code>prostate</code>]((<a href="https://carpentries-incubator.github.io/high-dimensional-stats-r/data/index.html" class="external-link uri">https://carpentries-incubator.github.io/high-dimensional-stats-r/data/index.html</a>)
dataset of 97 men who have prostate cancer. Although not strictly a
high-dimensional dataset, as with other episodes, we use this dataset to
explore the method.</p>
<p>In this example, we use the clinical variables to identify factors
representing various clinical variables from prostate cancer patients.
Two principal components have already been identified as explaining a
large proportion of variance in the data when these data were analysed
in the PCA episode. We may expect a similar number of factors to exist
in the data.</p>
<p>Let’s subset the data to just include the log-transformed clinical
variables for the purposes of this episode:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">prostate</span> <span class="op">&lt;-</span> <span class="fu">readRDS</span><span class="op">(</span><span class="st">"data/prostate.rds"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">nrow</span><span class="op">(</span><span class="va">prostate</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 97</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">head</span><span class="op">(</span><span class="va">prostate</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>      lcavol  lweight age      lbph svi       lcp gleason pgg45       lpsa
1 -0.5798185 2.769459  50 -1.386294   0 -1.386294       6     0 -0.4307829
2 -0.9942523 3.319626  58 -1.386294   0 -1.386294       6     0 -0.1625189
3 -0.5108256 2.691243  74 -1.386294   0 -1.386294       7    20 -0.1625189
4 -1.2039728 3.282789  58 -1.386294   0 -1.386294       6     0 -0.1625189
5  0.7514161 3.432373  62 -1.386294   0 -1.386294       6     0  0.3715636
6 -1.0498221 3.228826  50 -1.386294   0 -1.386294       6     0  0.7654678</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#select five log-transformed clinical variables for further analysis</span></span>
<span><span class="va">pros2</span> <span class="op">&lt;-</span> <span class="va">prostate</span><span class="op">[</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"lcavol"</span>, <span class="st">"lweight"</span>, <span class="st">"lbph"</span>, <span class="st">"lcp"</span>, <span class="st">"lpsa"</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="fu">head</span><span class="op">(</span><span class="va">pros2</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>      lcavol  lweight      lbph       lcp       lpsa
1 -0.5798185 2.769459 -1.386294 -1.386294 -0.4307829
2 -0.9942523 3.319626 -1.386294 -1.386294 -0.1625189
3 -0.5108256 2.691243 -1.386294 -1.386294 -0.1625189
4 -1.2039728 3.282789 -1.386294 -1.386294 -0.1625189
5  0.7514161 3.432373 -1.386294 -1.386294  0.3715636
6 -1.0498221 3.228826 -1.386294 -1.386294  0.7654678</code></pre>
</div>
</section><section><h2 class="section-heading" id="performing-exploratory-factor-analysis">Performing exploratory factor analysis<a class="anchor" aria-label="anchor" href="#performing-exploratory-factor-analysis"></a>
</h2>
<hr class="half-width">
<p>EFA may be implemented in R using the <code>factanal()</code>
function from the <strong><code>stats</code></strong> package (which is
a built-in package in base R). This function fits a factor analysis by
maximising the log-likelihood using a data matrix as input. The number
of factors to be fitted in the analysis is specified by the user using
the <code>factors</code> argument.</p>
<div id="challenge-1-3-mins" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-1-3-mins" class="callout-inner">
<h3 class="callout-title">Challenge 1 (3 mins)</h3>
<div class="callout-content">
<p>Use the <code>factanal()</code> function to identify the minimum
number of factors necessary to explain most of the variation in the
data</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Include one factor only</span></span>
<span><span class="va">pros_fa</span> <span class="op">&lt;-</span> <span class="fu">factanal</span><span class="op">(</span><span class="va">pros2</span>, factors <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">pros_fa</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Call:
factanal(x = pros2, factors = 1)

Uniquenesses:
 lcavol lweight    lbph     lcp    lpsa
  0.149   0.936   0.994   0.485   0.362

Loadings:
        Factor1
lcavol  0.923
lweight 0.253
lbph
lcp     0.718
lpsa    0.799

               Factor1
SS loadings      2.074
Proportion Var   0.415

Test of the hypothesis that 1 factor is sufficient.
The chi square statistic is 29.81 on 5 degrees of freedom.
The p-value is 1.61e-05 </code></pre>
</div>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># p-value &lt;0.05 suggests that one factor is not sufficient </span></span>
<span><span class="co"># we reject the null hypothesis that one factor captures full</span></span>
<span><span class="co"># dimensionality in the dataset</span></span>
<span></span>
<span><span class="co"># Include two factors</span></span>
<span><span class="va">pros_fa</span> <span class="op">&lt;-</span> <span class="fu">factanal</span><span class="op">(</span><span class="va">pros2</span>, factors <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">pros_fa</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Call:
factanal(x = pros2, factors = 2)

Uniquenesses:
 lcavol lweight    lbph     lcp    lpsa
  0.121   0.422   0.656   0.478   0.317

Loadings:
        Factor1 Factor2
lcavol   0.936
lweight  0.165   0.742
lbph             0.586
lcp      0.722
lpsa     0.768   0.307

               Factor1 Factor2
SS loadings      2.015   0.992
Proportion Var   0.403   0.198
Cumulative Var   0.403   0.601

Test of the hypothesis that 2 factors are sufficient.
The chi square statistic is 0.02 on 1 degree of freedom.
The p-value is 0.878 </code></pre>
</div>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># p-value &gt;0.05 suggests that two factors is sufficient </span></span>
<span><span class="co"># we cannot reject the null hypothesis that two factors captures</span></span>
<span><span class="co"># full dimensionality in the dataset</span></span>
<span></span>
<span><span class="co">#Include three factors</span></span>
<span><span class="va">pros_fa</span> <span class="op">&lt;-</span> <span class="fu">factanal</span><span class="op">(</span><span class="va">pros2</span>, factors <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in factanal(pros2, factors = 3): 3 factors are too many for 5 variables</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Error shows that fitting three factors are not appropriate</span></span>
<span><span class="co"># for only 5 variables (number of factors too high)</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
<p>The output of <code>factanal()</code> shows the loadings for each of
the input variables associated with each factor. The loadings are values
between -1 and 1 which represent the relative contribution each input
variable makes to the factors. Positive values show that these variables
are positively related to the factors, while negative values show a
negative relationship between variables and factors. Loading values are
missing for some variables because R does not print loadings less than
0.1.</p>
</section><section><h2 class="section-heading" id="how-many-factors-do-we-need">How many factors do we need?<a class="anchor" aria-label="anchor" href="#how-many-factors-do-we-need"></a>
</h2>
<hr class="half-width">
<p>There are numerous ways to select the “best” number of factors. One
is to use the minimum number of features that does not leave a
significant amount of variance unaccounted for. In practice, we repeat
the factor analysis for different numbers of factors (by specifying
different values in the <code>factors</code> argument). If we have an
idea of how many factors there will be before analysis, we can start
with that number. The final section of the analysis output then shows
the results of a hypothesis test in which the null hypothesis is that
the number of factors used in the model is sufficient to capture most of
the variation in the dataset. If the p-value is less than our
significance level (for example 0.05), we reject the null hypothesis
that the number of factors is sufficient and we repeat the analysis with
more factors. When the p-value is greater than our significance level,
we do not reject the null hypothesis that the number of factors used
captures variation in the data. We may therefore conclude that this
number of factors is sufficient.</p>
<p>Like PCA, the fewer factors that can explain most of the variation in
the dataset, the better. It is easier to explore and interpret results
using a smaller number of factors which represent underlying features in
the data.</p>
</section><section><h2 class="section-heading" id="variance-accounted-for-by-factors---communality-and-uniqueness">Variance accounted for by factors - communality and uniqueness<a class="anchor" aria-label="anchor" href="#variance-accounted-for-by-factors---communality-and-uniqueness"></a>
</h2>
<hr class="half-width">
<p>The <em>communality</em> of a variable is the sum of its squared
loadings. It represents the proportion of the variance in a variable
that is accounted for by the FA model.</p>
<p><em>Uniqueness</em> is the opposite of communality and represents the
amount of variation in a variable that is not accounted for by the FA
model. Uniqueness is calculated by subtracting the communality value
from 1. If uniqueness is high for a given variable, that means this
variable is not well explained/accounted for by the factors
identified.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">apply</span><span class="op">(</span><span class="va">pros_fa</span><span class="op">$</span><span class="va">loadings</span><span class="op">^</span><span class="fl">2</span>, <span class="fl">1</span>, <span class="va">sum</span><span class="op">)</span>  <span class="co">#communality</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>   lcavol   lweight      lbph       lcp      lpsa
0.8793780 0.5782317 0.3438669 0.5223639 0.6832788 </code></pre>
</div>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fl">1</span> <span class="op">-</span> <span class="fu">apply</span><span class="op">(</span><span class="va">pros_fa</span><span class="op">$</span><span class="va">loadings</span><span class="op">^</span><span class="fl">2</span>, <span class="fl">1</span>, <span class="va">sum</span><span class="op">)</span>  <span class="co">#uniqueness</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>   lcavol   lweight      lbph       lcp      lpsa
0.1206220 0.4217683 0.6561331 0.4776361 0.3167212 </code></pre>
</div>
</section><section><h2 class="section-heading" id="visualising-the-contribution-of-each-variable-to-the-factors">Visualising the contribution of each variable to the factors<a class="anchor" aria-label="anchor" href="#visualising-the-contribution-of-each-variable-to-the-factors"></a>
</h2>
<hr class="half-width">
<p>Similar to a biplot as we produced in the PCA episode, we can “plot
the loadings”. This shows how each original variable contributes to each
of the factors we chose to visualise.</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#First, carry out factor analysis using two factors</span></span>
<span><span class="va">pros_fa</span> <span class="op">&lt;-</span> <span class="fu">factanal</span><span class="op">(</span><span class="va">pros2</span>, factors <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#plot loadings for each factor</span></span>
<span><span class="fu">plot</span><span class="op">(</span></span>
<span>  <span class="va">pros_fa</span><span class="op">$</span><span class="va">loadings</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span>, </span>
<span>  <span class="va">pros_fa</span><span class="op">$</span><span class="va">loadings</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span>,</span>
<span>  xlab <span class="op">=</span> <span class="st">"Factor 1"</span>, </span>
<span>  ylab <span class="op">=</span> <span class="st">"Factor 2"</span>, </span>
<span>  ylim <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>  xlim <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>  main <span class="op">=</span> <span class="st">"Factor analysis of prostate data"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>h <span class="op">=</span> <span class="fl">0</span>, v <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#add column names to each point</span></span>
<span><span class="fu">text</span><span class="op">(</span></span>
<span>  <span class="va">pros_fa</span><span class="op">$</span><span class="va">loadings</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="fl">0.08</span>, </span>
<span>  <span class="va">pros_fa</span><span class="op">$</span><span class="va">loadings</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">+</span> <span class="fl">0.08</span>,</span>
<span>  <span class="fu">colnames</span><span class="op">(</span><span class="va">pros2</span><span class="op">)</span>,</span>
<span>  col <span class="op">=</span> <span class="st">"blue"</span></span>
<span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/05-factor-analysis-rendered-biplot-1.png" alt="A scatter plot of the factor 2 loadings for each feature versus the factor 2 loadings for each feature. The lpsa, lcavol and lcp feature points are located in the east of the plot, indicating a high loading on factor 1 and close to zero loading on factor 2. The lbph and lweight features are located in the north of the plot, indicating a close to zero loading on factor 1 and a high loading on factor 2." class="figure mx-auto d-block"><figcaption>
Factor 2 loadings versus factor 1 loadings for each feature.
</figcaption></figure><div id="k-medoids-pam" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="k-medoids-pam" class="callout-inner">
<h3 class="callout-title">K-medoids (PAM)</h3>
<div class="callout-content">
<p>One problem with K-means is that using the mean to define cluster
centroids means that clusters can be very sensitive to outlying
observations.</p>
<p>K-medoids, also known as “partitioning around medoids (PAM)” is
similar to K-means, but uses the median rather than the mean as the
method for defining cluster centroids. Using the median rather than the
mean reduces sensitivity of clusters to outliers in the data.</p>
<p>K-medioids has had popular application in genomics, for example the
well-known PAM50 gene set in breast cancer, which has seen some
prognostic applications.</p>
<p>The following example shows how cluster centroids differ when created
using medians rather than means.</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">rnorm</span><span class="op">(</span><span class="fl">20</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu">rnorm</span><span class="op">(</span><span class="fl">20</span><span class="op">)</span></span>
<span><span class="va">x</span><span class="op">[</span><span class="fl">10</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="fl">10</span><span class="op">]</span> <span class="op">+</span> <span class="fl">10</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, pch <span class="op">=</span> <span class="fl">16</span><span class="op">)</span></span>
<span><span class="fu">points</span><span class="op">(</span><span class="fu">mean</span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="fu">mean</span><span class="op">(</span><span class="va">y</span><span class="op">)</span>, pch <span class="op">=</span> <span class="fl">16</span>, col <span class="op">=</span> <span class="st">"firebrick"</span><span class="op">)</span></span>
<span><span class="fu">points</span><span class="op">(</span><span class="fu">median</span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="fu">median</span><span class="op">(</span><span class="va">y</span><span class="op">)</span>, pch <span class="op">=</span> <span class="fl">16</span>, col <span class="op">=</span> <span class="st">"dodgerblue"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/05-factor-analysis-rendered-unnamed-chunk-1-1.png" alt="Scatter plot of random data y versus x. There are many black points on the plot representing the data. Two additional points are shown: the (mean(x), mean(y)) co-ordinate point in red and the (median(x), median(y)) co-ordinate point in blue. The median co-ordinate point in blue has a lower x value and is shown to the left of the red mean co-ordinate point." class="figure mx-auto d-block"><figcaption>
Scatter plot of random data y versus x with the (mean(x), mean(y))
co-ordinate point shown in red and the (median(x), median(y))
co-ordinate point shown in blue.
</figcaption></figure><p>PAM can be carried out using <code>pam()</code> from the
<strong><code>cluster</code></strong> package.</p>
</div>
</div>
</div>
<div id="challenge-2-3-mins" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-2-3-mins" class="callout-inner">
<h3 class="callout-title">Challenge 2 (3 mins)</h3>
<div class="callout-content">
<p>Use the output from your factor analysis and the plots above to
interpret the results of your analysis.</p>
<p>What variables are most important in explaining each factor? Do you
think this makes sense biologically? Consider or discuss in groups.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>This plot suggests that the variables <code>lweight</code> and
<code>lbph</code> are associated with high values on factor 2 (but lower
values on factor 1) and the variables lcavol, lcp and lpsa are
associated with high values on factor 1 (but lower values on factor 2).
There appear to be two ‘clusters’ of variables which can be represented
by the two factors.</p>
<p>The grouping of weight and enlargement (lweight and lbph) makes sense
biologically, as we would expect prostate enlargement to be associated
with greater weight. The groupings of lcavol, lcp, and lpsa also make
sense biologically, as larger cancer volume may be expected to be
associated with greater cancer spread and therefore higher PSA in the
blood.</p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="advantages-and-disadvantages-of-factor-analysis">Advantages and disadvantages of Factor Analysis<a class="anchor" aria-label="anchor" href="#advantages-and-disadvantages-of-factor-analysis"></a>
</h2>
<hr class="half-width">
<p>There are several advantages and disadvantages of using FA as a
dimensionality reduction method.</p>
<p>Advantages:</p>
<ul>
<li>FA is a useful way of combining different groups of data into known
representative factors, thus reducing dimensionality in a dataset.</li>
<li>FA can take into account researchers’ expert knowledge when choosing
the number of factors to use, and can be used to identify latent or
hidden variables which may not be apparent from using other analysis
methods.</li>
<li>It is easy to implement with many software tools available to carry
out FA.</li>
<li>Confirmatory FA can be used to test hypotheses.</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>Justifying the choice of number of factors to use may be difficult
if little is known about the structure of the data before analysis is
carried out.</li>
<li>Sometimes, it can be difficult to interpret what factors mean after
analysis has been completed.</li>
<li>Like PCA, standard methods of carrying out FA assume that input
variables are continuous, although extensions to FA allow ordinal and
binary variables to be included (after transforming the input
matrix).</li>
</ul></section><section><h2 class="section-heading" id="further-reading">Further reading<a class="anchor" aria-label="anchor" href="#further-reading"></a>
</h2>
<hr class="half-width">
<ul>
<li>Gundogdu et al. (2019) Comparison of performances of Principal
Component Analysis (PCA) and Factor Analysis (FA) methods on the
identification of cancerous and healthy colon tissues. International
Journal of Mass Spectrometry 445:116204.</li>
<li>Kustra et al. (2006) A factor analysis model for functional
genomics. BMC Bioinformatics 7: <a href="../doi:10.1186/1471-2105-7-21" class="uri">doi:10.1186/1471-2105-7-21</a>.</li>
<li>Yong, A.G. &amp; Pearce, S. (2013) A beginner’s guide to factor
analysis: focusing on exploratory factor analysis. Tutorials in
Quantitative Methods for Psychology 9(2):79-94.</li>
<li>Confirmatory factor analysis can be carried out with the package <a href="https://www.lavaan.ugent.be/index.html" class="external-link">Lavaan</a>.</li>
<li>A more sophisticated implementation of EFA is available in the
packages <a href="https://cran.r-project.org/web/packages/EFA.dimensions/index.html" class="external-link">EFA.dimensions</a>
and <a href="https://personality-project.org/r/psych/" class="external-link">psych</a>.</li>
</ul>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>Factor analysis is a method used for reducing dimensionality in a
dataset by reducing variation contained in multiple variables into a
smaller number of uncorrelated factors.</li>
<li>PCA can be used to identify the number of factors to initially use
in factor analysis.</li>
<li>The <code>factanal()</code> function in R can be used to fit a
factor analysis, where the number of factors is specified by the
user.</li>
<li>Factor analysis can take into account expert knowledge when deciding
on the number of factors to use, but a disadvantage is that the output
requires careful interpretation.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-06-k-means"><p>Content from <a href="06-k-means.html">K-means</a></p>
<hr>
<p>Last updated on 2025-09-02 |

        <a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/edit/main/episodes/06-k-means.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 80 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do we detect real clusters in high-dimensional data?</li>
<li>How does K-means work and when should it be used?</li>
<li>How can we perform K-means in <code>R</code>?</li>
<li>How can we appraise a clustering and test cluster robustness?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand what clusters look like in in high-dimensional data.</li>
<li>Understand and perform K-means clustering in <code>R</code>.</li>
<li>Assess clustering performance using silhouette scores.</li>
<li>Assess cluster robustness using bootstrapping.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<hr class="half-width">
<p>As we saw in previous episodes, visualising high-dimensional data
with a large amount of features is difficult and can limit our
understanding of the data and associated processes. In some cases, a
known grouping causes this heterogeneity (sex, treatment groups, etc).
In other cases, heterogeneity may arise from the presence of unknown
subgroups in the data.</p>
<p>While PCA can be used to reduce the dimension of the dataset into a
smaller set of uncorrelated variables and factor analysis can be used to
identify underlying factors, clustering is a set of techniques that
allow us to discover unknown groupings.</p>
<p>Cluster analysis involves finding groups of observations that are
more similar to each other (according to some feature) than they are to
observations in other groups and are thus likely to represent the same
source of heterogeneity. Once groups (or clusters) of observations have
been identified using cluster analysis, further analyses or
interpretation can be carried out on the groups, for example, using
metadata to further explore groups.</p>
<p>Cluster analysis is commonly used to discover unknown groupings in
fields such as bioinformatics, genomics, and image processing, in which
large datasets that include many features are often produced.</p>
<p>There are various ways to look for clusters of observations in a
dataset using different <em>clustering algorithms</em>. One way of
clustering data is to minimise distance between observations within a
cluster and maximise distance between proposed clusters. Using this
process, we can also iteratively update clusters so that we become more
confident about the shape and size of the clusters.</p>
</section><section><h2 class="section-heading" id="what-is-k-means-clustering">What is K-means clustering?<a class="anchor" aria-label="anchor" href="#what-is-k-means-clustering"></a>
</h2>
<hr class="half-width">
<p><strong>K-means clustering</strong> groups data points into a
user-defined number of distinct, non-overlapping clusters. To create
clusters of ‘similar’ data points, K-means clustering creates clusters
that minimise the within-cluster variation and thus the amount that data
points within a cluster differ from each other. The distance between
data points within a cluster is used as a measure of within-cluster
variation.</p>
<p>To carry out K-means clustering, we first pick <span class="math inline">\(k\)</span> initial points as centres or
“centroids” of our clusters. There are a few ways to choose these
initial “centroids” and this is discussed below. Once we have picked
intitial points, we then follow these two steps until appropriate
clusters have been formed:</p>
<ol style="list-style-type: decimal">
<li>Assign each data point to the cluster with the closest centroid</li>
<li>Update centroid positions as the average of the points in that
cluster.</li>
</ol>
<p>We can see this process in action in this animation:</p>
<figure style="text-align: center"><img src="../fig/kmeans.gif" alt="An animated scatter plot of data y versus x. The animation starts by identifying 3 initial points, delineated by different colours. The animations then colour codes all points by an associated cluster colour, delineating three distinct and non-overlapping clusters in the space of the scatter plot." class="figure mx-auto d-block"><figcaption>
Animation showing the iterative process of K-means clustering on data y
versus x.
</figcaption></figure><p>While K-means has some advantages over other clustering methods (easy
to implement and to understand), it does have some disadvantages,
particularly difficulties in identifying initial clusters which
observations belong to and the need for the user to specify the number
of clusters that the data should be partitioned into.</p>
<div id="initialisation" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="initialisation" class="callout-inner">
<h3 class="callout-title">Initialisation</h3>
<div class="callout-content">
<p>The algorithm used in K-means clustering finds a <em>local</em>
rather than a <em>global</em> optimum, so that results of clustering are
dependent on the initial cluster that each observation is randomly
assigned to. This initial configuration can have a significant effect on
the final configuration of the clusters, so dealing with this limitation
is an important part of K-means clustering. Some strategies to deal with
this problem are:</p>
<ul>
<li>Choose <span class="math inline">\(K\)</span> points at random from
the data as the cluster centroids.</li>
<li>Randomly split the data into <span class="math inline">\(K\)</span>
groups, and then average these groups.</li>
<li>Use the K-means++ algorithm to choose initial values.</li>
</ul>
<p>These each have advantages and disadvantages. In general, it’s good
to be aware of this limitation of K-means clustering and that this
limitation can be addressed by choosing a good initialisation method,
initialising clusters manually, or running the algorithm from multiple
different starting points.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="believing-in-clusters">Believing in clusters<a class="anchor" aria-label="anchor" href="#believing-in-clusters"></a>
</h2>
<hr class="half-width">
<p>When using clustering, it’s important to realise that data may seem
to group together even when these groups are created randomly. It’s
especially important to remember this when making plots that add extra
visual aids to distinguish clusters.</p>
<p>For example, if we cluster data from a single 2D normal distribution
and draw ellipses around the points, these clusters suddenly become
almost visually convincing. This is a somewhat extreme example, since
there is genuinely no heterogeneity in the data, but it does reflect
what can happen if you allow yourself to read too much into faint
signals.</p>
<p>Let’s explore this further using an example. We create two columns of
data (‘x’ and ‘y’) and partition these data into three groups (‘a’, ‘b’,
‘c’) according to data values. We then plot these data and their
allocated clusters and put ellipses around the clusters using the
<code>stat_ellipse()</code> function in
<strong><code>ggplot</code></strong>.</p>
<figure style="text-align: center"><img src="../fig/06-k-means-rendered-fake-cluster-1.png" alt="A scatter plot of random data y versus x. The points are horizontally partitioned at 2 random groups, forming three colour coded clusters. Circles are drawn around each cluster. The data shown appears to have no clusters but the colours and circles give the appearance of clusters artificially." class="figure mx-auto d-block"><figcaption>
Example of artificial clusters fitted to data points.
</figcaption></figure><p>The randomly created data used here appear to form three clusters
when we plot the data. Putting ellipses around the clusters can further
convince us that the clusters are ‘real’. But how do we tell if clusters
identified visually are ‘real’?</p>
<div id="initialisation-1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="initialisation-1" class="callout-inner">
<h3 class="callout-title">Initialisation</h3>
<div class="callout-content">
<p>The algorithm used in K-means clustering finds a <em>local</em>
rather than a <em>global</em> optimum, so that results of clustering are
dependent on the initial cluster that each observation is randomly
assigned to. This initial configuration can have a significant effect on
the final configuration of the clusters, so dealing with this limitation
is an important part of K-means clustering. Some strategies to deal with
this problem are:</p>
<ul>
<li>Choose <span class="math inline">\(k\)</span> points at random from
the data as the cluster centroids.</li>
<li>Randomly split the data into <span class="math inline">\(k\)</span>
groups, and then average these groups.</li>
<li>Use the K-means++ algorithm to choose initial values.</li>
</ul>
<p>These each have advantages and disadvantages. In general, it’s good
to be aware of this limitation of K-means clustering and that this
limitation can be addressed by choosing a good initialisation method,
initialising clusters manually, or running the algorithm from multiple
different starting points.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="k-means-clustering-applied-to-the-single-cell-rna-sequencing-data">K-means clustering applied to the single-cell RNA sequencing
data<a class="anchor" aria-label="anchor" href="#k-means-clustering-applied-to-the-single-cell-rna-sequencing-data"></a>
</h2>
<hr class="half-width">
<p>Let’s carry out K-means clustering in <code>R</code> using some real
high-dimensional data. We’re going to work with single-cell RNA
sequencing data, <a href="https://carpentries-incubator.github.io/high-dimensional-stats-r/data/index.html" class="external-link"><em>scRNAseq</em></a>,
in these clustering challenges, which is often <em>very</em>
high-dimensional. Commonly, experiments profile the expression level of
10,000+ genes in thousands of cells. Even after filtering the data to
remove low quality observations, the dataset we’re using in this episode
contains measurements for over 9,000 genes in over 3,000 cells.</p>
<p>One way to get a handle on a dataset of this size is to use something
we covered earlier in the course - dimensionality reduction.
Dimensionality reduction allows us to visualise this incredibly complex
data in a small number of dimensions. In this case, we’ll be using
principal component analysis (PCA) to compress the data by identifying
the major axes of variation in the data, before running our clustering
algorithms on this lower-dimensional data to explore the similarity of
features.</p>
<p>The <strong><code>scater</code></strong> package has some easy-to-use
tools to calculate a PCA for <code>SummarizedExperiment</code> objects.
Let’s load the <em>scRNAseq</em> data and calculate some principal
components.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"SingleCellExperiment"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: SummarizedExperiment</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: MatrixGenerics</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: matrixStats</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'MatrixGenerics'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:matrixStats':

    colAlls, colAnyNAs, colAnys, colAvgsPerRowSet, colCollapse,
    colCounts, colCummaxs, colCummins, colCumprods, colCumsums,
    colDiffs, colIQRDiffs, colIQRs, colLogSumExps, colMadDiffs,
    colMads, colMaxs, colMeans2, colMedians, colMins, colOrderStats,
    colProds, colQuantiles, colRanges, colRanks, colSdDiffs, colSds,
    colSums2, colTabulates, colVarDiffs, colVars, colWeightedMads,
    colWeightedMeans, colWeightedMedians, colWeightedSds,
    colWeightedVars, rowAlls, rowAnyNAs, rowAnys, rowAvgsPerColSet,
    rowCollapse, rowCounts, rowCummaxs, rowCummins, rowCumprods,
    rowCumsums, rowDiffs, rowIQRDiffs, rowIQRs, rowLogSumExps,
    rowMadDiffs, rowMads, rowMaxs, rowMeans2, rowMedians, rowMins,
    rowOrderStats, rowProds, rowQuantiles, rowRanges, rowRanks,
    rowSdDiffs, rowSds, rowSums2, rowTabulates, rowVarDiffs, rowVars,
    rowWeightedMads, rowWeightedMeans, rowWeightedMedians,
    rowWeightedSds, rowWeightedVars</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: GenomicRanges</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: stats4</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: BiocGenerics</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'BiocGenerics'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:stats':

    IQR, mad, sd, var, xtabs</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:base':

    anyDuplicated, aperm, append, as.data.frame, basename, cbind,
    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,
    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,
    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,
    Position, rank, rbind, Reduce, rownames, sapply, saveRDS, setdiff,
    table, tapply, union, unique, unsplit, which.max, which.min</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: S4Vectors</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'S4Vectors'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following object is masked from 'package:utils':

    findMatches</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:base':

    expand.grid, I, unname</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: IRanges</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: GenomeInfoDb</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: Biobase</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Welcome to Bioconductor

    Vignettes contain introductory material; view with
    'browseVignettes()'. To cite Bioconductor, see
    'citation("Biobase")', and for packages 'citation("pkgname")'.</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'Biobase'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following object is masked from 'package:MatrixGenerics':

    rowMedians</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:matrixStats':

    anyMissing, rowMedians</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: replacing previous import 'S4Arrays::makeNindexFromArrayViewport' by
'DelayedArray::makeNindexFromArrayViewport' when loading 'SummarizedExperiment'</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"scater"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: scuttle</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">scrnaseq</span> <span class="op">&lt;-</span> <span class="fu">readRDS</span><span class="op">(</span><span class="st">"data/scrnaseq.rds"</span><span class="op">)</span></span>
<span><span class="va">scrnaseq</span> <span class="op">&lt;-</span> <span class="fu">runPCA</span><span class="op">(</span><span class="va">scrnaseq</span>, ncomponents <span class="op">=</span> <span class="fl">15</span><span class="op">)</span></span>
<span><span class="va">pcs</span> <span class="op">&lt;-</span> <span class="fu">reducedDim</span><span class="op">(</span><span class="va">scrnaseq</span><span class="op">)</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span></span></code></pre>
</div>
<p>The first two principal components capture almost 50% of the
variation within the data. For now, we’ll work with just these two
principal components, since we can visualise those easily, and they’re a
quantitative representation of the underlying data, representing the two
largest axes of variation.</p>
<p>We can now run K-means clustering on the first and second principal
components of the <em>scRNAseq</em> data using the <code>kmeans()</code>
function.</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">set.seed</span><span class="op">(</span><span class="fl">42</span><span class="op">)</span></span>
<span><span class="va">cluster</span> <span class="op">&lt;-</span> <span class="fu">kmeans</span><span class="op">(</span><span class="va">pcs</span>, centers <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span><span class="va">scrnaseq</span><span class="op">$</span><span class="va">kmeans</span> <span class="op">&lt;-</span> <span class="fu">as.character</span><span class="op">(</span><span class="va">cluster</span><span class="op">$</span><span class="va">cluster</span><span class="op">)</span></span>
<span><span class="fu">plotReducedDim</span><span class="op">(</span><span class="va">scrnaseq</span>, <span class="st">"PCA"</span>, colour_by <span class="op">=</span> <span class="st">"kmeans"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/06-k-means-rendered-kmeans-1.png" alt="A scatter plot of principal component 2 versus principal component 1 of the scrnaseq data. Each point is one of four colours, representing cluster membership. Points of the same colour appear in the same areas of the plot, showing four distinct clusters in the data." class="figure mx-auto d-block"><figcaption>
Scatter plot of principal component 2 versus principal component 1 with
points colour coded according to the cluster to which they belong.
</figcaption></figure><p>We can see that this produces a sensible-looking partition of the
data. However, is it totally clear whether there might be more or fewer
clusters here?</p>
<div id="challenge-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-1" class="callout-inner">
<h3 class="callout-title">Challenge 1</h3>
<div class="callout-content">
<p>Perform clustering to group the data into <span class="math inline">\(k=5\)</span> clusters, and plot it using
<code>plotReducedDim()</code>. Save this with a variable name that’s
different to what we just used, because we’ll use this again later.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">set.seed</span><span class="op">(</span><span class="fl">42</span><span class="op">)</span></span>
<span><span class="va">cluster5</span> <span class="op">&lt;-</span> <span class="fu">kmeans</span><span class="op">(</span><span class="va">pcs</span>, centers <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">scrnaseq</span><span class="op">$</span><span class="va">kmeans5</span> <span class="op">&lt;-</span> <span class="fu">as.character</span><span class="op">(</span><span class="va">cluster5</span><span class="op">$</span><span class="va">cluster</span><span class="op">)</span></span>
<span><span class="fu">plotReducedDim</span><span class="op">(</span><span class="va">scrnaseq</span>, <span class="st">"PCA"</span>, colour_by <span class="op">=</span> <span class="st">"kmeans5"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/06-k-means-rendered-kmeans-ex-1.png" alt="A scatter plot of principal component 1 versus principal component 2 of the scrnaseq data. Each point is one of five colours, representing cluster membership. Points of the same colour appear in the same areas of the plot, showing five distinct clusters in the data." class="figure mx-auto d-block"><figcaption>
Scatter plot of principal component 2 against principal component 1 in
the scRNAseq data, coloured by clusters produced by k-means clustering.
</figcaption></figure>
</div>
</div>
</div>
</div>
<div id="k-medoids-pam" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="k-medoids-pam" class="callout-inner">
<h3 class="callout-title">K-medoids (PAM)</h3>
<div class="callout-content">
<p>One problem with K-means is that using the mean to define cluster
centroids means that clusters can be very sensitive to outlying
observations.</p>
<p>K-medoids, also known as “partitioning around medoids (PAM)” is
similar to K-means, but uses the median rather than the mean as the
method for defining cluster centroids. Using the median rather than the
mean reduces sensitivity of clusters to outliers in the data.</p>
<p>K-medioids has had popular application in genomics, for example the
well-known PAM50 gene set in breast cancer, which has seen some
prognostic applications.</p>
<p>The following example shows how cluster centroids differ when created
using medians rather than means.</p>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">rnorm</span><span class="op">(</span><span class="fl">20</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu">rnorm</span><span class="op">(</span><span class="fl">20</span><span class="op">)</span></span>
<span><span class="va">x</span><span class="op">[</span><span class="fl">10</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="fl">10</span><span class="op">]</span> <span class="op">+</span> <span class="fl">10</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, pch <span class="op">=</span> <span class="fl">16</span><span class="op">)</span></span>
<span><span class="fu">points</span><span class="op">(</span><span class="fu">mean</span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="fu">mean</span><span class="op">(</span><span class="va">y</span><span class="op">)</span>, pch <span class="op">=</span> <span class="fl">16</span>, col <span class="op">=</span> <span class="st">"firebrick"</span><span class="op">)</span></span>
<span><span class="fu">points</span><span class="op">(</span><span class="fu">median</span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="fu">median</span><span class="op">(</span><span class="va">y</span><span class="op">)</span>, pch <span class="op">=</span> <span class="fl">16</span>, col <span class="op">=</span> <span class="st">"dodgerblue"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/06-k-means-rendered-unnamed-chunk-1-1.png" alt="Scatter plot of random data y versus x. There are many black points on the plot representing the data. Two additional points are shown: the (mean(x), mean(y)) co-ordinate point in red and the (median(x), median(y)) co-ordinate point in blue. The median co-ordinate point in blue has a lower x value and is shown to the left of the red mean co-ordinate point." class="figure mx-auto d-block"><figcaption>
Scatter plot of random data y versus x with the (mean(x), mean(y))
co-ordinate point shown in red and the (median(x), median(y))
co-ordinate point shown in blue.
</figcaption></figure><p>PAM can be carried out using <code>pam()</code> from the
<strong><code>cluster</code></strong> package.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="cluster-separation">Cluster separation<a class="anchor" aria-label="anchor" href="#cluster-separation"></a>
</h2>
<hr class="half-width">
<p>When performing clustering, it is important for us to be able to
measure how well our clusters are separated. One measure to test this is
silhouette width, which measures how similar a data point is to points
in the same cluster compared to other clusters.</p>
<p>The silhouette width is computed for every observation and can range
from -1 to 1. A high silhouette width means an observation is closer to
other observations within the same cluster. For each cluster, the
silhouette widths can then be averaged or an overall average can be
taken.</p>
<div id="more-detail-on-silhouette-widths" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="more-detail-on-silhouette-widths" class="callout-inner">
<h3 class="callout-title">More detail on silhouette widths</h3>
<div class="callout-content">
<p>In more detail, each observation’s silhouette width is computed as
follows:</p>
<ol style="list-style-type: decimal">
<li>Compute the average distance between the focal observation and all
other observations in the same cluster.</li>
<li>For each of the other clusters, compute the average distance between
focal observation and all observations in the other cluster. Keep the
smallest of these average distances.</li>
<li>Subtract (1.)-(2.) then divivde by whichever is smaller (1.) or
(2).</li>
</ol>
</div>
</div>
</div>
<p>Ideally, we would have only large positive silhouette widths,
indicating that each data point is much more similar to points within
its cluster than it is to the points in any other cluster. However, this
is rarely the case. Often, clusters are very fuzzy, and even if we are
relatively sure about the existence of discrete groupings in the data,
observations on the boundaries can be difficult to confidently place in
either cluster.</p>
<p>Here we use the <code>silhouette()</code> function from the
<strong><code>cluster</code></strong> package to calculate the
silhouette width of our K-means clustering using a distance matrix of
distances between points in the clusters.</p>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"cluster"</span><span class="op">)</span></span>
<span><span class="va">dist_mat</span> <span class="op">&lt;-</span> <span class="fu">dist</span><span class="op">(</span><span class="va">pcs</span><span class="op">)</span></span>
<span><span class="va">sil</span> <span class="op">&lt;-</span> <span class="fu">silhouette</span><span class="op">(</span><span class="va">cluster</span><span class="op">$</span><span class="va">cluster</span>, dist <span class="op">=</span> <span class="va">dist_mat</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">sil</span>, border <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/06-k-means-rendered-silhouette-1.png" alt="Plot with horizontal axis silhoutte width. The plot shows the silhouette width for each point in the data set according to cluster. Cluster 4 contains over half of the points in the data set and largely consists of points with a large silhouette list, leading to a bar that extends to the right side of the graph. The other clusters contain many fewer points and have slightly lower silhouette widths. The bars therefore reach further to the left than cluster 4." class="figure mx-auto d-block"><figcaption>
Silhouette plot for each point according to cluster.
</figcaption></figure><p>Let’s plot the silhouette score on the original dimensions used to
cluster the data. Here, we’re mapping cluster membership to point shape,
and silhouette width to colour.</p>
<div class="codewrapper sourceCode" id="cb32">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pc</span> <span class="op">&lt;-</span> <span class="fu">as.data.frame</span><span class="op">(</span><span class="va">pcs</span><span class="op">)</span></span>
<span><span class="fu">colnames</span><span class="op">(</span><span class="va">pc</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu">c</span><span class="op">(</span><span class="st">"x"</span>, <span class="st">"y"</span><span class="op">)</span></span>
<span><span class="va">pc</span><span class="op">$</span><span class="va">sil</span> <span class="op">&lt;-</span> <span class="va">sil</span><span class="op">[</span>, <span class="st">"sil_width"</span><span class="op">]</span></span>
<span><span class="va">pc</span><span class="op">$</span><span class="va">clust</span> <span class="op">&lt;-</span> <span class="fu">factor</span><span class="op">(</span><span class="va">cluster</span><span class="op">$</span><span class="va">cluster</span><span class="op">)</span></span>
<span><span class="fu">mean</span><span class="op">(</span><span class="va">sil</span><span class="op">[</span>, <span class="st">"sil_width"</span><span class="op">]</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 0.7065662</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb34">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">ggplot</span><span class="op">(</span><span class="va">pc</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">aes</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, shape <span class="op">=</span> <span class="va">clust</span>, colour <span class="op">=</span> <span class="va">sil</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">scale_colour_gradient2</span><span class="op">(</span></span>
<span>        low <span class="op">=</span> <span class="st">"dodgerblue"</span>, high <span class="op">=</span> <span class="st">"firebrick"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">scale_shape_manual</span><span class="op">(</span></span>
<span>        values <span class="op">=</span> <span class="fu">setNames</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">4</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/06-k-means-rendered-plot-silhouette-1.png" alt="A scatter plot of the random y versus x data. Cluster membership is delineated using different point characters. Data points in the same cluster have the same point character. Each point is coloured by its silhouette width: solid red delineating a silhouette width of 1 and white delineating a silhouette width of 0. Colours in between delineate the intermediate colours. Many points are red and fade to white at the boundaries of each cluster. " class="figure mx-auto d-block"><figcaption>
Scatter plot of y versus x coloured according to silhouette width and
point characters grouped according to cluster membership.
</figcaption></figure><p>This plot shows that silhouette values for individual observations
tends to be very high in the centre of clusters, but becomes quite low
towards the edges. This makes sense, as points that are “between” two
clusters may be more similar to points in another cluster than they are
to the points in the cluster one they belong to.</p>
<div id="challenge-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-2" class="callout-inner">
<h3 class="callout-title">Challenge 2</h3>
<div class="callout-content">
<p>Calculate the silhouette width for the k of 5 clustering we did
earlier. Are 5 clusters appropriate? Why/why not?</p>
<p>Can you identify where the differences lie?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb35">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sil5</span> <span class="op">&lt;-</span> <span class="fu">silhouette</span><span class="op">(</span><span class="va">cluster5</span><span class="op">$</span><span class="va">cluster</span>, dist <span class="op">=</span> <span class="va">dist_mat</span><span class="op">)</span></span>
<span><span class="va">scrnaseq</span><span class="op">$</span><span class="va">kmeans5</span> <span class="op">&lt;-</span> <span class="fu">as.character</span><span class="op">(</span><span class="va">cluster5</span><span class="op">$</span><span class="va">cluster</span><span class="op">)</span></span>
<span><span class="fu">plotReducedDim</span><span class="op">(</span><span class="va">scrnaseq</span>, <span class="st">"PCA"</span>, colour_by <span class="op">=</span> <span class="st">"kmeans5"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/06-k-means-rendered-silhouette-ex-1.png" alt="A scatter plot of principal component 1 versus principal component 2 of the scrnaseq data. Each point is one of five colours, representing cluster membership. Points of the same colour appear in the same areas of the plot, showing five distinct clusters in the data." class="figure mx-auto d-block"><figcaption>
Scatter plot of principal component 2 against principal component 1 in
the scRNAseq data, coloured by clusters produced by k-means clustering.
</figcaption></figure><div class="codewrapper sourceCode" id="cb36">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">mean</span><span class="op">(</span><span class="va">sil5</span><span class="op">[</span>, <span class="st">"sil_width"</span><span class="op">]</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 0.5849979</code></pre>
</div>
<p>The average silhouette width is lower when k=5.</p>
<div class="codewrapper sourceCode" id="cb38">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">plot</span><span class="op">(</span><span class="va">sil5</span>, border <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/06-k-means-rendered-unnamed-chunk-2-1.png" alt="Plot with horizontal axis silhoutte width. The plot shows the silhouette width for each point in the data set according to cluster. Cluster 4 contains almost half of the points in the data set and largely consists of points with a large silhouette list, leading to a bar that extends to the right side of the graph. The other clusters contain many fewer points and have similar silhouette widths. The bars for cluster 5 are much smaller, with a small number extending to the left of the origin, indicating negative silhouette widths." class="figure mx-auto d-block"><figcaption>
Silhouette plot for each point according to cluster.
</figcaption></figure><p>This seems to be because some observations in clusters 3 and 5 seem
to be more similar to other clusters than the one they have been
assigned to. This may indicate that k is too high.</p>
</div>
</div>
</div>
</div>
<div id="gap-statistic" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="gap-statistic" class="callout-inner">
<h3 class="callout-title">Gap statistic</h3>
<div class="callout-content">
<p>Another measure of how good our clustering is is the “gap statistic”.
This compares the observed squared distance between observations in a
cluster and the centre of the cluster to an “expected” squared
distances. The expected distances are calculated by randomly
distributing cells within the range of the original data. Larger values
represent lower squared distances within clusters, and thus better
clustering. We can see how this is calculated in the following
example.</p>
<div class="codewrapper sourceCode" id="cb39">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"cluster"</span><span class="op">)</span></span>
<span><span class="va">gaps</span> <span class="op">&lt;-</span> <span class="fu">clusGap</span><span class="op">(</span><span class="va">pcs</span>, <span class="va">kmeans</span>, K.max <span class="op">=</span> <span class="fl">20</span>, iter.max <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></span>
<span><span class="va">best_k</span> <span class="op">&lt;-</span> <span class="fu">maxSE</span><span class="op">(</span><span class="va">gaps</span><span class="op">$</span><span class="va">Tab</span><span class="op">[</span>, <span class="st">"gap"</span><span class="op">]</span>, <span class="va">gaps</span><span class="op">$</span><span class="va">Tab</span><span class="op">[</span>, <span class="st">"SE.sim"</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">best_k</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">gaps</span><span class="op">$</span><span class="va">Tab</span><span class="op">[</span>,<span class="st">"gap"</span><span class="op">]</span>, xlab <span class="op">=</span> <span class="st">"Number of clusters"</span>, ylab <span class="op">=</span> <span class="st">"Gap statistic"</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>v <span class="op">=</span> <span class="va">best_k</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code></pre>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="cluster-robustness-bootstrapping">Cluster robustness: bootstrapping<a class="anchor" aria-label="anchor" href="#cluster-robustness-bootstrapping"></a>
</h2>
<hr class="half-width">
<p>When we cluster data, we want to be sure that the clusters we
identify are not a result of the exact properties of the input data.
That is, we want to ensure that the clusters identified do not change
substantially if the observed data change slightly. This makes it more
likely that the clusters can be reproduced.</p>
<p>To assess this, we can <em>bootstrap</em>. We first resample the data
with replacement to reproduce a ‘new’ data set. We can then calculate
new clusters for this data set and compare these to those on the
original data set, thus helping us to see how the clusters may change
for small changes in the data. This is maybe easier to see with an
example. First, we define some data:</p>
<div class="codewrapper sourceCode" id="cb40">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">5</span></span></code></pre>
</div>
<p>Then, we can take a sample from this data without replacement:</p>
<div class="codewrapper sourceCode" id="cb41">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">sample</span><span class="op">(</span><span class="va">data</span>, <span class="fl">5</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 4 1 3 5 2</code></pre>
</div>
<p>This sample is a subset of the original data, and points are only
present once. This is the case every time even if we do it many
times:</p>
<div class="codewrapper sourceCode" id="cb43">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Each column is a sample</span></span>
<span><span class="fu">replicate</span><span class="op">(</span><span class="fl">10</span>, <span class="fu">sample</span><span class="op">(</span><span class="va">data</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    5    2    5    2    3    1    3    5    5     3
[2,]    4    5    4    5    4    4    1    3    1     2
[3,]    2    1    1    3    2    5    2    2    3     4
[4,]    1    4    2    1    5    3    5    1    2     5
[5,]    3    3    3    4    1    2    4    4    4     1</code></pre>
</div>
<p>However, if we sample <em>with replacement</em>, then sometimes
individual data points are present more than once.</p>
<div class="codewrapper sourceCode" id="cb45">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">replicate</span><span class="op">(</span><span class="fl">10</span>, <span class="fu">sample</span><span class="op">(</span><span class="va">data</span>, <span class="fl">5</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    3    1    2    2    1    3    3    2    4     2
[2,]    1    3    2    4    2    5    2    1    2     5
[3,]    5    5    4    4    2    2    1    1    1     3
[4,]    1    1    4    2    1    4    4    5    5     4
[5,]    3    1    2    1    4    2    5    3    3     2</code></pre>
</div>
</section><section><h2 class="section-heading" id="bootstrapping">Bootstrapping<a class="anchor" aria-label="anchor" href="#bootstrapping"></a>
</h2>
<hr class="half-width">
<p>Bootstrapping is a powerful and common statistical technique.</p>
<p>We would like to know about the sampling distribution of a statistic,
but we don’t have any knowledge of its behaviour under the null
hypothesis.</p>
<p>For example, we might want to understand the uncertainty around an
estimate of the mean of our data. To do this, we could resample the data
with replacement and calculate the mean of each average.</p>
<div class="codewrapper sourceCode" id="cb47">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">boots</span> <span class="op">&lt;-</span> <span class="fu">replicate</span><span class="op">(</span><span class="fl">1000</span>, <span class="fu">mean</span><span class="op">(</span><span class="fu">sample</span><span class="op">(</span><span class="va">data</span>, <span class="fl">5</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">hist</span><span class="op">(</span><span class="va">boots</span>,</span>
<span>    breaks <span class="op">=</span> <span class="st">"FD"</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"1,000 bootstrap samples"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Mean of sample"</span></span>
<span> <span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/06-k-means-rendered-boots-1.png" alt="A histogram of the mean of each bootstrapped sample. The histogram appears roughly symmetric around 2.8 on the x axis." class="figure mx-auto d-block"><figcaption>
Histogram of mean of bootstapped samples.
</figcaption></figure><p>In this case, the example is simple, but it’s possible to devise more
complex statistical tests using this kind of approach.</p>
<p>The bootstrap, along with permutation testing, can be a very flexible
and general solution to many statistical problems.</p>
<p>In applying the bootstrap to clustering, we want to see two
things:</p>
<ol style="list-style-type: decimal">
<li>Will observations within a cluster consistently cluster together in
different bootstrap replicates?</li>
<li>Will observations frequently swap between clusters?</li>
</ol>
<p>In the plot below, the diagonal of the plot shows how often the
clusters are reproduced in boostrap replicates. High scores on the
diagonal mean that the clusters are consistently reproduced in each
boostrap replicate. Similarly, the off-diagonal elements represent how
often observations swap between clusters in bootstrap replicates. High
scores indicate that observations rarely swap between clusters.</p>
<div class="codewrapper sourceCode" id="cb48">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"pheatmap"</span><span class="op">)</span></span>
<span><span class="kw">library</span><span class="op">(</span><span class="st">"bluster"</span><span class="op">)</span></span>
<span><span class="kw">library</span><span class="op">(</span><span class="st">"viridis"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: viridisLite</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb50">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">km_fun</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu">kmeans</span><span class="op">(</span><span class="va">x</span>, centers <span class="op">=</span> <span class="fl">4</span><span class="op">)</span><span class="op">$</span><span class="va">cluster</span></span>
<span><span class="op">}</span></span>
<span><span class="va">ratios</span> <span class="op">&lt;-</span> <span class="fu">bootstrapStability</span><span class="op">(</span><span class="va">pcs</span>, FUN <span class="op">=</span> <span class="va">km_fun</span>, clusters <span class="op">=</span> <span class="va">cluster</span><span class="op">$</span><span class="va">cluster</span><span class="op">)</span></span>
<span><span class="fu">pheatmap</span><span class="op">(</span><span class="va">ratios</span>,</span>
<span>    cluster_rows <span class="op">=</span> <span class="cn">FALSE</span>, cluster_cols <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>    col <span class="op">=</span> <span class="fu">viridis</span><span class="op">(</span><span class="fl">10</span><span class="op">)</span>,</span>
<span>    breaks <span class="op">=</span> <span class="fu">seq</span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, length.out <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/06-k-means-rendered-bs-heatmap-1.png" alt="Grid of 16 squares labelled 1-4 on each of the x and y axes. The diagonal and off-diagonal squares of the grid are coloured in green, indicating the highest scoring value of 1 according to the legend. The lower triangular squares are coloured in grey, indicating NA values since these would be the same as the upper triangular squares." class="figure mx-auto d-block"><figcaption>
Grid of empirical cluster swapping behaviour estimated by the bootstrap
samples.
</figcaption></figure><p>Yellow boxes indicate values slightly greater than 1, which may be
observed. These are “good” (despite missing in the colour bar).</p>
<div id="challenge-3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-3" class="callout-inner">
<h3 class="callout-title">Challenge 3</h3>
<div class="callout-content">
<p>Repeat the bootstrapping process with k=5. Do the results appear
better or worse? Can you identify where the differences occur on the
<code>plotReducedDim()</code>?</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb51">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">km_fun5</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu">kmeans</span><span class="op">(</span><span class="va">x</span>, centers <span class="op">=</span> <span class="fl">5</span><span class="op">)</span><span class="op">$</span><span class="va">cluster</span></span>
<span><span class="op">}</span></span>
<span><span class="fu">set.seed</span><span class="op">(</span><span class="fl">42</span><span class="op">)</span></span>
<span><span class="va">ratios5</span> <span class="op">&lt;-</span> <span class="fu">bootstrapStability</span><span class="op">(</span><span class="va">pcs</span>, FUN <span class="op">=</span> <span class="va">km_fun5</span>, clusters <span class="op">=</span> <span class="va">cluster5</span><span class="op">$</span><span class="va">cluster</span><span class="op">)</span></span>
<span><span class="fu">pheatmap</span><span class="op">(</span><span class="va">ratios5</span>,</span>
<span>    cluster_rows <span class="op">=</span> <span class="cn">FALSE</span>, cluster_cols <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>    col <span class="op">=</span> <span class="fu">viridis</span><span class="op">(</span><span class="fl">10</span><span class="op">)</span>,</span>
<span>    breaks <span class="op">=</span> <span class="fu">seq</span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, length.out <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/06-k-means-rendered-bs-ex-1.png" alt="Grid of 25 squares labelled 1-5 on each of the x and y axes. The diagonal and off-diagonal squares of the grid are coloured in green, indicating the highest scoring value of 1 according to the legend, with the exception of the square corresponding to (4, 5), which is slightly darker green indicating a lower value. The lower triangular squares are coloured in grey, indicating NA values since these would be the same as the upper triangular squares." class="figure mx-auto d-block"><figcaption>
Grid of empirical cluster swapping behaviour estimated by the bootstrap
samples.
</figcaption></figure><p>When k=5, we can see that the values on the diagonal of the matrix
are smaller, indicating that the clusters aren’t exactly reproducible in
the bootstrap samples.</p>
<p>Similarly, the off-diagonal elements are considerably lower for some
elements. This indicates that observations are “swapping” between these
clusters in bootstrap replicates.</p>
</div>
</div>
</div>
</div>
<div id="consensus-clustering" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="consensus-clustering" class="callout-inner">
<h3 class="callout-title">Consensus clustering</h3>
<div class="callout-content">
<p>One useful and generic method of clustering is <em>consensus
clustering</em>. This method can use k-means or other clustering
methods.</p>
<p>The idea behind this is to bootstrap the data repeatedly, and cluster
it each time, perhaps using different numbers of clusters. If a pair of
data points always end up in the same cluster, it’s likely that they
really belong to the same underlying cluster.</p>
<p>This is really computationally demanding but has been shown to
perform very well in some situations. It also allows you to visualise
how cluster membership changes over different values of k.</p>
</div>
</div>
</div>
<div id="speed" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="speed" class="callout-inner">
<h3 class="callout-title">Speed</h3>
<div class="callout-content">
<p>It’s worth noting that a lot of the methods we’ve discussed here are
very computationally demanding. When clustering data, we may have to
compare points to each other many times. This becomes more and more
difficult when we have many observations and many features. This is
especially problematic when we want to do things like bootstrapping that
requires us to cluster the data over and over.</p>
<p>As a result, there are a lot of approximate methods for finding
clusters in the data. For example, the <a href="https://www.bioconductor.org/packages/3.13/bioc/html/mbkmeans.html" class="external-link">mbkmeans</a>
package includes an algorithm for clustering extremely large data. The
idea behind this algorithm is that if the clusters we find are robust,
we don’t need to look at all of the data every time. This is very
helpful because it reduces the amount of data that needs to be held in
memory at once, but also because it minimises the computational
cost.</p>
<p>Similarly, approximate nearest neighbour methods like <a href="https://pypi.org/project/annoy/" class="external-link">Annoy</a> can be used to identify
what the <span class="math inline">\(k\)</span> closest points are in
the data, and this can be used in some clustering methods (for example,
graph-based clustering).</p>
<p>Generally, these methods sacrifice a bit of accuracy for a big gain
in speed.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="further-reading">Further reading<a class="anchor" aria-label="anchor" href="#further-reading"></a>
</h2>
<hr class="half-width">
<ul>
<li>
<a href="https://doi.org/10.1007/978-3-642-29807-3_1" class="external-link">Wu, J. (2012)
Cluster analysis and K-means clustering: An Introduction. In: Advances
in K-means Clustering. Springer Berlin, Heidelberg.</a>.</li>
<li>
<a href="https://web.stanford.edu/class/bios221/book/Chap-Clustering.html" class="external-link">Modern
statistics for modern biology, Susan Holmes and Wolfgang Huber (Chapter
5)</a>.</li>
<li>
<a href="https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1" class="external-link">Understanding
K-means clustering in machine learning, Towards Data Science</a>.</li>
</ul>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>K-means is an intuitive algorithm for clustering data.</li>
<li>K-means has various advantages but can be computationally
intensive.</li>
<li>Apparent clusters in high-dimensional data should always be treated
with some scepticism.</li>
<li>Silhouette width and bootstrapping can be used to assess how well
our clustering algorithm has worked.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-07-hierarchical"><p>Content from <a href="07-hierarchical.html">Hierarchical clustering</a></p>
<hr>
<p>Last updated on 2025-09-02 |

        <a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/edit/main/episodes/07-hierarchical.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 90 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is hierarchical clustering and how does it differ from other
clustering methods?</li>
<li>How do we carry out hierarchical clustering in R?</li>
<li>What distance matrix and linkage methods should we use?</li>
<li>How can we validate identified clusters?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand when to use hierarchical clustering on high-dimensional
data.</li>
<li>Perform hierarchical clustering on high-dimensional data and
evaluate dendrograms.</li>
<li>Understand and explore different distance matrix and linkage
methods.</li>
<li>Use the Dunn index to validate clustering methods.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="why-use-hierarchical-clustering-on-high-dimensional-data">Why use hierarchical clustering on high-dimensional data?<a class="anchor" aria-label="anchor" href="#why-use-hierarchical-clustering-on-high-dimensional-data"></a>
</h2>
<hr class="half-width">
<p>When analysing high-dimensional data in the life sciences, it is
often useful to identify groups of similar data points to understand
more about the relationships within the dataset. General clustering
algorithms group similar data points (or observations) into groups (or
clusters). This results in a set of clusters, where each cluster is
distinct, and the data points within each cluster have similar
characteristics. The clustering algorithm works by iteratively grouping
data points so that different clusters may exist at different stages of
the algorithm’s progression.</p>
<p>Here, we describe <em>hierarchical clustering</em>. Unlike K-means
clustering, hierarchical clustering does not require the number of
clusters <span class="math inline">\(k\)</span> to be specified by the
user before the analysis is carried out.</p>
<p>Hierarchical clustering also provides an attractive
<em>dendrogram</em>, a tree-like diagram showing the degree of
similarity between clusters. The dendrogram is a key feature of
hierarchical clustering. This tree-shaped graph allows the similarity
between data points in a dataset to be visualised and the arrangement of
clusters produced by the analysis to be illustrated. Dendrograms are
created using a distance (or dissimilarity) that quantify how different
are pairs of observations, and a clustering algorithm to fuse groups of
similar data points together.</p>
<p>In this episode we will explore hierarchical clustering for
identifying clusters in high-dimensional data. We will use
<em>agglomerative</em> hierarchical clustering (see box) in this
episode.</p>
<div id="agglomerative-and-divisive-hierarchical-clustering" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="agglomerative-and-divisive-hierarchical-clustering" class="callout-inner">
<h3 class="callout-title">Agglomerative and Divisive hierarchical
clustering</h3>
<div class="callout-content">
<p>There are two main methods of carrying out hierarchical clustering:
agglomerative clustering and divisive clustering. The former is a
‘bottom-up’ approach to clustering whereby the clustering approach
begins with each data point (or observation) being regarded as being in
its own separate cluster. Pairs of data points are merged as we move up
the tree. Divisive clustering is a ‘top-down’ approach in which all data
points start in a single cluster and an algorithm is used to split
groups of data points from this main group.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="the-agglomerative-hierarchical-clustering-algorithm">The agglomerative hierarchical clustering algorithm<a class="anchor" aria-label="anchor" href="#the-agglomerative-hierarchical-clustering-algorithm"></a>
</h2>
<hr class="half-width">
<p>To start with, we measure distance (or dissimilarity) between pairs
of observations. Initially, and at the bottom of the dendrogram, each
observation is considered to be in its own individual cluster. We start
the clustering procedure by fusing the two observations that are most
similar according to a distance matrix. Next, the next-most similar
observations are fused so that the total number of clusters is
<em>number of observations</em> minus 2 (see panel below). Groups of
observations may then be merged into a larger cluster (see next panel
below, green box). This process continues until all the observations are
included in a single cluster.</p>
<figure style="text-align: center"><img src="../fig/hierarchical_clustering_1.png" alt="Scatter plot of observations x2 versus x1. Two clusters of pairs of observations are shown by blue and red boxes, each grouping two observations that are close in their x and y distance." width="500px" class="figure mx-auto d-block"><figcaption>
Example data showing two clusters of observation pairs.
</figcaption></figure><figure style="text-align: center"><img src="../fig/hierarchical_clustering_2.png" alt="Scatter plot of observations x2 versus x1. Three boxes are shown this time. Blue and red boxes contain two observations each. The two boxes contain points that are relatively far apart. A third green box is shown encompassing the blue box and an additional data point." width="500px" class="figure mx-auto d-block"><figcaption>
Example data showing fusing of one observation into larger cluster.
</figcaption></figure></section><section><h2 class="section-heading" id="a-motivating-example">A motivating example<a class="anchor" aria-label="anchor" href="#a-motivating-example"></a>
</h2>
<hr class="half-width">
<p>To motivate this lesson, let’s first look at an example where
hierarchical clustering is really useful, and then we can understand how
to apply it in more detail. To do this, we’ll return to the large <a href="https://carpentries-incubator.github.io/high-dimensional-stats-r/data/index.html" class="external-link"><code>methylation</code></a>
dataset we worked with in the regression lessons. Let’s load and
visually inspect the data:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">methyl</span> <span class="op">&lt;-</span> <span class="fu">readRDS</span><span class="op">(</span><span class="st">"data/methylation.rds"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># transpose this Bioconductor dataset to show features in columns</span></span>
<span><span class="va">methyl_mat</span> <span class="op">&lt;-</span> <span class="fu">t</span><span class="op">(</span><span class="fu">assay</span><span class="op">(</span><span class="va">methyl</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in assay(methyl): could not find function "assay"</code></pre>
</div>
<p>Looking at a heatmap of these data, we may spot some patterns – many
columns appear to have a similar methylation levels across all rows.
However, they are all quite jumbled at the moment, so it’s hard to tell
how many line up exactly.</p>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in pheatmap(methyl_mat, legend_title = "Methylation level", cluster_rows = FALSE, : could not find function "pheatmap"</code></pre>
</div>
<p>Looking at a heatmap of these data, we may spot some patterns –
looking at the vertical stripes, many columns appear to have a similar
methylation levels across all rows. However, they are all quite jumbled
at the moment, so it’s hard to tell how many line up exactly. In
addition, it is challenging to tell how many groups containing similar
methylation levels we may have or what the similarities and differences
are between groups.</p>
<p>We can order these data to make the patterns more clear using
hierarchical clustering. To do this, we can change the arguments we pass
to <code>pheatmap()</code> from the
<strong><code>pheatmap</code></strong> package. <code>pheatmap()</code>
groups features based on dissimilarity (here, Euclidean distance) and
orders rows and columns to show clustering of features and
observations.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">pheatmap</span><span class="op">(</span><span class="va">methyl_mat</span>,</span>
<span>         legend_title <span class="op">=</span> <span class="st">"Methylation level"</span>,</span>
<span>         cluster_rows <span class="op">=</span> <span class="cn">TRUE</span>, </span>
<span>         cluster_cols <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>         show_rownames <span class="op">=</span> <span class="cn">FALSE</span>, </span>
<span>         show_colnames <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>         main <span class="op">=</span> <span class="st">"Methylation Sites vs Individuals"</span>,</span>
<span>         annotation_legend <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in pheatmap(methyl_mat, legend_title = "Methylation level", cluster_rows = TRUE, : could not find function "pheatmap"</code></pre>
</div>
<p>We can see that clustering the features (CpG sites) results in an
overall gradient of high to low methylation levels from left to right.
Maybe more interesting is the fact that the rows (corresponding to
individuals) are now grouped according to methylation patterns. For
example, 12 samples seem to have lower methylation levels for a small
subset of CpG sites in the middle, relative to all the other samples.
It’s not clear without investigating further what the cause of this is –
it could be a batch effect, or a known grouping (e.g., old vs young
samples). However, clustering like this can be a useful part of
exploratory analysis of data to build hypotheses.</p>
</section><section><h2 class="section-heading" id="hierarchical-clustering">Hierarchical clustering<a class="anchor" aria-label="anchor" href="#hierarchical-clustering"></a>
</h2>
<hr class="half-width">
<p>Now, let’s cover the inner workings of hierarchical clustering in
more detail. Hierarchical clustering is a type of clustering that also
allows us to estimate the number of clusters. There are two things to
consider before carrying out clustering:</p>
<ul>
<li>how to define dissimilarity between observations using a distance
matrix, and</li>
<li>how to define dissimilarity between clusters and when to fuse
separate clusters.</li>
</ul></section><section><h2 class="section-heading" id="defining-the-dissimilarity-between-observations-creating-the-distance-matrix">Defining the dissimilarity between observations: creating the
distance matrix<a class="anchor" aria-label="anchor" href="#defining-the-dissimilarity-between-observations-creating-the-distance-matrix"></a>
</h2>
<hr class="half-width">
<p>Agglomerative hierarchical clustering is performed in two steps:
calculating the distance matrix (containing distances between pairs of
observations) and iteratively grouping observations into clusters using
this matrix.</p>
<p>There are different ways to specify a distance matrix for
clustering:</p>
<ul>
<li>Specify distance as a pre-defined option using the
<code>method</code> argument in <code>dist()</code>. Methods include
<code>euclidean</code> (default), <code>maximum</code> and
<code>manhattan</code>.</li>
<li>Create a self-defined function which calculates distance from a
matrix or from two vectors. The function should only contain one
argument.</li>
</ul>
<p>Of pre-defined methods of calculating the distance matrix, Euclidean
is one of the most commonly used. This method calculates the shortest
straight-line distances between pairs of observations.</p>
<p>Another option is to use a correlation matrix as the input matrix to
the clustering algorithm. The type of distance matrix used in
hierarchical clustering can have a big effect on the resulting tree. The
decision of which distance matrix to use before carrying out
hierarchical clustering depends on the type of data and question to be
addressed.</p>
</section><section><h2 class="section-heading" id="defining-the-dissimilarity-between-clusters-linkage-methods">Defining the dissimilarity between clusters: Linkage methods<a class="anchor" aria-label="anchor" href="#defining-the-dissimilarity-between-clusters-linkage-methods"></a>
</h2>
<hr class="half-width">
<p>The second step in performing hierarchical clustering after defining
the distance matrix (or another function defining similarity between
data points) is determining how to fuse different clusters.</p>
<p><em>Linkage</em> is used to define dissimilarity between groups of
observations (or clusters) and is used to create the hierarchical
structure in the dendrogram. Different linkage methods of creating a
dendrogram are discussed below.</p>
<p><code>hclust()</code> supports various linkage methods (e.g
<code>complete</code>, <code>single</code>, <code>ward D</code>,
<code>ward D2</code>, <code>average</code>, <code>median</code>) and
these are also supported within the <code>pheatmap()</code> function.
The method used to perform hierarchical clustering in
<code>pheatmap()</code> can be specified by the argument
<code>clustering_method</code>. Each linkage method uses a slightly
different algorithm to calculate how clusters are fused together and
therefore different clustering decisions are made depending on the
linkage method used.</p>
<p>Complete linkage (the default in <code>hclust()</code>) works by
computing all pairwise dissimilarities between data points in different
clusters. For each pair of two clusters, it sets their dissimilarity to
the maximum dissimilarity value observed between any of these clusters’
constituent points. The two clusters with smallest dissimilarity value
are then fused.</p>
</section><section><h2 class="section-heading" id="computing-a-dendrogram">Computing a dendrogram<a class="anchor" aria-label="anchor" href="#computing-a-dendrogram"></a>
</h2>
<hr class="half-width">
<p>Dendrograms are useful tools that plot the grouping of points and
clusters into bigger clusters. We can create and plot dendrograms in R
using <code>hclust()</code> which takes a distance matrix as input and
creates a tree using hierarchical clustering. Here we create some
example data to carry out hierarchical clustering.</p>
<p>Let’s generate 20 data points in 2D space. Each point is generated to
belong to one of three classes/groups. Suppose we did not know which
class data points belonged to and we want to identify these via cluster
analysis. Let’s first generate and plot our data:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#First, create some example data with two variables x1 and x2</span></span>
<span><span class="fu">set.seed</span><span class="op">(</span><span class="fl">450</span><span class="op">)</span></span>
<span><span class="va">example_data</span> <span class="op">&lt;-</span> <span class="fu">data.frame</span><span class="op">(</span></span>
<span>    x1 <span class="op">=</span> <span class="fu">rnorm</span><span class="op">(</span><span class="fl">20</span>, <span class="fl">8</span>, <span class="fl">4.5</span><span class="op">)</span>,</span>
<span>    x2 <span class="op">=</span> <span class="fu">rnorm</span><span class="op">(</span><span class="fl">20</span>, <span class="fl">6</span>, <span class="fl">3.4</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co">#plot the data and name data points by row numbers</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">x1</span>, <span class="va">example_data</span><span class="op">$</span><span class="va">x2</span>, type <span class="op">=</span> <span class="st">"n"</span><span class="op">)</span></span>
<span><span class="fu">text</span><span class="op">(</span></span>
<span>    <span class="va">example_data</span><span class="op">$</span><span class="va">x1</span>,</span>
<span>    <span class="va">example_data</span><span class="op">$</span><span class="va">x2</span>,</span>
<span>    labels <span class="op">=</span> <span class="fu">rownames</span><span class="op">(</span><span class="va">example_data</span><span class="op">)</span>,</span>
<span>    cex <span class="op">=</span> <span class="fl">0.7</span></span>
<span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-plotexample-1.png" alt="A scatter plot of randomly-generated data x2 versus x1. The points appear fairly randomly scattered, arguably centered towards the bottom of the plot." class="figure mx-auto d-block"><figcaption>
Scatter plot of randomly-generated data x2 versus x1.
</figcaption></figure><div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## calculate distance matrix using euclidean distance</span></span>
<span><span class="va">dist_m</span> <span class="op">&lt;-</span> <span class="fu">dist</span><span class="op">(</span><span class="va">example_data</span>, method <span class="op">=</span> <span class="st">"euclidean"</span><span class="op">)</span></span></code></pre>
</div>
<p>Hierarchical clustering carried out on the data can be used to
produce a dendrogram showing how the data is partitioned into clusters.
But how do we interpret this dendrogram? Let’s explore this using our
example data in a Challenge.</p>
<div id="challenge-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-1" class="callout-inner">
<h3 class="callout-title">Challenge 1</h3>
<div class="callout-content">
<p>Use <code>hclust()</code> to implement hierarchical clustering using
the distance matrix <code>dist_m</code> and the <code>complete</code>
linkage method and plot the results as a dendrogram using
<code>plot()</code>. Why is hierarchical clustering and the resulting
dendrogram useful for performing clustering this case?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">
  Solution:
  </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">clust</span> <span class="op">&lt;-</span> <span class="fu">hclust</span><span class="op">(</span><span class="va">dist_m</span>, method <span class="op">=</span> <span class="st">"complete"</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">clust</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-plotclustex-1.png" alt="A line plot depicting a dendrogram --- a tree structure representing the hierarchical structure of the data. The data broadly fit into three clusters, with one sample (14) being quite dissimilar to all others, and the rest of the data comprising two other clusters (one larger than the other)." class="figure mx-auto d-block"><figcaption>
A dendrogram of the randomly-generated data.
</figcaption></figure><p>Hierarchical clustering is particularly useful (compared to K-means)
when we do not know the number of clusters before we perform clustering.
It is useful in this case since we have assumed we do not already know
what a suitable number of clusters may be.</p>
</div>
</div>
</div>
</div>
<p>A dendrogram, such as the one generated in Challenge 1, shows
similarities/differences in distances between data points. Each vertical
line at the bottom of the dendrogram (‘leaf’) represents one of the 20
data points. These leaves fuse into fewer vertical lines (‘branches’) as
the height increases. Observations that are similar fuse into the same
branches. The height at which any two data points fuse indicates how
different these two points are. Points that fuse at the top of the tree
are very different from each other compared with two points that fuse at
the bottom of the tree, which are quite similar. You can see this by
comparing the position of similar/dissimilar points according to the
scatterplot with their position on the tree.</p>
</section><section><h2 class="section-heading" id="identifying-the-number-of-clusters">Identifying the number of clusters<a class="anchor" aria-label="anchor" href="#identifying-the-number-of-clusters"></a>
</h2>
<hr class="half-width">
<p>To identify the number of clusters, we can make a horizontal cut
through the dendrogram at a user-defined height. The sets of
observations beneath this cut and single points representing clusters
above can be thought of as distinct clusters. Equivalently, we can count
the vertical lines we encounter crossing the horizontal cut and the
number of single points above the cut. For example, a cut at height 10
produces 3 clusters for the dendrogram in Challenge 1, while a cut at
height 4 produces 8 clusters.</p>
</section><section><h2 class="section-heading" id="dendrogram-visualisation">Dendrogram visualisation<a class="anchor" aria-label="anchor" href="#dendrogram-visualisation"></a>
</h2>
<hr class="half-width">
<p>We can first visualise cluster membership by highlight branches in
dendrograms. In this example, we calculate a distance matrix between
samples in the <code>methyl_mat</code> dataset. We then draw boxes round
clusters obtained with <code>cutree()</code>.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## create a distance matrix using euclidean distance</span></span>
<span><span class="va">distmat</span> <span class="op">&lt;-</span> <span class="fu">dist</span><span class="op">(</span><span class="va">methyl_mat</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'methyl_mat' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## hierarchical clustering using complete method</span></span>
<span><span class="va">clust</span> <span class="op">&lt;-</span> <span class="fu">hclust</span><span class="op">(</span><span class="va">distmat</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'distmat' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## plot resulting dendrogram</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">clust</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## draw border around three clusters</span></span>
<span><span class="fu">rect.hclust</span><span class="op">(</span><span class="va">clust</span>, k <span class="op">=</span> <span class="fl">3</span>, border <span class="op">=</span> <span class="fl">2</span><span class="op">:</span><span class="fl">6</span><span class="op">)</span> <span class="co">#border argument specifies the colours</span></span>
<span><span class="co">## draw border around two clusters</span></span>
<span><span class="fu">rect.hclust</span><span class="op">(</span><span class="va">clust</span>, k <span class="op">=</span> <span class="fl">2</span>, border <span class="op">=</span> <span class="fl">2</span><span class="op">:</span><span class="fl">6</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-plot-clust-method-1.png" alt="A dendrogram for the methyl_mat data with boxes overlaid on clusters. There are 5 boxes in total, each indicating separate clusters." class="figure mx-auto d-block"><figcaption>
Dendrogram with boxes around clusters.
</figcaption></figure><p>We can also colour clusters downstream of a specified cut using
<code>color_branches()</code> from the
<strong><code>dendextend</code></strong> package.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## cut tree at height = 4</span></span>
<span><span class="va">cut</span> <span class="op">&lt;-</span> <span class="fu">cutree</span><span class="op">(</span><span class="va">clust</span>, h <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></span>
<span></span>
<span><span class="kw">library</span><span class="op">(</span><span class="st">"dendextend"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
---------------------
Welcome to dendextend version 1.19.1
Type citation('dendextend') for how to cite the package.

Type browseVignettes(package = 'dendextend') for the package vignette.
The github page is: https://github.com/talgalili/dendextend/

Suggestions and bug-reports can be submitted at: https://github.com/talgalili/dendextend/issues
You may ask questions at stackoverflow, use the r and dendextend tags:
	 https://stackoverflow.com/questions/tagged/dendextend

	To suppress this message use:  suppressPackageStartupMessages(library(dendextend))
---------------------</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'dendextend'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following object is masked from 'package:stats':

    cutree</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">avg_dend_obj</span> <span class="op">&lt;-</span> <span class="fu">as.dendrogram</span><span class="op">(</span><span class="va">clust</span><span class="op">)</span>      </span>
<span><span class="co">## colour branches of dendrogram depending on clusters</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="fu">color_branches</span><span class="op">(</span><span class="va">avg_dend_obj</span>, h <span class="op">=</span> <span class="fl">50</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required namespace: colorspace</code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-plot-coloured-branches-1.png" alt="A dendrogram with the different clusters in 4 different colours." class="figure mx-auto d-block"><figcaption>
Dendrogram with coloured branches delineating different clusters.
</figcaption></figure></section><section><h2 class="section-heading" id="numerical-visualisation">Numerical visualisation<a class="anchor" aria-label="anchor" href="#numerical-visualisation"></a>
</h2>
<hr class="half-width">
<p>In addition to visualising clusters directly on the dendrogram, we
can cut the dendrogram to determine number of clusters at different
heights using <code>cutree()</code>. This function cuts a dendrogram
into several groups (or clusters) where the number of desired groups is
controlled by the user, by defining either <code>k</code> (number of
groups) or <code>h</code> (height at which tree is cut). The function
outputs the cluster labels of each data point in order.</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## k is a user defined parameter determining</span></span>
<span><span class="co">## the desired number of clusters at which to cut the treee</span></span>
<span><span class="fu">as.numeric</span><span class="op">(</span><span class="fu">cutree</span><span class="op">(</span><span class="va">clust</span>, k <span class="op">=</span> <span class="fl">9</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code> [1] 1 2 2 3 4 5 6 5 1 7 8 6 5 9 4 4 7 8 5 8</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## h is a user defined parameter determining</span></span>
<span><span class="co">## the numeric height at which to cut the tree</span></span>
<span><span class="fu">as.numeric</span><span class="op">(</span><span class="fu">cutree</span><span class="op">(</span><span class="va">clust</span>, h <span class="op">=</span> <span class="fl">36</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code> [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## both give same results </span></span>
<span></span>
<span><span class="va">four_cut</span> <span class="op">&lt;-</span> <span class="fu">cutree</span><span class="op">(</span><span class="va">clust</span>, h <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## we can produce the cluster each observation belongs to</span></span>
<span><span class="co">## using the mutate and count functions</span></span>
<span><span class="kw">library</span><span class="op">(</span><span class="va">dplyr</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'dplyr'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:stats':

    filter, lag</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:base':

    intersect, setdiff, setequal, union</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">example_cl</span> <span class="op">&lt;-</span> <span class="fu">mutate</span><span class="op">(</span><span class="fu">data.frame</span><span class="op">(</span><span class="va">methyl_mat</span><span class="op">)</span>, cluster <span class="op">=</span> <span class="va">four_cut</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'methyl_mat' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">count</span><span class="op">(</span><span class="va">example_cl</span>, <span class="va">cluster</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'example_cl' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb32">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#plot cluster each point belongs to on original scatterplot</span></span>
<span><span class="kw">library</span><span class="op">(</span><span class="va">ggplot2</span><span class="op">)</span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="va">example_cl</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">cg01644850</span>, y <span class="op">=</span> <span class="va">cg01656216</span>, color <span class="op">=</span> <span class="fu">factor</span><span class="op">(</span><span class="va">cluster</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'example_cl' not found</code></pre>
</div>
<p>Note that this cut produces 1 clusters (zero before the cut and
another four downstream of the cut).</p>
<div id="challenge-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-2" class="callout-inner">
<h3 class="callout-title">Challenge 2:</h3>
<div class="callout-content">
<p>Identify the value of <code>k</code> in <code>cutree()</code> that
gives the same output as <code>h = 36</code></p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">
  Solution:
  </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb34">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">plot</span><span class="op">(</span><span class="va">clust</span><span class="op">)</span></span>
<span><span class="co">## create horizontal line at height = 45</span></span>
<span><span class="fu">abline</span><span class="op">(</span>h <span class="op">=</span> <span class="fl">45</span>, lty <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-h-k-ex-plot-1.png" alt="A line plot depicting a dendrogram --- a tree structure representing the hierarchical structure of the data. The data broadly fit into three clusters, with one sample (14) being quite dissimilar to all others, and the rest of the data comprising two other clusters (one larger than the other). A dashed horizontal line at a height of 5 indicates the cut point used to divide the data into clusters." class="figure mx-auto d-block"><figcaption>
A dendrogram of the randomly-generated data.
</figcaption></figure><div class="codewrapper sourceCode" id="cb35">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">cutree</span><span class="op">(</span><span class="va">clust</span>, h <span class="op">=</span> <span class="fl">45</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code> [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb37">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">cutree</span><span class="op">(</span><span class="va">clust</span>, k <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code> [1] 1 2 2 3 2 1 2 1 1 4 4 2 1 5 2 2 4 4 1 4</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb39">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">five_cut</span> <span class="op">&lt;-</span> <span class="fu">cutree</span><span class="op">(</span><span class="va">clust</span>, h <span class="op">=</span> <span class="fl">45</span><span class="op">)</span></span>
<span></span>
<span><span class="kw">library</span><span class="op">(</span><span class="va">dplyr</span><span class="op">)</span></span>
<span><span class="va">example_cl</span> <span class="op">&lt;-</span> <span class="fu">mutate</span><span class="op">(</span><span class="fu">data.frame</span><span class="op">(</span><span class="va">methyl_mat</span><span class="op">)</span>, cluster <span class="op">=</span> <span class="va">five_cut</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'methyl_mat' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb41">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">count</span><span class="op">(</span><span class="va">example_cl</span>, <span class="va">cluster</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'example_cl' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb43">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="va">ggplot2</span><span class="op">)</span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="va">example_cl</span>, <span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">cg01644850</span>, y <span class="op">=</span> <span class="va">cg01656216</span>, color <span class="op">=</span> <span class="fu">factor</span><span class="op">(</span><span class="va">cluster</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'example_cl' not found</code></pre>
</div>
<p>Five clusters (<code>k = 5</code>) gives similar results to
<code>h = 45</code>. You can plot a horizontal line on the dendrogram at
<code>h = 45</code> to help identify corresponding value of
<code>k</code>.</p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="the-effect-of-different-linkage-methods">The effect of different linkage methods<a class="anchor" aria-label="anchor" href="#the-effect-of-different-linkage-methods"></a>
</h2>
<hr class="half-width">
<p>Now let us look into changing the default behaviour of
<code>hclust()</code>. First, we’ll load a synthetic dataset, comprised
of two variables representing two crescent-shaped point clouds:</p>
<div class="codewrapper sourceCode" id="cb45">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cres</span> <span class="op">&lt;-</span> <span class="fu">readRDS</span><span class="op">(</span><span class="st">"data/cres.rds"</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">cres</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-crescents-1.png" alt="A scatter plot of data simulated to form two crescent shapes. The crescents are horizontally orientated with a a rough line of vertical symmetry." class="figure mx-auto d-block"><figcaption>
Scatter plot of data simulated according to two crescent-shaped point
clouds.
</figcaption></figure><p>We might expect that the crescents are resolved into separate
clusters. But if we run hierarchical clustering with the default
arguments, we get this:</p>
<div class="codewrapper sourceCode" id="cb46">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cresClass</span> <span class="op">&lt;-</span> <span class="fu">cutree</span><span class="op">(</span><span class="fu">hclust</span><span class="op">(</span><span class="fu">dist</span><span class="op">(</span><span class="va">cres</span><span class="op">)</span><span class="op">)</span>, k<span class="op">=</span><span class="fl">2</span><span class="op">)</span> <span class="co"># save partition for colouring</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">cres</span>, col<span class="op">=</span><span class="va">cresClass</span><span class="op">)</span> <span class="co"># colour scatterplot by partition</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-cresClustDefault-1.png" alt="A scatter plot of the crescent-shaped simulated data calculated using Euclidean distance. The points are coloured in black or red according to their membership to 2 clusters. The points in the tails of each crescent have inherited the colour of the opposite crescent." class="figure mx-auto d-block"><figcaption>
Scatter plot of crescent-shaped simulated data, coloured according to
clusters calculated using Euclidean distance.
</figcaption></figure><div id="challenge-3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-3" class="callout-inner">
<h3 class="callout-title">Challenge 3</h3>
<div class="callout-content">
<p>Carry out hierarchical clustering on the <code>cres</code> data that
we generated above. Try out different linkage methods and use
<code>cutree()</code> to split each resulting dendrogram into two
clusters. Plot the results colouring the dots according to their
inferred cluster identity.</p>
<p>Which method(s) give you the expected clustering outcome?</p>
<p>Hint: Check <code>?hclust</code> to see the possible values of the
argument <code>method</code> (the linkage method used).</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">
  Solution:
  </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb47">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#?hclust</span></span>
<span><span class="co"># "complete", "single", "ward.D", "ward.D2", "average", "mcquitty", "median" or "centroid"</span></span>
<span><span class="va">cresClassSingle</span> <span class="op">&lt;-</span> <span class="fu">cutree</span><span class="op">(</span><span class="fu">hclust</span><span class="op">(</span><span class="fu">dist</span><span class="op">(</span><span class="va">cres</span><span class="op">)</span>,method <span class="op">=</span> <span class="st">"single"</span><span class="op">)</span>, k<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">cres</span>, col<span class="op">=</span><span class="va">cresClassSingle</span>, main<span class="op">=</span><span class="st">"single"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-plot-clust-comp-1.png" alt="A scatter plot of synthetic data, comprising two variables, with points forming two crescent-shaped clusters. Points are coloured based on hierarchical clustering with single linkage, with two clusters, corresponding to the two crescent-shaped clouds." class="figure mx-auto d-block"><figcaption>
A scatter plot of synthetic data coloured by cluster.
</figcaption></figure><div class="codewrapper sourceCode" id="cb48">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cresClassWard.D</span> <span class="op">&lt;-</span> <span class="fu">cutree</span><span class="op">(</span><span class="fu">hclust</span><span class="op">(</span><span class="fu">dist</span><span class="op">(</span><span class="va">cres</span><span class="op">)</span>, method<span class="op">=</span><span class="st">"ward.D"</span><span class="op">)</span>, k<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">cres</span>, col<span class="op">=</span><span class="va">cresClassWard.D</span>, main<span class="op">=</span><span class="st">"ward.D"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-plot-clust-wardD-1.png" alt="A scatter plot of synthetic data, comprising two variables, with points forming two crescent-shaped clusters. Points are coloured based on hierarchical clustering with ward.D linkage, with two clusters, corresponding to the two crescent-shaped clouds." class="figure mx-auto d-block"><figcaption>
A scatter plot of synthetic data coloured by cluster.
</figcaption></figure><div class="codewrapper sourceCode" id="cb49">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cresClassWard.D2</span> <span class="op">&lt;-</span> <span class="fu">cutree</span><span class="op">(</span><span class="fu">hclust</span><span class="op">(</span><span class="fu">dist</span><span class="op">(</span><span class="va">cres</span><span class="op">)</span>, method<span class="op">=</span><span class="st">"ward.D2"</span><span class="op">)</span>, k<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">cres</span>, col<span class="op">=</span><span class="va">cresClassWard.D2</span>, main<span class="op">=</span><span class="st">"ward.D2"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-plot-clust-wardD2-1.png" alt="A scatter plot of synthetic data, comprising two variables, with points forming two crescent-shaped clusters. Points are coloured based on hierarchical clustering with ward.D2 linkage, with two clusters, though these do not correspond to the two crescent-shaped clouds." class="figure mx-auto d-block"><figcaption>
A scatter plot of synthetic data coloured by cluster.
</figcaption></figure><div class="codewrapper sourceCode" id="cb50">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cresClassAverage</span> <span class="op">&lt;-</span> <span class="fu">cutree</span><span class="op">(</span><span class="fu">hclust</span><span class="op">(</span><span class="fu">dist</span><span class="op">(</span><span class="va">cres</span><span class="op">)</span>, method<span class="op">=</span><span class="st">"average"</span><span class="op">)</span>, k<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">cres</span>, col<span class="op">=</span><span class="va">cresClassAverage</span>, main<span class="op">=</span><span class="st">"average"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-plot-clust-average-1.png" alt="A scatter plot of synthetic data, comprising two variables, with points forming two crescent-shaped clusters. Points are coloured based on hierarchical clustering with average linkage, with two clusters, corresponding to the two crescent-shaped clouds." class="figure mx-auto d-block"><figcaption>
A scatter plot of synthetic data coloured by cluster.
</figcaption></figure><div class="codewrapper sourceCode" id="cb51">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cresClassMcquitty</span> <span class="op">&lt;-</span> <span class="fu">cutree</span><span class="op">(</span><span class="fu">hclust</span><span class="op">(</span><span class="fu">dist</span><span class="op">(</span><span class="va">cres</span><span class="op">)</span>, method<span class="op">=</span><span class="st">"mcquitty"</span><span class="op">)</span>, k<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">cres</span>, col<span class="op">=</span><span class="va">cresClassMcquitty</span>, main<span class="op">=</span><span class="st">"mcquitty"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-plot-clust-mcq-1.png" alt="A scatter plot of synthetic data, comprising two variables, with points forming two crescent-shaped clusters. Points are coloured based on hierarchical clustering with mcquitty linkage, with two clusters, though these do not correspond to the two crescent-shaped clouds." class="figure mx-auto d-block"><figcaption>
A scatter plot of synthetic data coloured by cluster.
</figcaption></figure><div class="codewrapper sourceCode" id="cb52">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cresClassMedian</span><span class="op">&lt;-</span> <span class="fu">cutree</span><span class="op">(</span><span class="fu">hclust</span><span class="op">(</span><span class="fu">dist</span><span class="op">(</span><span class="va">cres</span><span class="op">)</span>, method<span class="op">=</span><span class="st">"median"</span><span class="op">)</span>, k<span class="op">=</span><span class="fl">2</span><span class="op">)</span> </span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">cres</span>, col<span class="op">=</span><span class="va">cresClassMedian</span>, main<span class="op">=</span><span class="st">"median"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-plot-clust-median-1.png" alt="A scatter plot of synthetic data, comprising two variables, with points forming two crescent-shaped clusters. Points are coloured based on hierarchical clustering with median linkage, with two clusters, though these do not correspond to the two crescent-shaped clouds." class="figure mx-auto d-block"><figcaption>
A scatter plot of synthetic data coloured by cluster.
</figcaption></figure><div class="codewrapper sourceCode" id="cb53">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cresClassCentroid</span><span class="op">&lt;-</span> <span class="fu">cutree</span><span class="op">(</span><span class="fu">hclust</span><span class="op">(</span><span class="fu">dist</span><span class="op">(</span><span class="va">cres</span><span class="op">)</span>, method<span class="op">=</span><span class="st">"centroid"</span><span class="op">)</span>, k<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">cres</span>, col<span class="op">=</span><span class="va">cresClassCentroid</span>, main<span class="op">=</span><span class="st">"centroid"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-plot-clust-centroid-1.png" alt="A scatter plot of synthetic data, comprising two variables, with points forming two crescent-shaped clusters. Points are coloured based on hierarchical clustering with centroid linkage, with two clusters, though these do not correspond to the two crescent-shaped clouds." class="figure mx-auto d-block"><figcaption>
A scatter plot of synthetic data coloured by cluster.
</figcaption></figure><p>The linkage methods <code>single</code>, <code>ward.D</code>, and
<code>average</code> resolve each crescent as a separate cluster.</p>
</div>
</div>
</div>
</div>
<p>The help page of <code>hclust()</code> gives some intuition on
linkage methods. It describes <code>complete</code> (the default) and
<code>single</code> as opposite ends of a spectrum with all other
methods in between. When using complete linkage, the distance between
two clusters is assumed to be the distance between both clusters’ most
distant points. This opposite it true for single linkage, where the
minimum distance between any two points, one from each of two clusters
is used. Single linkage is described as friends-of-friends appporach -
and really, it groups all close-together points into the same cluster
(thus resolving one cluster per crescent). Complete linkage on the other
hand recognises that some points a the tip of a crescent are much closer
to points in the other crescent and so it splits both crescents.</p>
</section><section><h2 class="section-heading" id="using-different-distance-methods">Using different distance methods<a class="anchor" aria-label="anchor" href="#using-different-distance-methods"></a>
</h2>
<hr class="half-width">
<p>So far, we’ve been using Euclidean distance to define the
dissimilarity or distance between observations. However, this isn’t
always the best metric for how dissimilar the observations are. Let’s
make an example to demonstrate. Here, we’re creating two samples each
with ten observations of random noise:</p>
<div class="codewrapper sourceCode" id="cb54">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">set.seed</span><span class="op">(</span><span class="fl">20</span><span class="op">)</span></span>
<span><span class="va">cor_example</span> <span class="op">&lt;-</span> <span class="fu">data.frame</span><span class="op">(</span></span>
<span>  sample_a <span class="op">=</span> <span class="fu">rnorm</span><span class="op">(</span><span class="fl">10</span><span class="op">)</span>,</span>
<span>  sample_b <span class="op">=</span> <span class="fu">rnorm</span><span class="op">(</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="fu">rownames</span><span class="op">(</span><span class="va">cor_example</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu">paste</span><span class="op">(</span></span>
<span>  <span class="st">"Feature"</span>, <span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">cor_example</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre>
</div>
<p>Now, let’s create a new sample that has exactly the same pattern
across all our features as <code>sample_a</code>, just offset by 5:</p>
<div class="codewrapper sourceCode" id="cb55">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cor_example</span><span class="op">$</span><span class="va">sample_c</span> <span class="op">&lt;-</span> <span class="va">cor_example</span><span class="op">$</span><span class="va">sample_a</span> <span class="op">+</span> <span class="fl">5</span></span></code></pre>
</div>
<p>You can see that this is a lot like the <code>assay()</code> of our
methylation object from earlier, where columns are observations or
samples, and rows are features:</p>
<div class="codewrapper sourceCode" id="cb56">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">head</span><span class="op">(</span><span class="va">cor_example</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>            sample_a    sample_b sample_c
Feature 1  1.1626853 -0.02013537 6.162685
Feature 2 -0.5859245 -0.15038222 4.414076
Feature 3  1.7854650 -0.62812676 6.785465
Feature 4 -1.3325937  1.32322085 3.667406
Feature 5 -0.4465668 -1.52135057 4.553433
Feature 6  0.5696061 -0.43742787 5.569606</code></pre>
</div>
<p>If we plot a heatmap of this, we can see that <code>sample_a</code>
and <code>sample_b</code> are grouped together because they have a small
distance from each other, despite being quite different in their pattern
across the different features. In contrast, <code>sample_a</code> and
<code>sample_c</code> are very distant, despite having <em>exactly</em>
the same pattern across the different features.</p>
<div class="codewrapper sourceCode" id="cb58">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">pheatmap</span><span class="op">(</span><span class="fu">as.matrix</span><span class="op">(</span><span class="va">cor_example</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in pheatmap(as.matrix(cor_example)): could not find function "pheatmap"</code></pre>
</div>
<p>We can see that more clearly if we do a line plot:</p>
<div class="codewrapper sourceCode" id="cb60">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## create a blank plot (type = "n" means don't draw anything)</span></span>
<span><span class="co">## with an x range to hold the number of features we have.</span></span>
<span><span class="co">## the range of y needs to be enough to show all the values for every feature</span></span>
<span><span class="fu">plot</span><span class="op">(</span></span>
<span>  <span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">cor_example</span><span class="op">)</span>,</span>
<span>  <span class="fu">rep</span><span class="op">(</span><span class="fu">range</span><span class="op">(</span><span class="va">cor_example</span><span class="op">)</span>, <span class="fl">5</span><span class="op">)</span>,</span>
<span>  type <span class="op">=</span> <span class="st">"n"</span>, </span>
<span>  xlab <span class="op">=</span> <span class="st">"Feature number"</span>,</span>
<span>  ylab <span class="op">=</span> <span class="st">"Value"</span></span>
<span><span class="op">)</span></span>
<span><span class="co">## draw a red line for sample_a</span></span>
<span><span class="fu">lines</span><span class="op">(</span><span class="va">cor_example</span><span class="op">$</span><span class="va">sample_a</span>, col <span class="op">=</span> <span class="st">"firebrick"</span><span class="op">)</span></span>
<span><span class="co">## draw a blue line for sample_b</span></span>
<span><span class="fu">lines</span><span class="op">(</span><span class="va">cor_example</span><span class="op">$</span><span class="va">sample_b</span>, col <span class="op">=</span> <span class="st">"dodgerblue"</span><span class="op">)</span></span>
<span><span class="co">## draw a green line for sample_c</span></span>
<span><span class="fu">lines</span><span class="op">(</span><span class="va">cor_example</span><span class="op">$</span><span class="va">sample_c</span>, col <span class="op">=</span> <span class="st">"forestgreen"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-lineplot-cor-example-1.png" alt="A line plot of simulated value versus observation number, coloured by sample. Samples a and b are concentrated at the bottom of the plot, while sample c is concentrated at the top of the plot. Samples a and c have exactly the same vertical pattern." class="figure mx-auto d-block"><figcaption>
Line plot of simulated value versus observation number, coloured by
sample.
</figcaption></figure><p>We can see that <code>sample_a</code> and <code>sample_c</code> have
exactly the same pattern across all of the different features. However,
due to the overall difference between the values, they have a high
distance to each other. We can see that if we cluster and plot the data
ourselves using Euclidean distance:</p>
<div class="codewrapper sourceCode" id="cb61">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">clust_dist</span> <span class="op">&lt;-</span> <span class="fu">hclust</span><span class="op">(</span><span class="fu">dist</span><span class="op">(</span><span class="fu">t</span><span class="op">(</span><span class="va">cor_example</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">clust_dist</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-clust-euc-cor-example-1.png" alt="A dendrogram of the example simulated data clustered according to Euclidean distance. The dendrogram shows that sample c definitively forms its own cluster for any cut height and samples a and b merge into a cluster at a height of around 6." class="figure mx-auto d-block"><figcaption>
Dendrogram of the example simulated data clustered according to
Euclidean distance.
</figcaption></figure><p>In some cases, we might want to ensure that samples that have similar
patterns, whether that be of gene expression, or DNA methylation, have
small distances to each other. Correlation is a measure of this kind of
similarity in pattern. However, high correlations indicate similarity,
while for a distance measure we know that high distances indicate
dissimilarity. Therefore, if we wanted to cluster observations based on
the correlation, or the similarity of patterns, we can use
<code>1 - cor(x)</code> as the distance metric. The input to
<code>hclust()</code> must be a <code>dist</code> object, so we also
need to call <code>as.dist()</code> on it before passing it in.</p>
<div class="codewrapper sourceCode" id="cb62">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cor_as_dist</span> <span class="op">&lt;-</span> <span class="fu">as.dist</span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="fu">cor</span><span class="op">(</span><span class="va">cor_example</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">clust_cor</span> <span class="op">&lt;-</span> <span class="fu">hclust</span><span class="op">(</span><span class="va">cor_as_dist</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">clust_cor</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-clust-cor-cor-example-1.png" alt="A dendrogram of the example simulated data clustered according to correlation. The dendrogram shows that sample b definitively forms its own cluster and samples a and c form definitively form their own cluster for any cut height." class="figure mx-auto d-block"><figcaption>
Dendrogram of the example simulated data clustered according to
correlation.
</figcaption></figure><p>Now, <code>sample_a</code> and <code>sample_c</code> that have
identical patterns across the features are grouped together, while
<code>sample_b</code> is seen as distant because it has a different
pattern, even though its values are closer to <code>sample_a</code>.
Using your own distance function is often useful, especially if you have
missing or unusual data. It’s often possible to use correlation and
other custom dissimilarity measures in functions that perform
hierarchical clustering, such as <code>pheatmap()</code> and
<code>stats::heatmap()</code>:</p>
<div class="codewrapper sourceCode" id="cb63">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## pheatmap allows you to select correlation directly</span></span>
<span><span class="fu">pheatmap</span><span class="op">(</span><span class="fu">as.matrix</span><span class="op">(</span><span class="va">cor_example</span><span class="op">)</span>, clustering_distance_cols <span class="op">=</span> <span class="st">"correlation"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in pheatmap(as.matrix(cor_example), clustering_distance_cols = "correlation"): could not find function "pheatmap"</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb65">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Using the built-in stats::heatmap </span></span>
<span><span class="fu">heatmap</span><span class="op">(</span></span>
<span>  <span class="fu">as.matrix</span><span class="op">(</span><span class="va">cor_example</span><span class="op">)</span>,</span>
<span>  distfun <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">as.dist</span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="fu">cor</span><span class="op">(</span><span class="fu">t</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-heatmap-cor-cor-example-1.png" alt="Heatmaps of features versus samples, coloured by simulated value. The columns (samples) are clustered according to the correlation. Samples a and b have mostly low values, delineated by blue in the first plot and yellow in the second plot. Sample c has mostly high values, delineated by red in the first plot and brown in the second plot. Samples A and C form a cluster separate from sample B in the column dendrogram." class="figure mx-auto d-block"><figcaption>
Heatmaps of features versus samples clustered in the samples according
to correlation.
</figcaption></figure></section><section><h2 class="section-heading" id="validating-clusters">Validating clusters<a class="anchor" aria-label="anchor" href="#validating-clusters"></a>
</h2>
<hr class="half-width">
<p>Now that we know how to carry out hierarchical clustering, how do we
know how many clusters are optimal for the dataset?</p>
<p>Hierarchical clustering carried out on any dataset will produce
clusters, even when there are no ‘real’ clusters in the data! We need to
be able to determine whether identified clusters represent true groups
in the data, or whether clusters have been identified just due to
chance. In the last episode, we have introduced silhouette scores as a
measure of cluster compactness and bootstrapping to assess cluster
robustness. Such tests can be used to compare different clustering
algorithms, for example, those fitted using different linkage
methods.</p>
<p>Here, we introduce the Dunn index, which is a measure of cluster
compactness. The Dunn index is the ratio of the smallest distance
between any two clusters and to the largest intra-cluster distance found
within any cluster. This can be seen as a family of indices which differ
depending on the method used to compute distances. The Dunn index is a
metric that penalises clusters that have larger intra-cluster variance
and smaller inter-cluster variance. The higher the Dunn index, the
better defined the clusters.</p>
<p>Let’s calculate the Dunn index for clustering carried out on the
<code>methyl_mat</code> dataset using the
<strong><code>clValid</code></strong> package.</p>
<div class="codewrapper sourceCode" id="cb66">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## calculate dunn index</span></span>
<span><span class="co">## (ratio of the smallest distance between obs not in the same cluster</span></span>
<span><span class="co">## to the largest intra-cluster distance)</span></span>
<span><span class="kw">library</span><span class="op">(</span><span class="st">"clValid"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: cluster</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb68">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## calculate euclidean distance between points </span></span>
<span><span class="va">distmat</span> <span class="op">&lt;-</span> <span class="fu">dist</span><span class="op">(</span><span class="va">methyl_mat</span><span class="op">)</span>  </span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'methyl_mat' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb70">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">clust</span> <span class="op">&lt;-</span> <span class="fu">hclust</span><span class="op">(</span><span class="va">distmat</span>, method <span class="op">=</span> <span class="st">"complete"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'distmat' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb72">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">plot</span><span class="op">(</span><span class="va">clust</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/07-hierarchical-rendered-plot-clust-dunn-1.png" alt="A dendrogram for clustering of methylation data. Identical to that in the section Highlighting dendrogram branches, without the colour overlay to show clusters." class="figure mx-auto d-block"><figcaption>
Dendrogram for clustering of methylation data.
</figcaption></figure><div class="codewrapper sourceCode" id="cb73">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cut</span> <span class="op">&lt;-</span> <span class="fu">cutree</span><span class="op">(</span><span class="va">clust</span>, h <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## retrieve Dunn's index for given matrix and clusters</span></span>
<span><span class="fu">dunn</span><span class="op">(</span>distance <span class="op">=</span> <span class="va">distmat</span>, <span class="va">cut</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'distmat' not found</code></pre>
</div>
<p>The value of the Dunn index has no meaning in itself, but is used to
compare between sets of clusters with larger values being preferred.</p>
<div id="challenge-4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="challenge-4" class="callout-inner">
<h3 class="callout-title">Challenge 4</h3>
<div class="callout-content">
<p>Examine how changing the <code>h</code> or <code>k</code> arguments
in <code>cutree()</code> affects the value of the Dunn index.</p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">
  Solution:
  </h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" aria-labelledby="headingSolution4" data-bs-parent="#accordionSolution4">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb75">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"clValid"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">distmat</span> <span class="op">&lt;-</span> <span class="fu">dist</span><span class="op">(</span><span class="va">methyl_mat</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'methyl_mat' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb77">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">clust</span> <span class="op">&lt;-</span> <span class="fu">hclust</span><span class="op">(</span><span class="va">distmat</span>, method <span class="op">=</span> <span class="st">"complete"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'distmat' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb79">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Varying h</span></span>
<span><span class="co">## Obtaining the clusters</span></span>
<span><span class="va">cut_h_20</span> <span class="op">&lt;-</span> <span class="fu">cutree</span><span class="op">(</span><span class="va">clust</span>, h <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></span>
<span><span class="va">cut_h_30</span> <span class="op">&lt;-</span> <span class="fu">cutree</span><span class="op">(</span><span class="va">clust</span>, h <span class="op">=</span> <span class="fl">30</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## How many clusters?</span></span>
<span><span class="fu">length</span><span class="op">(</span><span class="fu">table</span><span class="op">(</span><span class="va">cut_h_20</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 1</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb81">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">length</span><span class="op">(</span><span class="fu">table</span><span class="op">(</span><span class="va">cut_h_30</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 1</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb83">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">dunn</span><span class="op">(</span>distance <span class="op">=</span> <span class="va">distmat</span>, <span class="va">cut_h_20</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'distmat' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb85">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">dunn</span><span class="op">(</span>distance <span class="op">=</span> <span class="va">distmat</span>, <span class="va">cut_h_30</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'distmat' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb87">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Varying k</span></span>
<span><span class="co">## Obtaining the clusters</span></span>
<span><span class="va">cut_k_10</span> <span class="op">&lt;-</span> <span class="fu">cutree</span><span class="op">(</span><span class="va">clust</span>, k <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">cut_k_5</span> <span class="op">&lt;-</span> <span class="fu">cutree</span><span class="op">(</span><span class="va">clust</span>, k <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## How many clusters?</span></span>
<span><span class="fu">length</span><span class="op">(</span><span class="fu">table</span><span class="op">(</span><span class="va">cut_k_5</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 5</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb89">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">length</span><span class="op">(</span><span class="fu">table</span><span class="op">(</span><span class="va">cut_k_10</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 10</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb91">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">dunn</span><span class="op">(</span>distance <span class="op">=</span> <span class="va">distmat</span>, <span class="va">cut_k_5</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'distmat' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb93">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">dunn</span><span class="op">(</span>distance <span class="op">=</span> <span class="va">distmat</span>, <span class="va">cut_k_10</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error: object 'distmat' not found</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>The figures below show in a more systematic way how changing the
values of <code>k</code> and <code>h</code> using <code>cutree()</code>
affect the Dunn index.</p>
<div class="codewrapper sourceCode" id="cb95">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">h_seq</span> <span class="op">&lt;-</span> <span class="fl">70</span><span class="op">:</span><span class="fl">10</span></span>
<span><span class="va">h_dunn</span> <span class="op">&lt;-</span> <span class="fu">sapply</span><span class="op">(</span><span class="va">h_seq</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">dunn</span><span class="op">(</span>distance <span class="op">=</span> <span class="va">distmat</span>, <span class="fu">cutree</span><span class="op">(</span><span class="va">clust</span>, h <span class="op">=</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in FUN(X[[i]], ...): object 'distmat' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb97">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">k_seq</span> <span class="op">&lt;-</span> <span class="fu">seq</span><span class="op">(</span><span class="fl">2</span>, <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">k_dunn</span> <span class="op">&lt;-</span> <span class="fu">sapply</span><span class="op">(</span><span class="va">k_seq</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">dunn</span><span class="op">(</span>distance <span class="op">=</span> <span class="va">distmat</span>, <span class="fu">cutree</span><span class="op">(</span><span class="va">clust</span>, k <span class="op">=</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in FUN(X[[i]], ...): object 'distmat' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb99">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">plot</span><span class="op">(</span><span class="va">h_seq</span>, <span class="va">h_dunn</span>, xlab <span class="op">=</span> <span class="st">"Height (h)"</span>, ylab <span class="op">=</span> <span class="st">"Dunn index"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in h(simpleError(msg, call)): error in evaluating the argument 'y' in selecting a method for function 'plot': object 'h_dunn' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb101">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">grid</span><span class="op">(</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet</code></pre>
</div>
<p>You can see that at low values of <code>h</code>, the Dunn index can
be high. But this is not very useful - cutting the given tree at a low
<code>h</code> value like 15 leads to allmost all observations ending up
each in its own cluster. More relevant is the second maximum in the
plot, around <code>h=55</code>. Looking at the dendrogram, this
corresponds to <code>k=4</code>.</p>
<div class="codewrapper sourceCode" id="cb103">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">plot</span><span class="op">(</span><span class="va">k_seq</span>, <span class="va">k_dunn</span>, xlab <span class="op">=</span> <span class="st">"Number of clusters (k)"</span>, ylab <span class="op">=</span> <span class="st">"Dunn index"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in h(simpleError(msg, call)): error in evaluating the argument 'y' in selecting a method for function 'plot': object 'k_dunn' not found</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb105">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">grid</span><span class="op">(</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet</code></pre>
</div>
<p>For the given range of <code>k</code> values explored, we obtain the
highest Dunn index with <code>k=4</code>. This is in agreement with the
previous plot.</p>
<p>There have been criticisms of the use of the Dunn index in validating
clustering results, due to its high sensitivity to noise in the dataset.
An alternative is to use silhouette scores (see the k-means clustering
episode).</p>
<p>As we said before (see previous episode), clustering is a non-trivial
task. It is important to think about the nature of your data and your
expectations rather than blindly using a some algorithm for clustering
or cluster validation.</p>
</section><section><h2 class="section-heading" id="further-reading">Further reading<a class="anchor" aria-label="anchor" href="#further-reading"></a>
</h2>
<hr class="half-width">
<ul>
<li>Dunn, J. C. (1974) Well-separated clusters and optimal fuzzy
partitions. Journal of Cybernetics 4(1):95–104.</li>
<li>Halkidi, M., Batistakis, Y. &amp; Vazirgiannis, M. (2001) On
clustering validation techniques. Journal of Intelligent Information
Systems 17(2/3):107-145.</li>
<li>James, G., Witten, D., Hastie, T. &amp; Tibshirani, R. (2013) An
Introduction to Statistical Learning with Applications in R. Section
10.3.2 (Hierarchical Clustering).</li>
<li>
<a href="https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec" class="external-link">Understanding
the concept of Hierarchical clustering Technique. towards data science
blog</a>.</li>
</ul>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>Hierarchical clustering uses an algorithm to group similar data
points into clusters. A dendrogram is used to plot relationships between
clusters (using the <code>hclust()</code> function in R).</li>
<li>Hierarchical clustering differs from k-means clustering as it does
not require the user to specify expected number of clusters</li>
<li>The distance (dissimilarity) matrix can be calculated in various
ways, and different clustering algorithms (linkage methods) can affect
the resulting dendrogram.</li>
<li>The Dunn index can be used to validate clusters using the original
dataset.</li>
</ul>
</div>
</div>
</div>
</section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:alan.ocallaghan@outlook.com">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.17.1" class="external-link">sandpaper (0.17.1)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.7" class="external-link">varnish (1.0.7)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://carpentries-incubator.github.io/high-dimensional-stats-r/instructor/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://carpentries-incubator.github.io/high-dimensional-stats-r/instructor/aio.html",
  "identifier": "https://carpentries-incubator.github.io/high-dimensional-stats-r/instructor/aio.html",
  "dateCreated": "2025-09-02",
  "dateModified": "2025-09-09",
  "datePublished": "2025-09-09"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo --><script>
          var _paq = window._paq = window._paq || [];
          /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
          _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
          _paq.push(["setDomains", ["*.lessons.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
          _paq.push(["setDoNotTrack", true]);
          _paq.push(["disableCookies"]);
          _paq.push(["trackPageView"]);
          _paq.push(["enableLinkTracking"]);
          (function() {
              var u="https://matomo.carpentries.org/";
              _paq.push(["setTrackerUrl", u+"matomo.php"]);
              _paq.push(["setSiteId", "1"]);
              var d=document, g=d.createElement("script"), s=d.getElementsByTagName("script")[0];
              g.async=true; g.src="https://matomo.carpentries.org/matomo.js"; s.parentNode.insertBefore(g,s);
          })();
        </script><!-- End Matomo Code -->
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

