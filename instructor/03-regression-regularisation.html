<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>High dimensional statistics with R: Regularised regression</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css"><script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../favicons/incubator/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicons/incubator/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="../favicons/incubator/favicon-16x16.png"><link rel="manifest" href="../favicons/incubator/site.webmanifest"><link rel="mask-icon" href="../favicons/incubator/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Lesson Description" src="../assets/images/incubator-logo.svg"><span class="badge text-bg-info">
          <abbr title="This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.">
            <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#polishing-beta-stage" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-circle" style="border-radius: 5px"></i>
              Beta
            </a>
            <span class="visually-hidden">This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='../03-regression-regularisation.html';">Learner View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Lesson Description" src="../assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      High dimensional statistics with R
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            High dimensional statistics with R
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><li><a class="dropdown-item" href="data.html">Data</a></li><hr></ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  High dimensional statistics with R
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 25%" class="percentage">
    25%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 25%" aria-valuenow="25" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../03-regression-regularisation.html">Learner View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction-to-high-dimensional-data.html">1. Introduction to high-dimensional data</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-high-dimensional-regression.html">2. Regression with many outcomes</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        3. Regularised regression
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#introduction">Introduction</a></li>
<li><a href="#coefficient-estimates-of-a-linear-model">Coefficient estimates of a linear model</a></li>
<li><a href="#model-selection-using-training-and-test-sets">Model selection using training and test sets</a></li>
<li><a href="#regularisation">Regularisation</a></li>
<li><a href="#ridge-regression">Ridge regression</a></li>
<li><a href="#lasso-regression">LASSO regression</a></li>
<li><a href="#cross-validation-to-find-the-best-value-of-lambda">Cross-validation to find the best value of <span class="math inline">\(\lambda\)</span></a></li>
<li><a href="#elastic-nets-blending-ridge-regression-and-the-lasso">Elastic nets: blending ridge regression and the LASSO</a></li>
<li><a href="#further-reading">Further reading</a></li>
<li><a href="#footnotes">Footnotes</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-principal-component-analysis.html">4. Principal component analysis</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="05-factor-analysis.html">5. Factor analysis</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-k-means.html">6. K-means</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="07-hierarchical.html">7. Hierarchical clustering</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <li><a href="data.html">Data</a></li><hr></ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/02-high-dimensional-regression.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/04-principal-component-analysis.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/02-high-dimensional-regression.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Regression with many
        </a>
        <a class="chapter-link float-end" href="../instructor/04-principal-component-analysis.html" rel="next">
          Next: Principal component...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Regularised regression</h1>
        <p>Last updated on 2025-04-01 |

        <a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/edit/main/episodes/03-regression-regularisation.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 170 minutes</p>

        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>What is regularisation?</li>
<li>How does regularisation work?</li>
<li>How can we select the level of regularisation for a model?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Understand the benefits of regularised models.</li>
<li>Understand how different types of regularisation work.</li>
<li>Apply and critically analyse regularised regression models.</li>
</ul></div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a></h2>
<hr class="half-width"><p>This episode is about <strong>regularisation</strong>, also called
<strong>regularised regression</strong> or <strong>penalised
regression</strong>. This approach can be used for prediction and for
feature selection and it is particularly useful when dealing with
high-dimensional data.</p>
<p>One reason that we need special statistical tools for
high-dimensional data is that standard linear models cannot handle
high-dimensional data sets – one cannot fit a linear model where there
are more features (predictor variables) than there are observations
(data points). In the previous lesson, we dealt with this problem by
fitting individual models for each feature and sharing information among
these models. Now we will take a look at an alternative approach that
can be used to fit models with more features than observations by
stabilising coefficient estimates. This approach is called
regularisation. Compared to many other methods, regularisation is also
often very fast and can therefore be extremely useful in practice.</p>
<p>First, let us check out what happens if we try to fit a linear model
to high-dimensional data! We start by reading in the <a href="https://carpentries-incubator.github.io/high-dimensional-stats-r/data/index.html" class="external-link"><code>methylation</code></a>
data from the last lesson:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"SummarizedExperiment"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: MatrixGenerics</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: matrixStats</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'MatrixGenerics'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:matrixStats':

    colAlls, colAnyNAs, colAnys, colAvgsPerRowSet, colCollapse,
    colCounts, colCummaxs, colCummins, colCumprods, colCumsums,
    colDiffs, colIQRDiffs, colIQRs, colLogSumExps, colMadDiffs,
    colMads, colMaxs, colMeans2, colMedians, colMins, colOrderStats,
    colProds, colQuantiles, colRanges, colRanks, colSdDiffs, colSds,
    colSums2, colTabulates, colVarDiffs, colVars, colWeightedMads,
    colWeightedMeans, colWeightedMedians, colWeightedSds,
    colWeightedVars, rowAlls, rowAnyNAs, rowAnys, rowAvgsPerColSet,
    rowCollapse, rowCounts, rowCummaxs, rowCummins, rowCumprods,
    rowCumsums, rowDiffs, rowIQRDiffs, rowIQRs, rowLogSumExps,
    rowMadDiffs, rowMads, rowMaxs, rowMeans2, rowMedians, rowMins,
    rowOrderStats, rowProds, rowQuantiles, rowRanges, rowRanks,
    rowSdDiffs, rowSds, rowSums2, rowTabulates, rowVarDiffs, rowVars,
    rowWeightedMads, rowWeightedMeans, rowWeightedMedians,
    rowWeightedSds, rowWeightedVars</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: GenomicRanges</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: stats4</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: BiocGenerics</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'BiocGenerics'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:stats':

    IQR, mad, sd, var, xtabs</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:base':

    anyDuplicated, aperm, append, as.data.frame, basename, cbind,
    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,
    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,
    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,
    Position, rank, rbind, Reduce, rownames, sapply, saveRDS, setdiff,
    table, tapply, union, unique, unsplit, which.max, which.min</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: S4Vectors</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'S4Vectors'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following object is masked from 'package:utils':

    findMatches</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:base':

    expand.grid, I, unname</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: IRanges</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: GenomeInfoDb</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: Biobase</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Welcome to Bioconductor

    Vignettes contain introductory material; view with
    'browseVignettes()'. To cite Bioconductor, see
    'citation("Biobase")', and for packages 'citation("pkgname")'.</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'Biobase'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following object is masked from 'package:MatrixGenerics':

    rowMedians</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following objects are masked from 'package:matrixStats':

    anyMissing, rowMedians</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: replacing previous import 'S4Arrays::makeNindexFromArrayViewport' by
'DelayedArray::makeNindexFromArrayViewport' when loading 'SummarizedExperiment'</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">methylation</span> <span class="op">&lt;-</span> <span class="fu">readRDS</span><span class="op">(</span><span class="st">"data/methylation.rds"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## here, we transpose the matrix to have features as rows and samples as columns</span></span>
<span><span class="va">methyl_mat</span> <span class="op">&lt;-</span> <span class="fu">t</span><span class="op">(</span><span class="fu">assay</span><span class="op">(</span><span class="va">methylation</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">age</span> <span class="op">&lt;-</span> <span class="va">methylation</span><span class="op">$</span><span class="va">Age</span></span></code></pre>
</div>
<p>Then, we try to fit a model with outcome age and all 5,000 features
in this dataset as predictors (average methylation levels, M-values,
across different sites in the genome).</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># by using methyl_mat in the formula below, R will run a multivariate regression</span></span>
<span><span class="co"># model in which each of the columns in methyl_mat is used as a predictor. </span></span>
<span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu">lm</span><span class="op">(</span><span class="va">age</span> <span class="op">~</span> <span class="va">methyl_mat</span><span class="op">)</span></span>
<span><span class="fu">summary</span><span class="op">(</span><span class="va">fit</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Call:
lm(formula = age ~ methyl_mat)

Residuals:
ALL 37 residuals are 0: no residual degrees of freedom!

Coefficients: (4964 not defined because of singularities)
                          Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)               2640.474        NaN     NaN      NaN
methyl_matcg00075967      -108.216        NaN     NaN      NaN
methyl_matcg00374717      -139.637        NaN     NaN      NaN
methyl_matcg00864867        33.102        NaN     NaN      NaN
methyl_matcg00945507        72.250        NaN     NaN      NaN
 [ reached getOption("max.print") -- omitted 4996 rows ]

Residual standard error: NaN on 0 degrees of freedom
Multiple R-squared:      1,	Adjusted R-squared:    NaN
F-statistic:   NaN on 36 and 0 DF,  p-value: NA</code></pre>
</div>
<p>You can see that we’re able to get some effect size estimates, but
they seem very high! The summary also says that we were unable to
estimate effect sizes for 4,964 features because of “singularities”. We
clarify what singularities are in the note below but this means that R
couldn’t find a way to perform the calculations necessary to fit the
model. Large effect sizes and singularities are common when naively
fitting linear regression models with a large number of features (i.e.,
to high-dimensional data), often since the model cannot distinguish
between the effects of many, correlated features or when we have more
features than observations.</p>
<div id="singularities" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="singularities" class="callout-inner">
<h3 class="callout-title">Singularities</h3>
<div class="callout-content">
<p>The message that <code>lm()</code> produced is not necessarily the
most intuitive. What are “singularities” and why are they an issue? A
singular matrix is one that cannot be <a href="https://en.wikipedia.org/wiki/Invertible_matrix" class="external-link">inverted</a>. R
uses inverse operations to fit linear models (find the coefficients)
using:</p>
<p><span class="math display">\[
(X^TX)^{-1}X^Ty,
\]</span></p>
<p>where <span class="math inline">\(X\)</span> is a matrix of predictor
features and <span class="math inline">\(y\)</span> is the outcome
vector. Thus, if the matrix <span class="math inline">\(X^TX\)</span>
cannot be inverted to give <span class="math inline">\((X^TX)^{-1}\)</span>, R cannot fit the model and
returns the error that there are singularities.</p>
<p>Why might R be unable to calculate <span class="math inline">\((X^TX)^{-1}\)</span> and return the error that
there are singularities? Well, when the <a href="https://en.wikipedia.org/wiki/Determinant" class="external-link">determinant</a> of the
matrix is zero, we are unable to find its inverse. The determinant of
the matrix is zero when there are more features than observations or
often when the features are highly correlated.</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">xtx</span> <span class="op">&lt;-</span> <span class="fu">t</span><span class="op">(</span><span class="va">methyl_mat</span><span class="op">)</span> <span class="op">%*%</span> <span class="va">methyl_mat</span></span>
<span><span class="fu">det</span><span class="op">(</span><span class="va">xtx</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 0</code></pre>
</div>
</div>
</div>
</div>
<div id="correlated-features-common-in-high-dimensional-data" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="correlated-features-common-in-high-dimensional-data" class="callout-inner">
<h3 class="callout-title">Correlated features – common in high-dimensional data</h3>
<div class="callout-content">
<p>In high-dimensional datasets, there are often multiple features that
contain redundant information (correlated features). If we visualise the
level of correlation between sites in the <code>methylation</code>
dataset, we can see that many of the features represent the same
information - there are many off-diagonal cells, which are deep red or
blue. For example, the following heatmap visualises the correlations for
the first 500 features in the <code>methylation</code> dataset (we
selected 500 features only as it can be hard to visualise patterns when
there are too many features!).</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"pheatmap"</span><span class="op">)</span></span>
<span><span class="va">small</span> <span class="op">&lt;-</span> <span class="va">methyl_mat</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">500</span><span class="op">]</span></span>
<span><span class="va">cor_mat</span> <span class="op">&lt;-</span> <span class="fu">cor</span><span class="op">(</span><span class="va">small</span><span class="op">)</span></span>
<span><span class="fu">pheatmap</span><span class="op">(</span><span class="va">cor_mat</span>,</span>
<span>         main <span class="op">=</span> <span class="st">"Feature-feature correlation in methylation data"</span>,</span>
<span>         legend_title <span class="op">=</span> <span class="st">"Pearson correlation"</span>,</span>
<span>         cluster_rows <span class="op">=</span> <span class="cn">FALSE</span>, </span>
<span>         cluster_cols <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>         show_rownames <span class="op">=</span> <span class="cn">FALSE</span>, </span>
<span>         show_colnames <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-corr-mat-meth-1.png" alt="A symmetrical heatmap where rows and columns are features in a DNA methylation dataset. Colour corresponds to correlation, with red being large positive correlations and blue being large negative correlations. There are large blocks of deep red and blue throughout the plot." class="figure mx-auto d-block"><figcaption>
Heatmap of pairwise feature-feature correlations between CpG sites in
DNA methylation data
</figcaption></figure></div>
</div>
</div>
<div id="challenge-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-1" class="callout-inner">
<h3 class="callout-title">Challenge 1</h3>
<div class="callout-content">
<p>Consider or discuss in groups:</p>
<ol style="list-style-type: decimal"><li>Why would we observe correlated features in high-dimensional
biological data?</li>
<li>Why might correlated features be a problem when fitting linear
models?</li>
<li>What issue might correlated features present when selecting features
to include in a model one at a time?</li>
</ol></div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal"><li>Many of the features in biological data represent very similar
information biologically. For example, sets of genes that form complexes
are often expressed in very similar quantities. Similarly, methylation
levels at nearby sites are often very highly correlated.</li>
<li>Correlated features can make inference unstable or even impossible
mathematically.</li>
<li>When we are selecting features one at a time we want to pick the
most predictive feature each time. When a lot of features are very
similar but encode slightly different information, which of the
correlated features we select to include can have a huge impact on the
later stages of model selection!</li>
</ol></div>
</div>
</div>
</div>
<p>Regularisation can help us to deal with correlated features, as well
as effectively reduce the number of features in our model. Before we
describe regularisation, let’s recap what’s going on when we fit a
linear model.</p>
</section><section><h2 class="section-heading" id="coefficient-estimates-of-a-linear-model">Coefficient estimates of a linear model<a class="anchor" aria-label="anchor" href="#coefficient-estimates-of-a-linear-model"></a></h2>
<hr class="half-width"><p>When we fit a linear model, we’re finding the line through our data
that minimises the sum of the squared residuals. We can think of that as
finding the slope and intercept that minimises the square of the length
of the dashed lines. In this case, the red line in the left panel is the
line that accomplishes this objective, and the red dot in the right
panel is the point that represents this line in terms of its slope and
intercept among many different possible models, where the background
colour represents how well different combinations of slope and intercept
accomplish this objective.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: viridisLite</code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-regplot-1.png" alt="For each observation, the left panel shows the residuals with respect to the optimal line (the one that minimises the sum of square errors). These are calculated as the difference between the value predicted by the line and the observed outcome. Right panel shows the sum of squared residuals across all possible linear regression models (as defined by different values of the regression coefficients)." class="figure mx-auto d-block"><figcaption>
Illustrative example demonstrated how regression coefficients are
inferred under a linear model framework.
</figcaption></figure><p>Mathematically, we can write the sum of squared residuals as</p>
<p><span class="math display">\[
\sum_{i=1}^N ( y_i-x'_i\beta)^2
\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is a vector of
(unknown) covariate effects which we want to learn by fitting a
regression model: the <span class="math inline">\(j\)</span>-th element
of <span class="math inline">\(\beta\)</span>, which we denote as <span class="math inline">\(\beta_j\)</span> quantifies the effect of the
<span class="math inline">\(j\)</span>-th covariate. For each individual
<span class="math inline">\(i\)</span>, <span class="math inline">\(x_i\)</span> is a vector of <span class="math inline">\(j\)</span> covariate values and <span class="math inline">\(y_i\)</span> is the true observed value for the
outcome. The notation <span class="math inline">\(x'_i\beta\)</span>
indicates matrix multiplication. In this case, the result is equivalent
to multiplying each element of <span class="math inline">\(x_i\)</span>
by its corresponding element in <span class="math inline">\(\beta\)</span> and then calculating the sum across
all of those values. The result of this product (often denoted by <span class="math inline">\(\hat{y}_i\)</span>) is the predicted value of the
outcome generated by the model. As such, <span class="math inline">\(y_i-x'_i\beta\)</span> can be interpreted as
the prediction error, also referred to as model residual. To quantify
the total error across all individuals, we sum the square residuals
<span class="math inline">\(( y_i-x'_i\beta)^2\)</span> across all
the individuals in our data.</p>
<p>Finding the value of <span class="math inline">\(\beta\)</span> that
minimises the sum above is the line of best fit through our data when
considering this goal of minimising the sum of squared error. However,
it is not the only possible line we could use! For example, we might
want to err on the side of caution when estimating effect sizes
(coefficients). That is, we might want to avoid estimating very large
effect sizes.</p>
<div id="challenge-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-2" class="callout-inner">
<h3 class="callout-title">Challenge 2</h3>
<div class="callout-content">
<p>Discuss in groups:</p>
<ol style="list-style-type: decimal"><li>What are we minimising when we fit a linear model?</li>
<li>Why are we minimising this objective? What assumptions are we making
about our data when we do so?</li>
</ol></div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<ol style="list-style-type: decimal"><li>When we fit a linear model we are minimising the squared error. In
fact, the standard linear model estimator is often known as “ordinary
least squares”. The “ordinary” really means “original” here, to
distinguish between this method, which dates back to ~1800, and some
more “recent” (think 1940s…) methods.</li>
<li>Squared error is useful because it ignores the <em>sign</em> of the
residuals (whether they are positive or negative). It also penalises
large errors much more than small errors. On top of all this, it also
makes the solution very easy to compute mathematically. Least squares
assumes that, when we account for the change in the mean of the outcome
based on changes in the income, the data are normally distributed. That
is, the <em>residuals</em> of the model, or the error left over after we
account for any linear relationships in the data, are normally
distributed, and have a fixed variance.</li>
</ol></div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="model-selection-using-training-and-test-sets">Model selection using training and test sets<a class="anchor" aria-label="anchor" href="#model-selection-using-training-and-test-sets"></a></h2>
<hr class="half-width"><p>Sets of models are often compared using statistics such as adjusted
<span class="math inline">\(R^2\)</span>, AIC or BIC. These show us how
well the model is learning the data used in fitting that same model <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. However,
these statistics do not really tell us how well the model will
generalise to new data. This is an important thing to consider – if our
model doesn’t generalise to new data, then there’s a chance that it’s
just picking up on a technical or batch effect in our data, or simply
some noise that happens to fit the outcome we’re modelling. This is
especially important when our goal is prediction – it’s not much good if
we can only predict well for samples where the outcome is already known,
after all!</p>
<p>To get an idea of how well our model generalises, we can split the
data into two - a “training” and a “test” set. We use the “training”
data to fit the model, and then see its performance on the “test”
data.</p>
<figure style="text-align: center"><img src="../fig/validation.png" alt="Schematic representation of how a dataset can be divided into a training (the portion of the data used to fit a model) and a test set (the portion of the data used to assess external generalisability)." width="500px" class="figure mx-auto d-block"><figcaption>
Schematic representation of how a dataset can be divided into a training
and a test set.
</figcaption></figure><p>One thing that often happens in this context is that large
coefficient values minimise the training error, but they don’t minimise
the test error on unseen data. First, we’ll go through an example of
what exactly this means.</p>
<p>To compare the training and test errors for a model of methylation
features and age, we’ll split the data into training and test sets, fit
a linear model and calculate the errors. First, let’s calculate the
training error. Let’s start by splitting the data into training and test
sets:</p>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">methylation</span> <span class="op">&lt;-</span> <span class="fu">readRDS</span><span class="op">(</span><span class="st">"/data/methylation.rds"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning in gzfile(file, "rb"): cannot open compressed file
'/data/methylation.rds', probable reason 'No such file or directory'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in gzfile(file, "rb"): cannot open the connection</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb34">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"SummarizedExperiment"</span><span class="op">)</span></span>
<span><span class="va">age</span> <span class="op">&lt;-</span> <span class="va">methylation</span><span class="op">$</span><span class="va">Age</span></span>
<span><span class="va">methyl_mat</span> <span class="op">&lt;-</span> <span class="fu">t</span><span class="op">(</span><span class="fu">assay</span><span class="op">(</span><span class="va">methylation</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<p>We will then subset the data to a set of CpG markers that are known
to be associated with age from a previous study by Horvath et al.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>, described
in <a href="https://carpentries-incubator.github.io/high-dimensional-stats-r/data/index.html" class="external-link">data</a>.</p>
<div class="codewrapper sourceCode" id="cb35">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cpg_markers</span> <span class="op">&lt;-</span> <span class="fu">c</span><span class="op">(</span><span class="st">"cg16241714"</span>, <span class="st">"cg14424579"</span>, <span class="st">"cg22736354"</span>, <span class="st">"cg02479575"</span>, <span class="st">"cg00864867"</span>, </span>
<span>    <span class="st">"cg25505610"</span>, <span class="st">"cg06493994"</span>, <span class="st">"cg04528819"</span>, <span class="st">"cg26297688"</span>, <span class="st">"cg20692569"</span>, </span>
<span>    <span class="st">"cg04084157"</span>, <span class="st">"cg22920873"</span>, <span class="st">"cg10281002"</span>, <span class="st">"cg21378206"</span>, <span class="st">"cg26005082"</span>, </span>
<span>    <span class="st">"cg12946225"</span>, <span class="st">"cg25771195"</span>, <span class="st">"cg26845300"</span>, <span class="st">"cg06144905"</span>, <span class="st">"cg27377450"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">horvath_mat</span> <span class="op">&lt;-</span> <span class="va">methyl_mat</span><span class="op">[</span>, <span class="va">cpg_markers</span><span class="op">]</span></span>
<span></span>
<span><span class="co">## Generate an index to split the data</span></span>
<span><span class="fu">set.seed</span><span class="op">(</span><span class="fl">42</span><span class="op">)</span></span>
<span><span class="va">train_ind</span> <span class="op">&lt;-</span> <span class="fu">sample</span><span class="op">(</span><span class="fu">nrow</span><span class="op">(</span><span class="va">methyl_mat</span><span class="op">)</span>, <span class="fl">25</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Split the data </span></span>
<span><span class="va">train_mat</span> <span class="op">&lt;-</span> <span class="va">horvath_mat</span><span class="op">[</span><span class="va">train_ind</span>, <span class="op">]</span></span>
<span><span class="va">train_age</span> <span class="op">&lt;-</span> <span class="va">age</span><span class="op">[</span><span class="va">train_ind</span><span class="op">]</span></span>
<span><span class="va">test_mat</span> <span class="op">&lt;-</span> <span class="va">horvath_mat</span><span class="op">[</span><span class="op">-</span><span class="va">train_ind</span>, <span class="op">]</span></span>
<span><span class="va">test_age</span> <span class="op">&lt;-</span> <span class="va">age</span><span class="op">[</span><span class="op">-</span><span class="va">train_ind</span><span class="op">]</span></span></code></pre>
</div>
<p>Now let’s fit a linear model to our training data and calculate the
training error. Here we use the mean of the squared difference between
our predictions and the observed data, or “mean squared error”
(MSE).</p>
<div class="codewrapper sourceCode" id="cb36">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Fit a linear model</span></span>
<span><span class="co"># as.data.frame() converts train_mat into a data.frame</span></span>
<span><span class="co"># Using the `.` syntax above together with a `data` argument will lead to</span></span>
<span><span class="co"># the same result as using `train_age ~ train_mat`: R will fit a multivariate </span></span>
<span><span class="co"># regression model in which each of the columns in `train_mat` is used as </span></span>
<span><span class="co"># a predictor. We opted to use the `.` syntax because it will help us to </span></span>
<span><span class="co"># obtain model predictions using the `predict()` function. </span></span>
<span></span>
<span><span class="va">fit_horvath</span> <span class="op">&lt;-</span> <span class="fu">lm</span><span class="op">(</span><span class="va">train_age</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="fu">as.data.frame</span><span class="op">(</span><span class="va">train_mat</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Function to calculate the (mean squared) error</span></span>
<span><span class="va">mse</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">true</span>, <span class="va">prediction</span><span class="op">)</span> <span class="op">{</span> </span>
<span>    <span class="fu">mean</span><span class="op">(</span><span class="op">(</span><span class="va">true</span> <span class="op">-</span> <span class="va">prediction</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> </span>
<span><span class="op">}</span> </span>
<span></span>
<span><span class="co">## Calculate the training error </span></span>
<span><span class="va">err_lm_train</span> <span class="op">&lt;-</span> <span class="fu">mse</span><span class="op">(</span><span class="va">train_age</span>, <span class="fu">fitted</span><span class="op">(</span><span class="va">fit_horvath</span><span class="op">)</span><span class="op">)</span> </span>
<span><span class="va">err_lm_train</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 1.319628</code></pre>
</div>
<p>The training error appears very low here – on average we’re only off
by about a year!</p>
<div id="challenge-3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-3" class="callout-inner">
<h3 class="callout-title">Challenge 3</h3>
<div class="callout-content">
<p>For the fitted model above, calculate the mean squared error for the
test set.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>First, let’s find the predicted values for the ‘unseen’ test
data:</p>
<div class="codewrapper sourceCode" id="cb38">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pred_lm</span> <span class="op">&lt;-</span> <span class="fu">predict</span><span class="op">(</span><span class="va">fit_horvath</span>, newdata <span class="op">=</span> <span class="fu">as.data.frame</span><span class="op">(</span><span class="va">test_mat</span><span class="op">)</span><span class="op">)</span> </span></code></pre>
</div>
<p>The mean squared error for the test set is the mean of the squared
error between the predicted values and true test data.</p>
<div class="codewrapper sourceCode" id="cb39">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">err_lm</span> <span class="op">&lt;-</span> <span class="fu">mse</span><span class="op">(</span><span class="va">test_age</span>, <span class="va">pred_lm</span><span class="op">)</span></span>
<span><span class="va">err_lm</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 223.3571</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>Unfortunately, the test error is a lot higher than the training
error. If we plot true age against predicted age for the samples in the
test set, we can gain more insight into the performance of the model on
the test set. Ideally, the predicted values should be close to the test
data.</p>
<div class="codewrapper sourceCode" id="cb41">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">par</span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">test_age</span>, <span class="va">pred_lm</span>, pch <span class="op">=</span> <span class="fl">19</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>coef <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-test-plot-lm-1.png" alt="A scatter plot of observed age versus predicted age for individuals in the test set. Each dot represents one individual. Dashed line is used as a reference to indicate how perfect predictions would look (observed = predicted). In this case we observe high prediction error in the test set." class="figure mx-auto d-block"><figcaption>
A scatter plot of observed age versus predicted age for individuals in
the test set. Each dot represents one individual. Dashed line is used as
a reference to indicate how perfect predictions would look (observed =
predicted).
</figcaption></figure><p>This figure shows the predicted ages obtained from a linear model fit
plotted against the true ages, which we kept in the test dataset. If the
prediction were good, the dots should follow a line. Regularisation can
help us to make the model more generalisable, improving predictions for
the test dataset (or any other dataset that is not used when fitting our
model).</p>
</section><section><h2 class="section-heading" id="regularisation">Regularisation<a class="anchor" aria-label="anchor" href="#regularisation"></a></h2>
<hr class="half-width"><p>Regularisation can be used to reduce correlation between predictors,
the number of features, and improve generalisability by restricting
model parameter estimates. Essentially, we add another condition to the
problem we’re solving with linear regression that controls the total
size of the coefficients that come out and shrinks many coefficients to
zero (or near zero).</p>
<p>For example, we might say that the point representing the slope and
intercept must fall within a certain distance of the origin, <span class="math inline">\((0, 0)\)</span>. For the 2-parameter model (slope
and intercept), we could visualise this constraint as a circle with a
given radius. We want to find the “best” solution (in terms of
minimising the residuals) that also falls within a circle of a given
radius (in this case, 2).</p>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-ridgeplot-1.png" alt="For each observation, the left panel shows the residuals with respect to the optimal lines obtained with and without regularisation. Right panel shows the sum of squared residuals across all possible linear regression models. Regularisation moves the line away from the optimal (in terms of minimising the sum of squared residuals). " class="figure mx-auto d-block"><figcaption>
Illustrative example demonstrated how regression coefficients are
inferred under a linear model framework, with (blue line) and without
(red line) regularisation. A ridge penalty is used in this example
</figcaption></figure></section><section><h2 class="section-heading" id="ridge-regression">Ridge regression<a class="anchor" aria-label="anchor" href="#ridge-regression"></a></h2>
<hr class="half-width"><p>There are multiple ways to define the distance that our solution must
fall in. The one we’ve plotted above controls the squared sum of the
coefficients, <span class="math inline">\(\beta\)</span>. This is also
sometimes called the <span class="math inline">\(L^2\)</span> norm. This
is defined as</p>
<p><span class="math display">\[
\left\lVert \beta\right\lVert_2 = \sqrt{\sum_{j=1}^p \beta_j^2}
\]</span></p>
<p>To control this, we specify that the solution for the equation above
also has to have an <span class="math inline">\(L^2\)</span> norm
smaller than a certain amount. Or, equivalently, we try to minimise a
function that includes our <span class="math inline">\(L^2\)</span> norm
scaled by a factor that is usually written <span class="math inline">\(\lambda\)</span>.</p>
<p><span class="math display">\[
\sum_{i=1}^N \biggl( y_i - x'_i\beta\biggr)^2  + \lambda \left\lVert
\beta \right\lVert_2 ^2
\]</span></p>
<p>This type of regularisation is called <em>ridge regression</em>.
Another way of thinking about this is that when finding the best model,
we’re weighing up a balance of the ordinary least squares objective and
a “penalty” term that punishes models with large coefficients. The
balance between the penalty and the ordinary least squares objective is
controlled by <span class="math inline">\(\lambda\)</span> - when <span class="math inline">\(\lambda\)</span> is large, we want to penalise
large coefficients. In other words, we care a lot about the size of the
coefficients and we want to reduce the complexity of our model. When
<span class="math inline">\(\lambda\)</span> is small, we don’t really
care a lot about shrinking our coefficients and we opt for a more
complex model. When it’s zero, we’re back to just using ordinary least
squares. We see how a penalty term, <span class="math inline">\(\lambda\)</span>, might be chosen later in this
episode.</p>
<p>For now, to see how regularisation might improve a model, let’s fit a
model using the same set of 20 features (stored as
<code>cpg_markers</code>) selected earlier in this episode (these are a
subset of the features identified by Horvarth et al), using both
regularised and ordinary least squares. To fit regularised regression
models, we will use the <strong><code>glmnet</code></strong>
package.</p>
<div class="codewrapper sourceCode" id="cb42">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"glmnet"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loading required package: Matrix</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
Attaching package: 'Matrix'</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The following object is masked from 'package:S4Vectors':

    expand</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Loaded glmnet 4.1-8</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb47">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## glmnet() performs scaling by default, supply un-scaled data:</span></span>
<span><span class="va">horvath_mat</span> <span class="op">&lt;-</span> <span class="va">methyl_mat</span><span class="op">[</span>, <span class="va">cpg_markers</span><span class="op">]</span> <span class="co"># select the same 20 sites as before</span></span>
<span><span class="va">train_mat</span> <span class="op">&lt;-</span> <span class="va">horvath_mat</span><span class="op">[</span><span class="va">train_ind</span>, <span class="op">]</span> <span class="co"># use the same individuals as selected before</span></span>
<span><span class="va">test_mat</span> <span class="op">&lt;-</span> <span class="va">horvath_mat</span><span class="op">[</span><span class="op">-</span><span class="va">train_ind</span>, <span class="op">]</span></span>
<span></span>
<span><span class="va">ridge_fit</span> <span class="op">&lt;-</span> <span class="fu">glmnet</span><span class="op">(</span>x <span class="op">=</span> <span class="va">train_mat</span>, y <span class="op">=</span> <span class="va">train_age</span>, alpha <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">ridge_fit</span>, xvar <span class="op">=</span> <span class="st">"lambda"</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>h <span class="op">=</span> <span class="fl">0</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-plot-ridge-1.png" alt="A line plot of coefficient estimates against log lambda for a ridge regression model. Lines are depicted in different colours, with coefficients generally having large values on the left of the plot (small log lambda) and moving smoothly and gradually towards zero to the right of the plot (large log lambda). Some coefficients appear to increase and then decrease in magnitude as lambda increases, or switch signs." class="figure mx-auto d-block"><figcaption>
A line plot of coefficient estimates against log lambda for a ridge
regression model.
</figcaption></figure><p>This plot shows how the estimated coefficients for each CpG site
change as we increase the penalty, <span class="math inline">\(\lambda\)</span>. That is, as we decrease the size
of the region that solutions can fall into, the values of the
coefficients that we get back tend to decrease. In this case,
coefficients tend towards zero but generally don’t reach it until the
penalty gets very large. We can see that initially, some parameter
estimates are really, really large, and these tend to shrink fairly
rapidly.</p>
<p>We can also notice that some parameters “flip signs”; that is, they
start off positive and become negative as lambda grows. This is a sign
of collinearity, or correlated predictors. As we reduce the importance
of one feature, we can “make up for” the loss in accuracy from that one
feature by adding a bit of weight to another feature that represents
similar information.</p>
<p>Since we split the data into test and training data, we can prove
that ridge regression predicts the test data better than the model with
no regularisation. Let’s generate our predictions under the ridge
regression model and calculate the mean squared error in the test
set:</p>
<div class="codewrapper sourceCode" id="cb48">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Obtain a matrix of predictions from the ridge model,</span></span>
<span><span class="co"># where each column corresponds to a different lambda value</span></span>
<span><span class="va">pred_ridge</span> <span class="op">&lt;-</span> <span class="fu">predict</span><span class="op">(</span><span class="va">ridge_fit</span>, newx <span class="op">=</span> <span class="va">test_mat</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calculate MSE for every column of the prediction matrix against the vector of true ages</span></span>
<span><span class="va">err_ridge</span> <span class="op">&lt;-</span> <span class="fu">apply</span><span class="op">(</span><span class="va">pred_ridge</span>, <span class="fl">2</span>, <span class="kw">function</span><span class="op">(</span><span class="va">col</span><span class="op">)</span> <span class="fu">mse</span><span class="op">(</span><span class="va">test_age</span>, <span class="va">col</span><span class="op">)</span><span class="op">)</span> </span>
<span><span class="va">min_err_ridge</span> <span class="op">&lt;-</span> <span class="fu">min</span><span class="op">(</span><span class="va">err_ridge</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Identify the lambda value that results in the lowest MSE (ie, the "best" lambda value)</span></span>
<span><span class="va">which_min_err</span> <span class="op">&lt;-</span> <span class="fu">which.min</span><span class="op">(</span><span class="va">err_ridge</span><span class="op">)</span></span>
<span><span class="va">pred_min_ridge</span> <span class="op">&lt;-</span> <span class="va">pred_ridge</span><span class="op">[</span>, <span class="va">which_min_err</span><span class="op">]</span></span>
<span></span>
<span><span class="co">## Return errors</span></span>
<span><span class="va">min_err_ridge</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 46.76802</code></pre>
</div>
<p>This is much lower than the test error for the model without
regularisation:</p>
<div class="codewrapper sourceCode" id="cb50">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">err_lm</span>  </span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 223.3571</code></pre>
</div>
<p>We can see where on the continuum of lambdas we’ve picked a model by
plotting the coefficient paths again. In this case, we’ve picked a model
with fairly modest coefficient shrinking.</p>
<div class="codewrapper sourceCode" id="cb52">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">chosen_lambda</span> <span class="op">&lt;-</span> <span class="va">ridge_fit</span><span class="op">$</span><span class="va">lambda</span><span class="op">[</span><span class="fu">which.min</span><span class="op">(</span><span class="va">err_ridge</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">ridge_fit</span>, xvar <span class="op">=</span> <span class="st">"lambda"</span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>v <span class="op">=</span> <span class="fu">log</span><span class="op">(</span><span class="va">chosen_lambda</span><span class="op">)</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-chooselambda-1.png" alt="A line plot of coefficient estimates against log lambda for a ridge regression model. A dashed vertical line depicts the optimal lambda value towards the left of the plot. Lines are depicted in different colours, with coefficients generally having large values on the left of the plot (small log lambda) and moving smoothly and gradually towards zero to the right of the plot (large log lambda). Some coefficients appear to increase and then decrease in magnitude as lambda increases, or switch signs." class="figure mx-auto d-block"><figcaption>
A line plot of coefficient estimates against log lambda for a ridge
regression model, showing the optimal value based on the minimal test
error.
</figcaption></figure><div id="challenge-4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-4" class="callout-inner">
<h3 class="callout-title">Challenge 4</h3>
<div class="callout-content">
<ol style="list-style-type: decimal"><li>Which performs better, ridge or OLS?</li>
<li>Plot predicted ages for each method against the true ages. How do
the predictions look for both methods? Why might ridge be performing
better?</li>
</ol></div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4"> Show me the solution </h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" aria-labelledby="headingSolution4" data-bs-parent="#accordionSolution4">
<div class="accordion-body">
<ol style="list-style-type: decimal"><li>Ridge regression performs significantly better on unseen data,
despite being “worse” on the training data.</li>
</ol><div class="codewrapper sourceCode" id="cb53">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">min_err_ridge</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 46.76802</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb55">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">err_lm</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 223.3571</code></pre>
</div>
<ol start="2" style="list-style-type: decimal"><li>The ridge ones are much less spread out with far fewer extreme
predictions.</li>
</ol><div class="codewrapper sourceCode" id="cb57">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">all</span> <span class="op">&lt;-</span> <span class="fu">c</span><span class="op">(</span><span class="va">pred_lm</span>, <span class="va">test_age</span>, <span class="va">pred_min_ridge</span><span class="op">)</span></span>
<span><span class="va">lims</span> <span class="op">&lt;-</span> <span class="fu">range</span><span class="op">(</span><span class="va">all</span><span class="op">)</span></span>
<span><span class="fu">par</span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">test_age</span>, <span class="va">pred_lm</span>,</span>
<span>    xlim <span class="op">=</span> <span class="va">lims</span>, ylim <span class="op">=</span> <span class="va">lims</span>,</span>
<span>    pch <span class="op">=</span> <span class="fl">19</span></span>
<span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>coef <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">test_age</span>, <span class="va">pred_min_ridge</span>,</span>
<span>    xlim <span class="op">=</span> <span class="va">lims</span>, ylim <span class="op">=</span> <span class="va">lims</span>,</span>
<span>    pch <span class="op">=</span> <span class="fl">19</span></span>
<span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span>coef <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-plot-ridge-prediction-1.png" alt="Two plots showing OLS predictions and ridge regression predictions of age (y) against true age (x). A dashed line shows the line y=x. In the OLS plot, predictions are quite extreme, while in the ridge regression plot, they are generally more conservative." class="figure mx-auto d-block"><figcaption>
Two plots showing OLS predictions (left) and ridge regression
predictions (right) of age (y) against true age (x).
</figcaption></figure></div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="lasso-regression">LASSO regression<a class="anchor" aria-label="anchor" href="#lasso-regression"></a></h2>
<hr class="half-width"><p><em>LASSO</em> is another type of regularisation. In this case we use
the <span class="math inline">\(L^1\)</span> norm, or the sum of the
absolute values of the coefficients.</p>
<p><span class="math display">\[
\left\lVert \beta \right\lVert_1 = \sum_{j=1}^p |\beta_j|
\]</span></p>
<p>This tends to produce sparse models; that is to say, it tends to
remove features from the model that aren’t necessary to produce accurate
predictions. This is because the region we’re restricting the
coefficients to has sharp edges. So, when we increase the penalty
(reduce the norm), it’s more likely that the best solution that falls in
this region will be at the corner of this diagonal (i.e., one or more
coefficient is exactly zero).</p>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-shrink-lasso-1.png" alt="For each observation, the left panel shows the residuals with respect to the optimal lines obtained with and without regularisation. Right panel shows the sum of squared residuals across all possible linear regression models. Regularisation moves the line away from the optimal (in terms of minimising the sum of squared residuals)" class="figure mx-auto d-block"><figcaption>
Illustrative example demonstrated how regression coefficients are
inferred under a linear model framework, with (blue line) and without
(red line) regularisation. A LASSO penalty is used in this example.
</figcaption></figure><div id="challenge-5" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-5" class="callout-inner">
<h3 class="callout-title">Challenge 5</h3>
<div class="callout-content">
<ol style="list-style-type: decimal"><li>Use <code>glmnet()</code> to fit a LASSO model (hint: set
<code>alpha = 1</code>).</li>
<li>Plot the model object. Remember that for ridge regression, we set
<code>xvar = "lambda"</code>. What if you don’t set this? What’s the
relationship between the two plots?</li>
<li>How do the coefficient paths differ to the ridge case?</li>
</ol></div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5"> Show me the solution </h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" aria-labelledby="headingSolution5" data-bs-parent="#accordionSolution5">
<div class="accordion-body">
<ol style="list-style-type: decimal"><li>Fitting a LASSO model is very similar to a ridge model, we just need
to change the <code>alpha</code> setting.</li>
</ol><div class="codewrapper sourceCode" id="cb58">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit_lasso</span> <span class="op">&lt;-</span> <span class="fu">glmnet</span><span class="op">(</span>x <span class="op">=</span> <span class="va">methyl_mat</span>, y <span class="op">=</span> <span class="va">age</span>, alpha <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre>
</div>
<ol start="2" style="list-style-type: decimal"><li>When <code>xvar = "lambda"</code>, the x-axis represents increasing
model sparsity from left to right, because increasing lambda increases
the penalty added to the coefficients. When we instead plot the L1 norm
on the x-axis, increasing L1 norm means that we are allowing our
coefficients to take on increasingly large values.</li>
</ol><div class="codewrapper sourceCode" id="cb59">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">par</span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">fit_lasso</span>, xvar <span class="op">=</span> <span class="st">"lambda"</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">fit_lasso</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-plotlas-1.png" alt="Two line plots side-by-side, showing coefficient values from a LASSO model against log lambda (left) and L1 norm (right). The coefficients, generally, suddenly become zero as log lambda increases or, equivalently, L1 norm decreases. However, some coefficients increase in size before decreasing as log lamdba increases." class="figure mx-auto d-block"><figcaption>
Line plots showing coefficient values from a LASSO model against log
lambda (left) and L1 norm (right).
</figcaption></figure><ol start="3" style="list-style-type: decimal"><li>When using LASSO, the paths tend to go to exactly zero much more as
the penalty ($ \lambda $) increases. In the ridge case, the paths tend
towards zero but less commonly reach exactly zero.</li>
</ol></div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="cross-validation-to-find-the-best-value-of-lambda">Cross-validation to find the best value of <span class="math inline">\(\lambda\)</span>
<a class="anchor" aria-label="anchor" href="#cross-validation-to-find-the-best-value-of-lambda"></a></h2>
<hr class="half-width"><p>Ideally, we want <span class="math inline">\(\lambda\)</span> to be
large enough to reduce the complexity of our model, thus reducing the
number of and correlations between the features, and improving
generalisability. However, we don’t want the value of <span class="math inline">\(\lambda\)</span> to be so large that we lose a lot
of the valuable information in the features.</p>
<p>Various methods can be used to balance this trade-off and thus select
the “best” value for <span class="math inline">\(\lambda\)</span>. One
method splits the data into <span class="math inline">\(K\)</span>
chunks. We then use <span class="math inline">\(K-1\)</span> of these as
the training set, and the remaining <span class="math inline">\(1\)</span> chunk as the test set. We can repeat
this until we’ve rotated through all <span class="math inline">\(K\)</span> chunks, giving us a good estimate of
how well each of the lambda values work in our data. This is called
cross-validation, and doing this repeated test/train split gives us a
better estimate of how generalisable our model is.</p>
<figure style="text-align: center"><img src="../fig/cross_validation.png" alt="The data is divided into $K$ chunks. For each cross-validation iteration, one data chunk is used as the test set. The remaining $K-1$ chunks are combined into a training set." width="695" class="figure mx-auto d-block"><figcaption>
Schematic representiation of a <span class="math inline">\(K\)</span>-fold cross-validation procedure.
</figcaption></figure><p>We can use this new idea to choose a lambda value by finding the
lambda that minimises the error across each of the test and training
splits. In R:</p>
<div class="codewrapper sourceCode" id="cb60">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># fit lasso model with cross-validation across a range of lambda values</span></span>
<span><span class="va">lasso</span> <span class="op">&lt;-</span> <span class="fu">cv.glmnet</span><span class="op">(</span><span class="va">methyl_mat</span>, <span class="va">age</span>, alpha <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">lasso</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-lasso-cv-1.png" alt="Alt" class="figure mx-auto d-block"><figcaption>
Cross-validated mean squared error for different values of lambda under
a LASSO penalty.
</figcaption></figure><div class="codewrapper sourceCode" id="cb61">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Extract the coefficients from the model with the lowest mean squared error from cross-validation</span></span>
<span><span class="va">coefl</span> <span class="op">&lt;-</span> <span class="fu">coef</span><span class="op">(</span><span class="va">lasso</span>, <span class="va">lasso</span><span class="op">$</span><span class="va">lambda.min</span><span class="op">)</span></span>
<span><span class="co"># select only non-zero coefficients</span></span>
<span><span class="va">selection</span> <span class="op">&lt;-</span> <span class="fu">which</span><span class="op">(</span><span class="va">coefl</span> <span class="op">!=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="co"># and convert to a normal matrix</span></span>
<span><span class="va">selected_coefs</span> <span class="op">&lt;-</span> <span class="fu">as.matrix</span><span class="op">(</span><span class="va">coefl</span><span class="op">)</span><span class="op">[</span><span class="va">selection</span>, <span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">selected_coefs</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>  (Intercept)    cg02388150    cg06493994    cg22449114    cg22736354
-8.4133296328  0.6966503392  0.1615535465  6.4255580409 12.0507794749
   cg03330058    cg09809672    cg11299964    cg19761273    cg26162695
-0.0002362055 -0.7487594618 -2.0399663416 -5.2538055304 -0.4486970332
   cg09502015    cg24771684    cg08446924    cg13215762    cg24549277
 1.0787003366  4.5743800395 -0.5960137381  0.1481402638  0.6290915767
   cg12304482    cg13131095    cg17962089    cg13842639    cg04080666
-1.0167896196  2.8860222552  6.3065284096  0.1590465147  2.4889065761
   cg06147194    cg03669936    cg14230040    cg19848924    cg23964682
-0.6838637838 -0.0352696698  0.1280760909 -0.0006938337  1.3378854603
   cg13578928    cg02745847    cg17410295    cg17459023    cg06223736
-0.8601170264  2.2346315955 -2.3008028295  0.0370389967  1.6158734083
   cg06717750    cg20138604    cg12851161    cg20972027    cg23878422
 2.3401693309  0.0084327521 -3.3033355652  0.2442751682  1.1059030593
   cg16612298    cg03762081    cg14428146    cg16908369    cg16271524
 0.0050053190 -6.5228858163  0.3167227488  0.2302773154 -1.3787104336
   cg22071651    cg04262805    cg24969251    cg11233105    cg03156032
 0.3480551279  1.1841804186  8.3024629942  0.6130598151 -1.1121959544 </code></pre>
</div>
<p>We can see that cross-validation has selected a value of <span class="math inline">\(\lambda\)</span> resulting in 44 features and the
intercept.</p>
</section><section><h2 class="section-heading" id="elastic-nets-blending-ridge-regression-and-the-lasso">Elastic nets: blending ridge regression and the LASSO<a class="anchor" aria-label="anchor" href="#elastic-nets-blending-ridge-regression-and-the-lasso"></a></h2>
<hr class="half-width"><p>So far, we’ve used ridge regression (where <code>alpha = 0</code>)
and LASSO regression, (where <code>alpha = 1</code>). What if
<code>alpha</code> is set to a value between zero and one? Well, this
actually lets us blend the properties of ridge and LASSO regression.
This allows us to have the nice properties of the LASSO, where
uninformative variables are dropped automatically, and the nice
properties of ridge regression, where our coefficient estimates are a
bit more conservative, and as a result our predictions are a bit
better.</p>
<p>Formally, the objective function of elastic net regression is to
optimise the following function:</p>
<p><span class="math display">\[
\left(\sum_{i=1}^N y_i - x'_i\beta\right)
+ \lambda \left(
\alpha \left\lVert \beta \right\lVert_1 +
(1-\alpha)  \left\lVert \beta \right\lVert_2
\right)
\]</span></p>
<p>You can see that if <code>alpha = 1</code>, then the L1 norm term is
multiplied by one, and the L2 norm is multiplied by zero. This means we
have pure LASSO regression. Conversely, if <code>alpha = 0</code>, the
L2 norm term is multiplied by one, and the L1 norm is multiplied by
zero, meaning we have pure ridge regression. Anything in between gives
us something in between.</p>
<p>The contour plots we looked at previously to visualise what this
penalty looks like for different values of <code>alpha</code>.</p>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-elastic-contour-1.png" alt="For lower values of alpha, the penalty resembles ridge regression. For higher values of alpha, the penalty resembles LASSO regression." class="figure mx-auto d-block"><figcaption>
For an elastic net, the panels show the effect of the regularisation
across different values of alpha
</figcaption></figure><div id="challenge-6" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-6" class="callout-inner">
<h3 class="callout-title">Challenge 6</h3>
<div class="callout-content">
<ol style="list-style-type: decimal"><li>Fit an elastic net model (hint: alpha = 0.5) without
cross-validation and plot the model object.</li>
<li>Fit an elastic net model with cross-validation and plot the error.
Compare with LASSO.</li>
<li>Select the lambda within one standard error of the minimum
cross-validation error (hint: <code>lambda.1se</code>). Compare the
coefficients with the LASSO model.</li>
<li>Discuss: how could we pick an <code>alpha</code> in the range (0,
1)? Could we justify choosing one <em>a priori</em>?</li>
</ol></div>
</div>
</div>
<div id="accordionSolution6" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution6" aria-expanded="false" aria-controls="collapseSolution6">
  <h4 class="accordion-header" id="headingSolution6"> Show me the solution </h4>
</button>
<div id="collapseSolution6" class="accordion-collapse collapse" aria-labelledby="headingSolution6" data-bs-parent="#accordionSolution6">
<div class="accordion-body">
<ol style="list-style-type: decimal"><li>Fitting an elastic net model is just like fitting a LASSO model. You
can see that coefficients tend to go exactly to zero, but the paths are
a bit less extreme than with pure LASSO; similar to ridge.</li>
</ol><div class="codewrapper sourceCode" id="cb63">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">elastic</span> <span class="op">&lt;-</span> <span class="fu">glmnet</span><span class="op">(</span><span class="va">methyl_mat</span><span class="op">[</span>, <span class="op">-</span><span class="fl">1</span><span class="op">]</span>, <span class="va">age</span>, alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">elastic</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-elastic-1.png" alt="A line plot showing coefficient values from an elastic net model against L1 norm. The coefficients, generally, suddenly become zero as L1 norm decreases. However, some coefficients increase in size before decreasing as L1 norm decreases." class="figure mx-auto d-block"><figcaption>
Line plot showing coefficient values from an elastic net model against
L1 norm.
</figcaption></figure><ol start="2" style="list-style-type: decimal"><li>The process of model selection is similar for elastic net models as
for LASSO models.</li>
</ol><div class="codewrapper sourceCode" id="cb64">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">elastic_cv</span> <span class="op">&lt;-</span> <span class="fu">cv.glmnet</span><span class="op">(</span><span class="va">methyl_mat</span><span class="op">[</span>, <span class="op">-</span><span class="fl">1</span><span class="op">]</span>, <span class="va">age</span>, alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="fu">plot</span><span class="op">(</span><span class="va">elastic_cv</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-elastic-cv-1.png" alt="A plot of the cross-validation mean squared error of an elastic net model against log lambda." class="figure mx-auto d-block"><figcaption>
A plot of the cross-validation mean squared error of an elastic net
model against log lambda.
</figcaption></figure><ol start="3" style="list-style-type: decimal"><li>You can see that the coefficients from these two methods are broadly
similar, but the elastic net coefficients are a bit more conservative.
Further, more coefficients are exactly zero in the LASSO model.</li>
</ol><div class="codewrapper sourceCode" id="cb65">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">coefe</span> <span class="op">&lt;-</span> <span class="fu">coef</span><span class="op">(</span><span class="va">elastic_cv</span>, <span class="va">elastic_cv</span><span class="op">$</span><span class="va">lambda.1se</span><span class="op">)</span></span>
<span><span class="fu">sum</span><span class="op">(</span><span class="va">coefe</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 4973</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb67">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">sum</span><span class="op">(</span><span class="va">coefl</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 4956</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb69">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">plot</span><span class="op">(</span></span>
<span>    <span class="va">coefl</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span>, <span class="va">coefe</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span>,</span>
<span>    pch <span class="op">=</span> <span class="fl">16</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"LASSO coefficients"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Elastic net coefficients"</span></span>
<span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in xy.coords(x, y, xlabel, ylabel, log): 'x' and 'y' lengths differ</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb71">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">abline</span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">ERROR<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="error" tabindex="0"><code>Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet</code></pre>
</div>
<ol start="4" style="list-style-type: decimal"><li>You could pick an arbitrary value of <code>alpha</code>, because
arguably pure ridge regression or pure LASSO regression are also
arbitrary model choices. To be rigorous and to get the best-performing
model and the best inference about predictors, it’s usually best to find
the best combination of <code>alpha</code> and <code>lambda</code> using
a grid search approach in cross-validation. However, this can be very
computationally demanding.</li>
</ol></div>
</div>
</div>
</div>
<div id="the-bias-variance-tradeoff" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="the-bias-variance-tradeoff" class="callout-inner">
<h3 class="callout-title">The bias-variance tradeoff</h3>
<div class="callout-content">
<p>When we make predictions in statistics, there are two sources of
error that primarily influence the (in)accuracy of our predictions.
these are <em>bias</em> and <em>variance</em>.</p>
<p>The total expected error in our predictions is given by the following
equation:</p>
<p><span class="math display">\[
E(y - \hat{y}) = \text{Bias}^2 + \text{Variance} + \sigma^2
\]</span></p>
<p>Here, <span class="math inline">\(\sigma^2\)</span> represents the
irreducible error, that we can never overcome. Bias results from
erroneous assumptions in the model used for predictions. Fundamentally,
bias means that our model is mis-specified in some way, and fails to
capture some components of the data-generating process (which is true of
all models). If we have failed to account for a confounding factor that
leads to very inaccurate predictions in a subgroup of our population,
then our model has high bias.</p>
<p>Variance results from sensitivity to particular properties of the
input data. For example, if a tiny change to the input data would result
in a huge change to our predictions, then our model has high
variance.</p>
<p>Linear regression is an unbiased model under certain conditions. In
fact, the <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem" class="external-link">Gauss-Markov
theorem</a> shows that under the right conditions, OLS is the best
possible type of unbiased linear model.</p>
<p>Introducing penalties to means that our model is no longer unbiased,
meaning that the coefficients estimated from our data will
systematically deviate from the ground truth. Why would we do this? As
we saw, the total error is a function of bias and variance. By accepting
a small amount of bias, it’s possible to achieve huge reductions in the
total expected error.</p>
<p>This bias-variance tradeoff is also why people often favour elastic
net regression over pure LASSO regression.</p>
</div>
</div>
</div>
<div id="other-types-of-outcomes" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="other-types-of-outcomes" class="callout-inner">
<h3 class="callout-title">Other types of outcomes</h3>
<div class="callout-content">
<p>You may have noticed that <code>glmnet</code> is written as
<code>glm</code>, not <code>lm</code>. This means we can actually model
a variety of different outcomes using this regularisation approach. For
example, we can model binary variables using logistic regression, as
shown below. The type of outcome can be specified using the
<code>family</code> argument, which specifies the family of the outcome
variable.</p>
<p>In fact, <code>glmnet</code> is somewhat cheeky as it also allows you
to model survival using Cox proportional hazards models, which aren’t
GLMs, strictly speaking.</p>
<p>For example, in the current dataset we can model smoking status as a
binary variable in logistic regression by setting
<code>family = "binomial"</code>.</p>
<p>The <a href="https://glmnet.stanford.edu/articles/glmnet.html" class="external-link">package
documentation</a> explains this in more detail.</p>
<div class="codewrapper sourceCode" id="cb73">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">smoking</span> <span class="op">&lt;-</span> <span class="fu">as.numeric</span><span class="op">(</span><span class="fu">factor</span><span class="op">(</span><span class="va">methylation</span><span class="op">$</span><span class="va">smoker</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span></span>
<span><span class="co"># binary outcome</span></span>
<span><span class="fu">table</span><span class="op">(</span><span class="va">smoking</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>smoking
 0  1
30  7 </code></pre>
</div>
<div class="codewrapper sourceCode" id="cb75">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu">cv.glmnet</span><span class="op">(</span>x <span class="op">=</span> <span class="va">methyl_mat</span>, nfolds <span class="op">=</span> <span class="fl">5</span>, y <span class="op">=</span> <span class="va">smoking</span>, family <span class="op">=</span> <span class="st">"binomial"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb77">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">coef</span> <span class="op">&lt;-</span> <span class="fu">coef</span><span class="op">(</span><span class="va">fit</span>, s <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">lambda.min</span><span class="op">)</span></span>
<span><span class="va">coef</span> <span class="op">&lt;-</span> <span class="fu">as.matrix</span><span class="op">(</span><span class="va">coef</span><span class="op">)</span></span>
<span><span class="va">coef</span><span class="op">[</span><span class="fu">which</span><span class="op">(</span><span class="va">coef</span> <span class="op">!=</span> <span class="fl">0</span><span class="op">)</span>, <span class="fl">1</span><span class="op">]</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] -1.455287</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb79">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">plot</span><span class="op">(</span><span class="va">fit</span><span class="op">)</span></span></code></pre>
</div>
<figure style="text-align: center"><img src="../fig/03-regression-regularisation-rendered-binomial-1.png" alt="A plot of the cross-validation binomial deviance of a logistic regression elastic net model against log lambda." class="figure mx-auto d-block"><figcaption>
A plot of the cross-validation binomial deviance of a logistic
regression elastic net model against log lambda.
</figcaption></figure><p>In this case, the results aren’t very interesting! We select an
intercept-only model. However, as highlighted by the warnings above, we
should not trust this result too much as the data was too small to
obtain reliable results! We only included it here to provide the code
that <em>could</em> be used to perform penalised regression for binary
outcomes (i.e. penalised logistic regression).</p>
</div>
</div>
</div>
<div id="tidymodels" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="tidymodels" class="callout-inner">
<h3 class="callout-title">tidymodels</h3>
<div class="callout-content">
<p>A lot of the packages for fitting predictive models like regularised
regression have different user interfaces. To do predictive modelling,
it’s important to consider things like choosing a good performance
metric and how to run normalisation. It’s also useful to compare
different model “engines”.</p>
<p>To this end, the <strong><code>tidymodels</code></strong> R framework
exists. We’re not doing a course on advanced topics in predictive
modelling so we are not covering this framework in detail. However, the
code below would be useful to perform repeated cross-validation. More
information about <strong><code>tidymodels</code></strong>, including
installation instructions, can be found <a href="https://www.tidymodels.org/" class="external-link">on the tidymodels website</a>.</p>
<div class="codewrapper sourceCode" id="cb80">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">library</span><span class="op">(</span><span class="st">"tidymodels"</span><span class="op">)</span></span>
<span><span class="va">all_data</span> <span class="op">&lt;-</span> <span class="fu">as.data.frame</span><span class="op">(</span><span class="fu">cbind</span><span class="op">(</span>age <span class="op">=</span> <span class="va">age</span>, <span class="va">methyl_mat</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">split_data</span> <span class="op">&lt;-</span> <span class="fu">initial_split</span><span class="op">(</span><span class="va">all_data</span><span class="op">)</span></span>
<span></span>
<span><span class="va">norm_recipe</span> <span class="op">&lt;-</span> <span class="fu">recipe</span><span class="op">(</span><span class="fu">training</span><span class="op">(</span><span class="va">split_data</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="co">## everything other than age is a predictor</span></span>
<span>    <span class="fu">update_role</span><span class="op">(</span><span class="fu">everything</span><span class="op">(</span><span class="op">)</span>, new_role <span class="op">=</span> <span class="st">"predictor"</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">update_role</span><span class="op">(</span><span class="va">age</span>, new_role <span class="op">=</span> <span class="st">"outcome"</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="co">## center and scale all the predictors</span></span>
<span>    <span class="fu">step_center</span><span class="op">(</span><span class="fu">all_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">step_scale</span><span class="op">(</span><span class="fu">all_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> </span>
<span></span>
<span><span class="co">## set the "engine" to be a linear model with tunable alpha and lambda</span></span>
<span><span class="va">glmnet_model</span> <span class="op">&lt;-</span> <span class="fu">linear_reg</span><span class="op">(</span>penalty <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span><span class="op">)</span>, mixture <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">set_engine</span><span class="op">(</span><span class="st">"glmnet"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## define a workflow, with normalisation recipe into glmnet engine</span></span>
<span><span class="va">workflow</span> <span class="op">&lt;-</span> <span class="fu">workflow</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">add_recipe</span><span class="op">(</span><span class="va">norm_recipe</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">add_model</span><span class="op">(</span><span class="va">glmnet_model</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## 5-fold cross-validation repeated 5 times</span></span>
<span><span class="va">folds</span> <span class="op">&lt;-</span> <span class="fu">vfold_cv</span><span class="op">(</span><span class="fu">training</span><span class="op">(</span><span class="va">split_data</span><span class="op">)</span>, v <span class="op">=</span> <span class="fl">5</span>, repeats <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## define a grid of lambda and alpha parameters to search</span></span>
<span><span class="va">glmn_set</span> <span class="op">&lt;-</span> <span class="fu">parameters</span><span class="op">(</span></span>
<span>    <span class="fu">penalty</span><span class="op">(</span>range <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="op">-</span><span class="fl">5</span>, <span class="fl">1</span><span class="op">)</span>, trans <span class="op">=</span> <span class="fu">log10_trans</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    <span class="fu">mixture</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">glmn_grid</span> <span class="op">&lt;-</span> <span class="fu">grid_regular</span><span class="op">(</span><span class="va">glmn_set</span><span class="op">)</span></span>
<span><span class="va">ctrl</span> <span class="op">&lt;-</span> <span class="fu">control_grid</span><span class="op">(</span>save_pred <span class="op">=</span> <span class="cn">TRUE</span>, verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## use the metric "rmse" (root mean squared error) to grid search for the</span></span>
<span><span class="co">## best model</span></span>
<span><span class="va">results</span> <span class="op">&lt;-</span> <span class="va">workflow</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">tune_grid</span><span class="op">(</span></span>
<span>        resamples <span class="op">=</span> <span class="va">folds</span>,</span>
<span>        metrics <span class="op">=</span> <span class="fu">metric_set</span><span class="op">(</span><span class="va">rmse</span><span class="op">)</span>,</span>
<span>        control <span class="op">=</span> <span class="va">ctrl</span></span>
<span>    <span class="op">)</span></span>
<span><span class="co">## select the best model based on RMSE</span></span>
<span><span class="va">best_mod</span> <span class="op">&lt;-</span> <span class="va">results</span> <span class="op">%&gt;%</span> <span class="fu">select_best</span><span class="op">(</span><span class="st">"rmse"</span><span class="op">)</span></span>
<span><span class="va">best_mod</span></span>
<span><span class="co">## finalise the workflow and fit it with all of the training data</span></span>
<span><span class="va">final_workflow</span> <span class="op">&lt;-</span> <span class="fu">finalize_workflow</span><span class="op">(</span><span class="va">workflow</span>, <span class="va">best_mod</span><span class="op">)</span></span>
<span><span class="va">final_workflow</span></span>
<span><span class="va">final_model</span> <span class="op">&lt;-</span> <span class="va">final_workflow</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">fit</span><span class="op">(</span>data <span class="op">=</span> <span class="fu">training</span><span class="op">(</span><span class="va">split_data</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## plot predicted age against true age for test data</span></span>
<span><span class="fu">plot</span><span class="op">(</span></span>
<span>    <span class="fu">testing</span><span class="op">(</span><span class="va">split_data</span><span class="op">)</span><span class="op">$</span><span class="va">age</span>,</span>
<span>    <span class="fu">predict</span><span class="op">(</span><span class="va">final_model</span>, new_data <span class="op">=</span> <span class="fu">testing</span><span class="op">(</span><span class="va">split_data</span><span class="op">)</span><span class="op">)</span><span class="op">$</span><span class="va">.pred</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"True age"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Predicted age"</span>,</span>
<span>    pch <span class="op">=</span> <span class="fl">16</span>,</span>
<span>    log <span class="op">=</span> <span class="st">"xy"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu">abline</span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></span></code></pre>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="further-reading">Further reading<a class="anchor" aria-label="anchor" href="#further-reading"></a></h2>
<hr class="half-width"><ul><li>
<a href="https://www.statlearning.com/" class="external-link">An introduction to
statistical learning</a>.</li>
<li>
<a href="https://web.stanford.edu/~hastie/ElemStatLearn/" class="external-link">Elements
of statistical learning</a>.</li>
<li>
<a href="https://glmnet.stanford.edu/articles/glmnet.html" class="external-link">glmnet
vignette</a>.</li>
<li>
<a href="https://www.tidymodels.org/" class="external-link">tidymodels</a>.</li>
</ul></section><section><h2 class="section-heading" id="footnotes">Footnotes<a class="anchor" aria-label="anchor" href="#footnotes"></a></h2>
<hr class="half-width"><div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul><li>Regularisation is a way to fit a model, get better estimates of
effect sizes, and perform variable selection simultaneously.</li>
<li>Test and training splits, or cross-validation, are a useful way to
select models or hyperparameters.</li>
<li>Regularisation can give us a more predictive set of variables, and
by restricting the magnitude of coefficients, can give us a better (and
more stable) estimate of our outcome.</li>
<li>Regularisation is often <em>very</em> fast! Compared to other
methods for variable selection, it is very efficient. This makes it
easier to practice rigorous variable selection.</li>
</ul></div>
</div>
</div>
</section><div class="footnotes footnotes-end-of-document">
<hr><ol><li id="fn1"><p>Model selection including <span class="math inline">\(R^2\)</span>, AIC and BIC are covered in the
additional feature selection for regression episode of this course.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0014821" class="external-link">Epigenetic
Predictor of Age, Bocklandt et al. (2011)</a><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol></div>



      </div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/02-high-dimensional-regression.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/04-principal-component-analysis.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/02-high-dimensional-regression.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Regression with many
        </a>
        <a class="chapter-link float-end" href="../instructor/04-principal-component-analysis.html" rel="next">
          Next: Principal component...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/edit/main/episodes/03-regression-regularisation.Rmd" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries-incubator/high-dimensional-stats-r/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:alan.ocallaghan@outlook.com">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.11" class="external-link">sandpaper (0.16.11)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.5" class="external-link">varnish (1.0.5)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://carpentries-incubator.github.io/high-dimensional-stats-r/instructor/03-regression-regularisation.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "Regularised regression",
  "creativeWorkStatus": "active",
  "url": "https://carpentries-incubator.github.io/high-dimensional-stats-r/instructor/03-regression-regularisation.html",
  "identifier": "https://carpentries-incubator.github.io/high-dimensional-stats-r/instructor/03-regression-regularisation.html",
  "dateCreated": "2025-03-04",
  "dateModified": "2025-04-01",
  "datePublished": "2025-04-01"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo --><script>
          var _paq = window._paq = window._paq || [];
          /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
          _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
          _paq.push(["setDomains", ["*.lessons.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
          _paq.push(["setDoNotTrack", true]);
          _paq.push(["disableCookies"]);
          _paq.push(["trackPageView"]);
          _paq.push(["enableLinkTracking"]);
          (function() {
              var u="https://matomo.carpentries.org/";
              _paq.push(["setTrackerUrl", u+"matomo.php"]);
              _paq.push(["setSiteId", "1"]);
              var d=document, g=d.createElement("script"), s=d.getElementsByTagName("script")[0];
              g.async=true; g.src="https://matomo.carpentries.org/matomo.js"; s.parentNode.insertBefore(g,s);
          })();
        </script><!-- End Matomo Code --></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

