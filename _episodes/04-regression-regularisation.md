---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 04-regression-regularisation.md in _episodes_rmd/
title: "Regularised regression with many features"
teaching: 0
exercises: 0
questions:
- "Can we fit a model that accounts for and selects many features?"
- "How does regularisation work?"
- "What are some considerations for a regularised model?"
objectives:
- "Understand the benefits of regularised models."
- "Understand how different types of regularisation work."
- "Perform and critically analyse penalised regression."
keypoints:
- "Regularisation is a way to avoid the problems of stepwise
  or iterative model building processes."
- "Modelling features together can help to identify a subset of features
    that contribute to the outcome."
math: yes
---





~~~
library("minfi")
library("here")
if (!file.exists(here("data/methylation.rds"))) {
    source(here("data/methylation.R"))
}
methylation <- readRDS(here("data/methylation.rds"))

y <- methylation$Age
methyl_mat <- getM(methylation)
~~~
{: .language-r}

In the previous episode we covered variable selection using stepwise/best subset
selection.
These have issues with respect to computation time and efficiency.

In low noise settings and with few or strong relationships, stepwise/subset
works well. However that's often not what we're faced with in biomedicine.


# Ridge regression

When we fit a linear model, we're minimising the RSS.

$$
    \sum_{i=1}^N y_i - X\beta
$$

The idea of regularisation is to add another condition to this to control
the size of the coefficients that come out.

One idea is to control the squared sum of the coefficients, $\beta$.
This is also sometimes called the $L^2$ norm. This is defined as

$$
    |\beta|_2 = \sqrt{\sum_{j=1}^p \beta_j^2}
$$

To control this, we specify that the solution for the equation above
also has to have an $L^2$ norm smaller than a certain amount. Or, equivalently,
we try to minimise a function that includes our $L^2$ norm scaled by a 
factor that is usually written $\lambda$.

$$
    \sum_{i=1}^N y_i - X\beta + \lambda|\beta|_2
$$

> # Exercise
> 
> Run `shinystats::ridgeApp()` and play with the parameters
> 
> Questions:
> 
> > ## Solution
> > 
> {: .solution}
{: .challenge}

Now we can fit a model using ridge regression.


~~~
library("glmnet")
ridge <- glmnet(methyl_mat, age, alpha = 0)
~~~
{: .language-r}



~~~
Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'drop': object 'age' not found
~~~
{: .error}

# LASSO regression


~~~
lasso <- cv.glmnet(methyl_mat[, -1], age, alpha = 1)
~~~
{: .language-r}



~~~
Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'drop': object 'age' not found
~~~
{: .error}


~~~
## Challenge 5:
## one of these...? probably lasso
elastic <- cv.glmnet(methyl_mat[, -1], age, alpha = 0.5, intercept = FALSE)
~~~
{: .language-r}



~~~
Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'drop': object 'age' not found
~~~
{: .error}


> ## Other types of outcomes
> 
> You may have noticed that `glmnet` is written as `glm`, not `lm`.
> This means we can actually model a variety of different outcomes
> using this regularisation approach. For example, we can model binary
> variables using logistic regression, as shown below.
> 
> In fact, `glmnet` is somewhat cheeky as it also allows you to model
> survival using Cox proportional hazards models, which aren't GLMs, strictly
> speaking.
> 
> 
> ~~~
> smoking <- as.numeric(factor(norm$smoker)) - 1
> ~~~
> {: .language-r}
> 
> 
> 
> ~~~
> Error in norm$smoker: object of type 'closure' is not subsettable
> ~~~
> {: .error}
> 
> 
> 
> ~~~
> # binary outcome
> smoking
> ~~~
> {: .language-r}
> 
> 
> 
> ~~~
> Error in eval(expr, envir, enclos): object 'smoking' not found
> ~~~
> {: .error}
> 
> 
> 
> ~~~
> fit <- cv.glmnet(x = methyl_mat, y = smoking, family = "binomial")
> ~~~
> {: .language-r}
> 
> 
> 
> ~~~
> Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'drop': object 'smoking' not found
> ~~~
> {: .error}
> 
> 
> 
> ~~~
> coef <- coef(fit, s = fit$lambda.1se)
> ~~~
> {: .language-r}
> 
> 
> 
> ~~~
> Error in h(simpleError(msg, call)): error in evaluating the argument 'object' in selecting a method for function 'coef': object 'fit' not found
> ~~~
> {: .error}
> 
> 
> 
> ~~~
> coef[coef[, 1] != 0, 1]
> ~~~
> {: .language-r}
> 
> 
> 
> ~~~
> Error in h(simpleError(msg, call)): error in evaluating the argument 'i' in selecting a method for function '[': object of type 'closure' is not subsettable
> ~~~
> {: .error}
> 
> 
> 
> ~~~
> plot(smoking, methyl_mat[, names(which.max(coef[-1]))])
> ~~~
> {: .language-r}
> 
> 
> 
> ~~~
> Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'plot': object 'smoking' not found
> ~~~
> {: .error}
{: .callout}


Figure taken from [Hastie et al. (2020)](https://doi.org/10.1214/19-STS733).


~~~
knitr::include_graphics("../fig/bs_fs_lasso.png")
~~~
{: .language-r}

<img src="../fig/bs_fs_lasso.png" title="plot of chunk unnamed-chunk-7" alt="plot of chunk unnamed-chunk-7" style="display: block; margin: auto;" />


> ## Selecting hyperparameters
> 
> There are various methods to select the "best"
> value for $\lambda$. One idea is to split
> the data into $K$ chunks. We then use $K-1$ of
> these as the training set, and the remaining $1$ chunk
> as the test set. Repeating this process for each of the
> $K$ chunks produces more variability.
> 
> <img src="../fig/cross_validation.svg" title="plot of chunk unnamed-chunk-8" alt="plot of chunk unnamed-chunk-8" style="display: block; margin: auto;" />
>
> To be really rigorous, we could even repeat this *cross-validation*
> process a number of times! This is termed "repeated cross-validation".
{: .callout}


{% include links.md %}
