---
title: "K-means"
teaching: 0
exercises: 0
questions:
- "What does clustering mean?"
- "Why would we want to find clusters in data?"
- "How can we cluster data with a model?"
- "How can we cluster data without a model?"
- "How can we check if clusters are robust?"
objectives:
- "Understand and perform clustering with K-means, mixture models, and hierarchical clustering."
- "Assess clustering performance with silhouette score and bootstrapping/consensus clustering."
keypoints:
- "KP1"
math: yes
---


```{r settings, include=FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("09-")
if (!file.exists(here::here("data/scrnaseq.rds"))) {
    source(here::here("data/scrnaseq.R"))
}
```

```{r pkgs, echo=FALSE}
## preamble
suppressPackageStartupMessages({
    library("scRNAseq")
    library("mixtools")
    library("irlba")
    library("scater")
    library("scuttle")
    library("scran")
    library("Rtsne")
    library("mclust")
    library("igraph")
    library("bluster")
    library("here")
})
```

# Introduction

High-dimensional data, especially in biological settings, commonly has
many sources of heterogeneity. Some of these are stochastic variation
arising from measurement error or random differences between organisms. In
some cases, this heterogeneity arises from the presence of subgroups in the
data.

```{r data}
library("SingleCellExperiment")
scrnaseq <- readRDS(here::here("data/scrnaseq.rds"))
set.seed(42)
```

```{r dimred}
library("scater")
scrnaseq <- runPCA(scrnaseq, ncomponents = 15)
scrnaseq <- runTSNE(scrnaseq, dimred = "PCA")
```


# Choosing the correct clustering

When doing clustering, it's important to realise that data may seem to
group together randomly. It's especially important to remember that when making
plots that add extra visual aids to distinguish clusters. For example, if we
cluster data from a single 2D normal distribution and draw ellipses around the
points, it suddenly is almost visually convincing. This is a somewhat extreme
example, since there is genuinely no heterogeneity in the data, but

```{r, echo = FALSE}
set.seed(11)
library("MASS")
data <- mvrnorm(n = 200, mu = rep(1, 2), Sigma = matrix(runif(4), ncol = 2))
data <- as.data.frame(data)
colnames(data) <- c("x", "y")

data$cluster <- ifelse(
    data$y < (data$x * -0.06 + 0.9),
    # data$x < 1.5 & data$y < 1,
    "a",
    ifelse(
        data$y < 1.15,
        "b",
        "c"
    )
)
ggplot(data, aes(x, y, colour = cluster)) +
    geom_point() +
    stat_ellipse()
```

# K-means

Generalisation of mixture models to have arbitrary density, just using distance.

```{r kmeans-animation, echo = FALSE, fig.cap="Cap", fig.alt="Alt"}
set.seed(66)
true_means <- list(c(-1, 4), c(-2, -2), c(2, 2))
true_sds <- c(0.5, 1, 0.12)
n <- c(20, 40, 70)

data <- list()
for (i in 1:length(n)) {
    data[[i]] <- mvrnorm(n[[i]], mu = true_means[[i]], Sigma = diag(2) * true_sds[[i]])
}

df <- do.call(rbind, data)
df <- as.data.frame(df)
colnames(df) <- c("x", "y")
means <- list(c(1, 1), c(1.1, 1.1), c(1.2, 1.2))

label_points_kmeans <- function(x, means) {
    apply(x,
        1,
        function(i) {
            which.min(
                c(
                    sapply(means, function(j) {
                        dist(rbind(i, j))
                    })
                )
            )
        }
    )
}

iterate_kmeans <- function(x, means, sds) {
    means <- lapply(unique(components), function(i) colMeans(x[components == i, ]))
    components <- label_points_kmeans(x, means)
    list(components = components, means = means)
}

plot_kmeans <- function(x, means, components) {
    # browser()
    ggplot(x) +
        geom_point(
            aes(x, y, colour = factor(components, levels = 1:3))
        ) +
        lapply(
            seq_along(means),
            function(i) {
                m <- means[[i]]
                geom_point(
                    x = m[[1]], y = m[[2]],
                    aes(colour = factor(i, levels = 1:3)),
                    shape = 13,
                    size = 4
                )
            }
        ) +
        lapply(1:nrow(x), function(i) {
            geom_segment(
                x = means[[components[[i]]]][[1]],
                xend = x[i, 1],
                y = means[[components[[i]]]][[2]],
                yend = x[i, 2],
                alpha = 0.1,
                size = 0.2,
                aes(colour = factor(components[[i]], levels = 1:3)))
        }) +
        scale_colour_discrete(guide = "none") +
        lims(x = range(x) * 1.5)
}
components <- label_points_kmeans(df, means)
plot_kmeans(df, means, components)
out <- iterate_kmeans(df, means)
plot_kmeans(df, out$means, out$components)
```




```{r kmeans, fig.cap = "Title", fig.alt = "Alt"}
cluster <- kmeans(reducedDim(scrnaseq), centers = 7, iter.max = 1000, nstart = 100)
scrnaseq$kmeans <- as.character(cluster$cluster)

plotReducedDim(scrnaseq, "TSNE", colour_by = "kmeans")
```

> ## k-medioids (PAM)
> 
> This is like k-means but using the median instead of mean. Median is slower
> but more robust.
>
> 
{: .callout}

# Cluster robustness (silhouette)

We want to be sure that

# Consensus clustering

```{r}
library("scater")
library("pheatmap")
library("bluster")
library("viridis")


pc <- reducedDim(scrnaseq)
km_fun <- function(x) {
    kmeans(x, 5)$cluster
}
originals <- km_fun(pc)
ratios <- bootstrapStability(pc, FUN = km_fun, clusters = originals)
pheatmap(ratios,
    cluster_row = FALSE, cluster_col = FALSE,
    color = viridis::magma(100),
    breaks = seq(-1, 1, length.out = 101)
)
```


## Further reading

- https://web.stanford.edu/class/bios221/book/Chap-Clustering.html



{% include links.md %}

