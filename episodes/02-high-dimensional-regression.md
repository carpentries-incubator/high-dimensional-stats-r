---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 02-high-dimensional-regression.md in _episodes_rmd/
title: "Regression with many features"
teaching: 60
exercises: 30
questions:
- "How can we apply regression methods in a high-dimensional setting?"
- "How can we control for the fact that we do many tests?"
- "How can we benefit from the fact that we have many variables?"
objectives:
- "Perform and critically analyse high dimensional regression."
- "Perform multiple testing adjustment."
- "Understand methods for shrinkage of noise parameters in
  high-dimensional regression."
keypoints:
- "When running a lot of tests for high-dimensional data, it's important to
  correct for the number of tests performed."
- "Multiple testing correction can enable us to account for many null hypothesis
  significance tests while retaining power."
- "Multiple testing methods can be more conservative or more liberal, depending
  on our goals."
- "Sharing information between features can increase power and reduce false 
  positives."
math: yes
---




# Introduction

In high-throughput studies, it's common to have one or more 
phenotypes or groupings that we want to relate to features of 
interest (eg, gene expression, DNA methylation levels).
In general, we want to identify differences in the 
features of interest
that are related to a phenotype or grouping of our samples.
Identifying features of interest that vary along with
phenotypes or groupings can allow us to understand how
phenotypes arise or manifest.

For example, we might want to identify genes that are
expressed at a higher level in mutant mice relative
to wild-type mice to understand the effect
of a mutation on cellular phenotypes.
Alternatively, we might have
samples from a set of patients, and wish to identify
epigenetic features that are different in young patients
relative to old patients, to help us understand how aging
manifests.

Using linear regression, it's possible to identify differences
like these . However, high-dimensional data like the ones we're
working with require some special considerations.

Ideally, we want to identify cases like this, where there is a
clear difference, and we probably "don't need" statistics:

<img src="../fig/rmd-02-example1-1.png" title="An example of a strong linear association between a continuous phenotype (age) on the x-axis and a feature of interest (gene expression for a given gene) on the y-axis. A strong linear relationship with a positive slope exists between the two." alt="An example of a strong linear association between a continuous phenotype (age) on the x-axis and a feature of interest (gene expression for a given gene) on the y-axis. A strong linear relationship with a positive slope exists between the two." width="432" style="display: block; margin: auto;" />

or equivalently for a discrete covariate:

<img src="../fig/rmd-02-example2-1.png" title="An example of a strong linear association between a discrete phenotype (group) on the x-axis and a feature of interest (gene expression for a given gene) on the y-axis. The two groups clearly differ with respect to gene expression." alt="An example of a strong linear association between a discrete phenotype (group) on the x-axis and a feature of interest (gene expression for a given gene) on the y-axis. The two groups clearly differ with respect to gene expression." width="432" style="display: block; margin: auto;" />

However, often due to small differences and small sample sizes,
the problem is a bit more difficult:
<img src="../fig/rmd-02-example3-1.png" title="An example of a strong linear association between a discrete phenotype (group) on the x-axis and a feature of interest (gene expression for a given gene) on the y-axis. The two groups seem to differ with respect to gene expression, but the relationship is weak." alt="An example of a strong linear association between a discrete phenotype (group) on the x-axis and a feature of interest (gene expression for a given gene) on the y-axis. The two groups seem to differ with respect to gene expression, but the relationship is weak." width="432" style="display: block; margin: auto;" />

And, of course, we often have an awful lot of features and need
to prioritise a subset of them! We need a rigorous way to
prioritise genes for further analysis.

# Linear regression (recap)

Linear regression is a tool we can use to quantify the relationship
between two variables. In linear regression we try to estimate the mean
or expectation of our outcome variable $y$ based on the values of
the input $x$. With one variable, that amount to the equation

$$
    \mathbf{E}(y) = \beta_0 + \beta_1 x
$$

where $\beta_0$ is the difference in mean between the two variables independent
of the value of $x$, and $\beta_1$ is the change in the mean of $y$ for a unit
change in $x$.

However, around the mean for an individual point $y_i$, we expect some departure
from the mean $\mathbf{E(y)}$. As explained in the
[introduction to linear regression](https://carpentries-incubator.github.io/simple-linear-regression-public-health/05-fitAndAssumptionSLR/)
lesson, the residual $y_i - \mathbf{E(y)}$ is assumed to be normally distributed
in linear regression.[^1] Therefore for an individual data point, $y_i$, we
could also write

$$
    y = \beta_0 + \beta_1 x + \epsilon_i
$$

where $\epsilon_i$ is the residual or *noise*. Another way of saying
that this noise follows a normal distribution is to say that:

$$
    \epsilon_i \sim N(0, \sigma^2)
$$

Because we're modelling the *mean* of the response at a given value of
the predictor features, we can also write

$$
    y \sim N(\beta_0 + \beta_1 x, \sigma^2)
$$



Or, visually, that in an example linear regression, this is the distribution 
of new points conditional on their $x$ values:
<img src="../fig/rmd-02-conditionalprob-1.png" title="The generative model of a simple linear regression with a fixed slope and intercept. Lightly shaded regions represent regions where observations are probable, and darker regions represent lower probability." alt="The generative model of a simple linear regression with a fixed slope and intercept. Lightly shaded regions represent regions where observations are probable, and darker regions represent lower probability." width="432" style="display: block; margin: auto;" />

In this example of a linear regression model, we're saying that for a new
value of $x$, the corresponding value of $y$ is most likely to fall in the
bright region. The probability of observing a given $y$ value falls off quickly
towards the darker regions.



# DNA methylation data

For the following few episodes, we'll be working with human
DNA methylation data from flow-sorted blood samples.
DNA methylation assays measure, for many sites in the genome,
the proportion of DNA that carries a methyl mark.
In this case, the methylation data come in the form of a matrix
of normalised methylation levels, where negative values correspond to
unmethylated DNA and positive values correspond to methylated DNA.
Along with this, we have a number of sample phenotypes
(eg, age in years, BMI).

Let's read in the data for this episode:


~~~
library("here")
library("minfi")
methylation <- readRDS(here("data/methylation.rds"))
~~~
{: .language-r}

This `methylation` object is a `GenomicRatioSet`, a Bioconductor data object
derived from the `SummarizedExperiment` class.
These `SummarizedExperiment` objects contain `assay`s, in this case methylation
levels, and optional sample-level `colData` and feature-level `metadata`.
These objects are very convenient to contain all of the information about 
a dataset in a high-throughput context and may be covered in more
detail in other Carpentries lessons.


~~~
methylation
~~~
{: .language-r}



~~~
class: GenomicRatioSet 
dim: 5000 37 
metadata(0):
assays(2): M CN
rownames(5000): cg00075967 cg00374717 ... cg12238634 cg03037856
rowData names(0):
colnames(37): 201868500150_R01C01 201868500150_R03C01 ...
  201870610111_R06C01 201870610111_R07C01
colData names(14): Sample_Well Sample_Name ... Array Slide
Annotation
  array: IlluminaHumanMethylationEPIC
  annotation: ilm10b4.hg19
Preprocessing
  Method: Raw (no normalization or bg correction)
  minfi version: 1.38.0
  Manifest version: 0.3.0
~~~
{: .output}

To extract the matrix of methylation values, we use the `assay` function.
One thing to bear in mind with these objects (and data 
structures for computational biology in R generally) is that
in the matrix of methylation data, samples or observations
are stored as columns, while features (in this case, CpG sites)
are stored as rows. This is in contrast to usual tabular data,
where features or variables are stored as columns and 
observations are stored as rows.


~~~
methyl_mat <- assay(methylation)
~~~
{: .language-r}

The distribution of these M-values looks like this:


~~~
hist(methyl_mat, breaks = "FD", xlab = "M-value")
~~~
{: .language-r}

<img src="../fig/rmd-02-histx-1.png" title="Histogram of M-values for all features. The distribution appears to be bimodal, with a large number of unmethylated features as well as many methylated features, and many intermediate features." alt="Histogram of M-values for all features. The distribution appears to be bimodal, with a large number of unmethylated features as well as many methylated features, and many intermediate features." width="432" style="display: block; margin: auto;" />

You can see that there are two peaks in this distribution, corresponding to
features which are largely unmethylated and methylated, respectively.

Similarly, we can examine the `colData`, which represents the sample-level
metadata we have relating to these data.
In this case, the metadata, phenotypes, and groupings in the `colData` 
look like this for the first 6 samples:


|Sample_Well |Sample_Name | purity|Sex | Age| weight_kg| height_m|      bmi|bmi_clas   |Ethnicity_wide |Ethnic_self    |smoker |Array  |        Slide|
|:-----------|:-----------|------:|:---|---:|---------:|--------:|--------:|:----------|:--------------|:--------------|:------|:------|------------:|
|A07         |PCA0612     |     94|M   |  39|  88.45051|   1.8542| 25.72688|Overweight |Mixed          |Hispanic       |No     |R01C01 | 201868500150|
|C07         |NKpan2510   |     95|M   |  49|  81.19303|   1.6764| 28.89106|Overweight |Indo-European  |Caucasian      |No     |R03C01 | 201868500150|
|E07         |WB1148      |     95|M   |  20|  80.28585|   1.7526| 26.13806|Overweight |Indo-European  |Persian        |No     |R05C01 | 201868500150|
|G07         |B0044       |     97|M   |  49|  82.55381|   1.7272| 27.67272|Overweight |Indo-European  |Caucasian      |No     |R07C01 | 201868500150|
|H07         |NKpan1869   |     95|F   |  33|  87.54333|   1.7272| 29.34525|Overweight |Indo-European  |Caucasian      |No     |R08C01 | 201868500150|
|B03         |NKpan1850   |     93|F   |  21|  87.54333|   1.6764| 31.15070|Obese      |Mixed          |Finnish/Creole |No     |R02C01 | 201868590193|

In this case, we will focus on age. The association between
age and methylation status in blood samples has been studied extensively,
and is actually a good case-study in how to perform some of the techniques
we will cover in this lesson. The methylation levels for these data 
can be presented in a heatmap:

<img src="../fig/rmd-02-heatmap-1.png" title="Heatmap of methylation values across all features. Samples are ordered according to age." alt="Heatmap of methylation values across all features. Samples are ordered according to age." width="432" style="display: block; margin: auto;" />

We would like to identify features that are related to our outcome of interest
(age). It's clear from the heatmap that there are too many features to do so
manually, even with this reduced number of features - the original dataset
contained over 800,000!

> ## Measuring DNA Methylation
> 
> DNA methylation is an epigenetic modification of DNA.
> Generally, we are interested in the proportion of 
> methylation at many sites or regions in the genome.
> DNA methylation microarrays, as we are using here,
> measure DNA methylation using two-channel microarrays,
> where one channel captures signal from methylated
> DNA and the other captures unmethylated signal.
> These data can be summarised
> as "Beta values" ($\beta$ values), which is the ratio
> of the methylated signal to the total signal 
> (methylated plus unmethylated).
> The $\beta$ value for site $i$ is calculated as
> 
> $$
>     \beta_i = \frac{
>         m_i
>     } {
>         u_{i} + m_{i}
>     }
> $$
> 
> where $m_i$ is the methylated signal for site $i$ and
> $u_i$ is the unmethylated signal for site $i$.
> $\beta$ values take on a value in the range 
> $[0, 1]$, with 0 representing a completely unmethylated 
> site and 1 representing a completely methylated site.
> 
> The M-values we use here are the $\log_2$ ratio of 
> methylated versus unmethylated signal:
>
> $$
>     M_i = \log_2\left(\frac{m_i}{u_i}\right)
> $$
> 
> M-values are not bounded to an interval as Beta values
> are, and therefore can be easier to work with in statistical models.
{: .callout}


# Fitting a linear model

So, in the data we've read in, we have a matrix of methylation values $X$ and a
vector of ages, $y$.
One way to model this is to see if we can "predict" methylation at a given
locus using age. Formally we'd describe that as:

$$
    \mathbf{E}(\text{M}_{i,j}) = \beta_0 + \beta_1 \text{Age}_j
$$

where $y_j$ is the age of sample $j$. In this model, $\beta_1$ represents
the unit change in mean methylation level for each unit (year) change in age.
You may remember how to fit this model from a previous lesson, and how to
get more information from the model object:


~~~
library("jtools")
age <- methylation$Age
fit <- lm(methyl_mat[1, ] ~ age)
fit
~~~
{: .language-r}



~~~

Call:
lm(formula = methyl_mat[1, ] ~ age)

Coefficients:
(Intercept)          age  
   0.902334     0.008911  
~~~
{: .output}

We now have estimates for the overall difference in means between these
two variables (the intercept) and the change in methylation level for
a unit change in age (the slope). We could plot this linear model:


~~~
plot(age, methyl_mat[1, ], pch = 16)
abline(fit)
~~~
{: .language-r}

<img src="../fig/rmd-02-unnamed-chunk-2-1.png" title="plot of chunk unnamed-chunk-2" alt="plot of chunk unnamed-chunk-2" width="432" style="display: block; margin: auto;" />

For this feature, we can see that there isn't a strong relationship between
methylation and age. We could try to repeat this for every feature in our
dataset; however, we have a lot of features! Repeating this 5,000 times would
be very time-consuming. It might help if we had a statistical method to
determine how much confidence we have that the relationship we've modelled
really exists. 


# Hypothesis testing in linear regression

Once we get estimates of the parameters of a linear regression, we want to
determine if the relationship we've fitted is interesting enough to warrant
further investigation. For linear regression, we usually do this
using null hypothesis significance testing.
For this linear model, we can use `broom` to extract detailed information about
the coefficients and the associated hypothesis tests in this model:


~~~
library("broom")
tidy(fit)
~~~
{: .language-r}



~~~
# A tibble: 2 × 5
  term        estimate std.error statistic p.value
  <chr>          <dbl>     <dbl>     <dbl>   <dbl>
1 (Intercept)  0.902      0.344      2.62   0.0129
2 age          0.00891    0.0100     0.888  0.381 
~~~
{: .output}

The standard errors (`std.error`) represent the uncertainty in our effect size
estimate. The test statistics and p-values represent measures of how (un)likely
it would be to observe results like this under the "null hypothesis".

Null hypothesis significance testing generally compares our observed results
with a set of hypothetical counter-examples of what we would expect to observe
if we repeated the same experiment and analysis over and over again under
the null hypothesis. In the case of linear regression, the null
hypothesis is that there is absolutely no relationship between the predictor
variable(s) and the outcome. This may not always be the most realistic or
useful null hypothesis, but it's the one we have!

> ## Exercise
> 
> Launch `shinystats::regressionApp` and adjust the parameters.
> 
> 1. If the noise parameter is small (eg, 0.5), how small an effect is significant?
>    If the noise parameter is large (eg, 5), how big must an effect be?
> 2. With a small number of observations (eg, 10), how strong does the relationship 
>    need to be (or how small the noise) before it is significant?
> 3. With a large number of observations (eg, 1000), how weak of an effect can you 
>    detect? Is a really small effect (slope=0.1) really "significant" in the way 
>    you'd use that word conversationally?
>
> > ## Solution
> > 1. 
> >    <img src="../fig/rmd-02-regex1-1.png" title="An example of a linear relationship for 100 points with a small amount of noise and small effect sizes that is statistically significant." alt="An example of a linear relationship for 100 points with a small amount of noise and small effect sizes that is statistically significant." width="432" style="display: block; margin: auto;" />
> >    
> >    ~~~
> >    [1] 1.190653e-05
> >    ~~~
> >    {: .output}
> >    
> >    <img src="../fig/rmd-02-regex2-1.png" title="An example of a linear relationship for 100 points with a large amount of noise and large effect sizes that is not statistically significant." alt="An example of a linear relationship for 100 points with a large amount of noise and large effect sizes that is not statistically significant." width="432" style="display: block; margin: auto;" />
> >    
> >    ~~~
> >    [1] 0.0002911055
> >    ~~~
> >    {: .output}
> > 2. 
> >    <img src="../fig/rmd-02-regex3-1.png" title="An example of a linear relationship for 10 points with a large amount of noise and large effect sizes that is not statistically significant." alt="An example of a linear relationship for 10 points with a large amount of noise and large effect sizes that is not statistically significant." width="432" style="display: block; margin: auto;" />
> >    
> >    ~~~
> >    [1] 0.00116021
> >    ~~~
> >    {: .output}
> >    
> >    <img src="../fig/rmd-02-regex4-1.png" title="An example of a linear relationship for 10 points with a small amount of noise and small effect sizes that is statistically significant." alt="An example of a linear relationship for 10 points with a small amount of noise and small effect sizes that is statistically significant." width="432" style="display: block; margin: auto;" />
> >    
> >    ~~~
> >    [1] 1.347143e-05
> >    ~~~
> >    {: .output}
> > 3. 
> >    <img src="../fig/rmd-02-regex5-1.png" title="An example of a linear relationship for 1,000 points with a large amount of noise and small effect sizes that is statistically significant." alt="An example of a linear relationship for 1,000 points with a large amount of noise and small effect sizes that is statistically significant." width="432" style="display: block; margin: auto;" />
> >    
> >    ~~~
> >    [1] 0.8089027
> >    ~~~
> >    {: .output}
> >    
> >    <img src="../fig/rmd-02-regex6-1.png" title="An example of a linear relationship for 1,000 points with a small amount of noise and small effect sizes that is statistically significant." alt="An example of a linear relationship for 1,000 points with a small amount of noise and small effect sizes that is statistically significant." width="432" style="display: block; margin: auto;" />
> >    
> >    ~~~
> >    [1] 0.0001118911
> >    ~~~
> >    {: .output}
> {: .solution}
{: .challenge}


# Fitting a lot of linear models

If we wanted to repeat this process for each feature in our data, we need
to narrow our focus. In particular,
The first coefficient in a linear model like this is the intercept,
which measures the overall 
offset between age and methylation levels. It's not really interesting
if this is zero or non-zero, since we don't really expect age and 
methylation level to have the same mean. We're more 
interested if there is a relationship between increasing age and methylation
levels. Therefore, we'll focus only on the second coefficient.


~~~
coef1 <- tidy(fit)[2, ]
coef1
~~~
{: .language-r}



~~~
# A tibble: 1 × 5
  term  estimate std.error statistic p.value
  <chr>    <dbl>     <dbl>     <dbl>   <dbl>
1 age    0.00891    0.0100     0.888   0.381
~~~
{: .output}

Instead of repeating this model fitting and extraction focus over and over, we
can write a function to fit this kind of model for any given row of the
matrix:


~~~
lm_feature <- function(i, methyl_mat, age) {
    tidy(lm(methyl_mat[i, ] ~ age))[2, ]
}
## fit a model to the 2nd and 3rd feature
coef2 <- lm_feature(2, methyl_mat, age)
coef3 <- lm_feature(3, methyl_mat, age)
~~~
{: .language-r}

We have a lot of features, though! Instead of looping through these values,
we can `lapply` over the number of rows of the matrix to fit a model for each
feature.


~~~
coefs <- lapply(
    seq_len(nrow(methyl_mat)),
    lm_feature,
    age = age,
    methyl_mat = methyl_mat
)
head(coefs)
~~~
{: .language-r}



~~~
[[1]]
# A tibble: 1 × 5
  term  estimate std.error statistic p.value
  <chr>    <dbl>     <dbl>     <dbl>   <dbl>
1 age    0.00891    0.0100     0.888   0.381

[[2]]
# A tibble: 1 × 5
  term  estimate std.error statistic p.value
  <chr>    <dbl>     <dbl>     <dbl>   <dbl>
1 age    -0.0198    0.0145     -1.36   0.181

[[3]]
# A tibble: 1 × 5
  term  estimate std.error statistic p.value
  <chr>    <dbl>     <dbl>     <dbl>   <dbl>
1 age    0.00586   0.00411      1.43   0.163

[[4]]
# A tibble: 1 × 5
  term  estimate std.error statistic p.value
  <chr>    <dbl>     <dbl>     <dbl>   <dbl>
1 age    0.00217   0.00766     0.283   0.779

[[5]]
# A tibble: 1 × 5
  term  estimate std.error statistic p.value
  <chr>    <dbl>     <dbl>     <dbl>   <dbl>
1 age    0.00257   0.00305     0.841   0.406

[[6]]
# A tibble: 1 × 5
  term  estimate std.error statistic p.value
  <chr>    <dbl>     <dbl>     <dbl>   <dbl>
1 age     0.0180   0.00615      2.93 0.00596
~~~
{: .output}

Now we have a list of coefficients, standard errors, and associated p-values
for each of the rows of our matrix, we can `rbind` them together.
To do this with each of the elements of our list, we can use `do.call`.
This calls the function supplied as the first argument (here, `rbind`) using
the second argument as a list of arguments to that function.
Here, it's equivalent to writing `rbind(coefs[[1]], coefs[[2]], [etc])`.


~~~
## bind together all of our small tables to make one big table
coef_df <- do.call(rbind, coefs)
## add a "feature name" column
coef_df$feature <- rownames(methyl_mat)
~~~
{: .language-r}

We can then create a plot of effect size estimates (model coefficients) against
p-values for each of these figures, to visualise the magnitude of effects.
These plots are often called "volcano plots", because they
resemble an eruption.


~~~
plot(coef_df$estimate, -log10(coef_df$p.value),
    xlab = "Effect size", ylab = bquote(-log[10](p)),
    pch = 19
)
~~~
{: .language-r}

<img src="../fig/rmd-02-volcplot1-1.png" title="Plot of -log10(p) against effect size estimates for a regression of age against methylation level for each feature in the data." alt="Plot of -log10(p) against effect size estimates for a regression of age against methylation level for each feature in the data." width="432" style="display: block; margin: auto;" />

In this figure, every point represents a feature of interest. The x-axis
represents the effect size observed for that feature in a linear model,
while the y-axis is the $-\log_{10}(\text{p})$, where larger values
indicate increasing statistical evidence of a non-zero effect size. 
A positive effect size represents increasing methylation with increasing age,
and a negative effect size represents decreasing methylation with increasing
age. Points higher on the x-axis represent features for which we think the 
results we observed would be very unlikely under the null hypothesis.

Since we want to identify feature that have different methylation levels in
different age groups, 
in an ideal case there would be clear separation between "null" and "non-null"
features. However, usually we observe results as we do here: there is a 
continuum of effect sizes and p-values, with no clear separation between these 
two classes of features. While statistical methods exist to
derive insights from continuous measures like these, it is often convenient to
obtain a list of features which we are confident have non-zero effect sizes.
This is made more difficult by the number of tests we perform.


> ## Exercise
> 
> The effect size estimates are very small, and yet many of the p-values are
> well below a usual significance level of p < 0.05. Why is this?
> 
> > ## Solution
> > 
> > Because age has a much larger range than methylation levels, the unit change
> > in methylation level even for a strong relationship is very small!
> > 
> > As we mentioned, the p-value is a function of both the effect size estimate
> > and the uncertainty (standard error) of that estimate. Because the
> > uncertainty in our estimates is much smaller than the estimates themselves,
> > the p-values are also small.
> > 
> > If we predicted age using methylation level, it's likely we'd see much
> > larger coefficients, though broadly similar p-values!
> > 
> {: .solution}
{: .challenge}


# The problem of multiple tests

With such a large number of features, it would be useful
to decide which features are "interesting" or "significant"
for further study. However, if we were to apply a normal significance threshold
of 0.05, it's likely that we'd end up with a lot of false positives. That's
because a p-value threshold like this represents a $\frac{1}{20}$ chance that
we'd observe results as extreme or more extreme under the null hypothesis
(that there is no assocation between age and methylation level). If we do
many more than 20 such tests, we can expect that for a lot of the tests, the
null hypothesis is true, but we will still observe extreme results.
To demonstrate this, it's useful to see what happens if
we scramble age and run the same test again:


~~~
age_perm <- age[sample(ncol(methyl_mat), ncol(methyl_mat))]
coefs_perm <- lapply(
    seq_len(nrow(methyl_mat)),
    lm_feature,
    methyl_mat = methyl_mat,
    age = age_perm
)
coef_df_perm <- do.call(rbind, coefs_perm)
plot(coef_df_perm$estimate, -log10(coef_df_perm$p.value),
    xlab = "Effect size", ylab = bquote(-log[10](p)),
    pch = 19
)
abline(h = -log10(0.05), lty = "dashed", col = "red")
~~~
{: .language-r}

<img src="../fig/rmd-02-volcplotfake-1.png" title="Plot of -log10(p) against effect size estimates for a regression of a made-up feature against methylation level for each feature in the data. A dashed line represents a 0.05 significance level." alt="Plot of -log10(p) against effect size estimates for a regression of a made-up feature against methylation level for each feature in the data. A dashed line represents a 0.05 significance level." width="432" style="display: block; margin: auto;" />

Since we've generated a random sequence of ages, we have no reason to suspect
that there is a true association between methylation levels and this sequence
of random numbers. However, you can see that the p-value for many features is
still lower than a traditional significance level of $p=0.05$. If we were to
use this in a real experiment, it's likely that we'd identify many features
as associated with age, when the results we're observing are simply due to
chance.

> ## Exercise
>
> 
> 1. If we run 10,000 tests under the null hypothesis,
>    how many of them (on average) will be statistically
>    significant at a threshold of $p < 0.05$?
> 2. Why would we want to be conservative in labelling features
>    as significantly different?
>    By conservative, we mean to err towards labelling true
>    differences as "not significant" rather than vice versa.
> 3. How could we account for a varying number of tests to
>    ensure "significant" changes are truly different? 
> 
> > ## Solution
> > 1. By default we expect $10,000 \times 0.05 = 250$
> >    features to be statistically significant under the null hypothesis,
> >    because p-values should always be uniformly distributed under
> >    the null hypothesis.
> > 2. Features that we label as "significantly different" will often
> >    be reported in manuscripts. We may also spend time and money
> >    investigating them further, computationally or in the lab.
> >    Therefore, spurious results have a real cost for ourselves and
> >    for others.
> > 3. One approach to controlling for the number of tests is to
> >    divide our significance threshold by the number of tests
> >    performed. This is termed "Bonferroni correction" and
> >    we'll discuss this further now.
> {: .solution}
{: .challenge}


# Adjusting for multiple tests

When performing many statistical tests to
categorise features, we're effectively classifying
features as "significant", meaning those for which we reject the null 
hypothesis, and "non-significant". We also generally hope that there is a subset
of features for which the null hypothesis is truly false, as well as many
for which the null truly does hold. We hope that for all features that the null
hypothesis is true, we accept it, and for all features that the null hypothesis
is not true, we reject it. As we showed in the example with permuted
age, with a large number of tests it's inevitable that we'll get some of these wrong.

We can think of these features as being 
"truly different" or "not truly different"[^2].
Using this idea, we can see that each 
categorisation we make falls into four categories:

|              |Reject null|Accept null|
|-------------:|-------------:|--------------:|
|Null is true |True positive |False negative |
|Null is false|False positive|True negative  |

If the null hypothesis was true for every feature, then as we perform more and
more tests we'd tend to correctly categorise most
results as negative. However, since p-values
are uniformly distributed under the null,
at a significance level of 5%, 5% of all
results will be "significant" even though
these are results we expect under the null.
These would fall under the label "false positives" in the table
above, and are also termed "false discoveries."

There are two common ways of controlling these
false discoveries.
The first is to say, when we're doing $n$ tests,
that we want to have the same certainty of making
one false discovery with $n$ tests as we have if we're only doing
one test. This is "Bonferroni" correction,[^3] which
divides the significance level by the number of
tests performed, $n$. Equivalently, we can use the
non-transformed p-value threshold but multiply
our p-values by the number of tests.
This is often very conservative, especially
with a lot of features!


~~~
p_raw <- coef_df$p.value
p_fwer <- p.adjust(p_raw, method = "bonferroni")
library("ggplot2")
ggplot() +
    aes(p_raw, p_fwer) +
    geom_point() +
    scale_x_log10() + scale_y_log10() +
    geom_abline(slope = 1, linetype = "dashed") +
    geom_hline(yintercept = 0.05, linetype = "dashed", col = "red") +
    geom_vline(xintercept = 0.05, linetype = "dashed", col = "red") +
    labs(x = "Raw p-value", y = "Bonferroni p-value")
~~~
{: .language-r}

<img src="../fig/rmd-02-p-fwer-1.png" title="Plot of Bonferroni-adjusted p-values (y) against unadjusted p-values (x). A dashed black line represents the identity (where x=y), while dashed red lines represent 0.05 significance thresholds." alt="Plot of Bonferroni-adjusted p-values (y) against unadjusted p-values (x). A dashed black line represents the identity (where x=y), while dashed red lines represent 0.05 significance thresholds." width="432" style="display: block; margin: auto;" />

You can see that the p-values are exactly one for the vast majority of tests
we performed! This is not ideal sometimes, because unfortunately we usually
don't have very large sample sizes in health sciences.

The second main way of controlling for multiple tests
is to control the *false discovery rate*.[^4]
This is the proportion of false positives, or false discoveries,
we'd expect to get each time if we repeated
the experiment over and over.

1. Rank the p-values
2. Assign each a rank (1 is smallest)
3. Calculate the critical value 
    $$
        q = \left(\frac{i}{m}\right)Q
    $$,
    where $i$ is rank, $m$ is the number of tests, and $Q$ is the
    false discovery rate we want to target.[^5]
4. Find the largest p-value less than the critical value.
    All smaller than this are significant.


|FWER|FDR|
|:-------------|:--------------|
|+ Controls probability of identifying a false positive|+ Controls rate of false discoveries|
|+ Strict error rate control |+ Allows error control with less stringency|
|- Very conservative |- Does not control probability of making errors|
|- Requires larger statistical power|- May result in false discoveries|

> ## Exercise
>
> 1. At a significance level of 0.05, with 100 tests
>    performed, what is the Bonferroni significance
>    threshold?
> 2. In a gene expression experiment, after FDR 
>    correction we observe 500 significant genes.
>    What proportion of these genes are truly
>    different?
> 3. Try running FDR correction on the `p_raw` vector.
>    *Hint: check `help("p.adjust")` to see what the method
>    is called*.  
>    Compare these values to the raw p-values
>    and the Bonferroni p-values.
>  
> > ## Solution
> > 
> > 1. The Bonferroni threshold for this significance
> >    threshold is
> >    $$
> >         \frac{0.05}{100} = 0.0005
> >    $$
> > 2. Trick question! We can't say what proportion
> >    of these genes are truly different. However, if
> >    we repeated this experiment and statistical test
> >    over and over, on average 5% of the results from
> >    each run would be false discoveries.
> > 3. The following code runs FDR correction and compares it to
> >    non-corrected values and to Bonferroni:
> >    
> >    ~~~
> >    p_fdr <- p.adjust(p_raw, method = "BH")
> >    ggplot() +
> >        aes(p_raw, p_fdr) +
> >        geom_point() +
> >        scale_x_log10() + scale_y_log10() +
> >        geom_abline(slope = 1, linetype = "dashed") +
> >        geom_hline(yintercept = 0.05, linetype = "dashed", color = "red") +
> >        geom_vline(xintercept = 0.05, linetype = "dashed", color = "red") +
> >        labs(x = "Raw p-value", y = "Benjamini-Hochberg p-value")
> >    ~~~
> >    {: .language-r}
> >    
> >    <img src="../fig/rmd-02-p-fdr-1.png" title="Plot of Benjamini-Hochberg-adjusted p-values (y) against unadjusted p-values (x). A dashed black line represents the identity (where x=y), while dashed red lines represent 0.05 significance thresholds." alt="Plot of Benjamini-Hochberg-adjusted p-values (y) against unadjusted p-values (x). A dashed black line represents the identity (where x=y), while dashed red lines represent 0.05 significance thresholds." width="432" style="display: block; margin: auto;" />
> >    
> {: .solution}
{: .challenge}

# How does hypothesis testing for a linear model work?

In order to decide whether a result would be unlikely
under the null hypothesis, we can calculate a test statistic.
For coefficient $j$ in a linear model (in our case, it would be the slope),
the test statistic is a t-statistic given by:

$$
    t_{j} = \frac{\hat{\beta}_{j}}{SE\left(\hat{\beta}_{j}\right)}
$$

$SE\left(\hat{\beta}_{j}\right)$ measures the uncertainty we have in our effect
size estimate.

Knowing what distribution these t-values follow under the null
hypothesis allows us to determine how unlikely it would be for
us to observe what we have under those circumstances, if we repeated the
experiment and analysis over and over again.

To demonstrate, we can manually demonstrate the relationship between these
quantities (this is not important to remember).


~~~
x <- rnorm(100)
y <- rnorm(100)
fit <- lm(y ~ x)
tab <- tidy(fit)
tab
~~~
{: .language-r}



~~~
# A tibble: 2 × 5
  term        estimate std.error statistic p.value
  <chr>          <dbl>     <dbl>     <dbl>   <dbl>
1 (Intercept)  -0.0321    0.0967    -0.332   0.741
2 x            -0.0871    0.102     -0.856   0.394
~~~
{: .output}

We can see that the t-statistic is just the ratio of the estimate to the 
standard error:


~~~
tvals <- tab$estimate / tab$std.error
all.equal(tvals, tab$statistic)
~~~
{: .language-r}



~~~
[1] TRUE
~~~
{: .output}

Calculating the p-values is a bit more tricky. Specifically, it's the proportion
of the distribution of the test statistic under the null hypothesis that is 
*as extreme or more extreme* than the observed value of the test statistic.
This is easy to observe visually, by plotting the distribution:

<img src="../fig/rmd-02-tdist-1.png" title="Density plot of a t-distribution showing the observed test statistics (here, t-statistics). The p-values, visualised here with shaded regions, represent the portion of the null distribution that is as extreme or more extreme as the observed test statistics, which are shown as dashed lines." alt="Density plot of a t-distribution showing the observed test statistics (here, t-statistics). The p-values, visualised here with shaded regions, represent the portion of the null distribution that is as extreme or more extreme as the observed test statistics, which are shown as dashed lines." width="432" style="display: block; margin: auto;" />

The red-ish shaded region represents the portion of the distribution of the test
statistic under the null hypothesis for the intercept term. This shaded region
comprises most of the total area of the distribution; therefore, the p-value is
large ($p=0.741$).
The blue-ish shaded region represents the same measure for the slope 
term; this is smaller relative to the total area of the distribution, therefore
the p-value is smaller than the intercept term
($p=0.394$).
You can see that the p-value is a function of the size of the effect we're 
estimating and the uncertainty we have in that effect. A large effect with
large uncertainty may not lead to a small p-value, and a small effect with small
uncertainty may lead to a small p-value.


> ## Calculating p-values from a linear model
> 
> Manually calculating the p-value for a linear model is a little bit more 
> complex than calculating the t-statistic. The intuition posted above is
> definitely sufficient for most cases, but for completeness, here is how
> we do it:
> 
> Since the statistic in a linear model is a t-statistic, it follows a student
> t distribution under the null hypothesis. We want to know what portion of
> the distribution function of the test statistic is as extreme as, or more 
> extreme than, the value we observed. The function `pt` (similar to `pnorm`, 
> etc) can give us this information.
> 
> Since we're not sure if the coefficient will be larger or smaller than zero,
> we want to do a 2-tail test. Therefore we take the absolute value of the
> t-statistic, and look at the upper rather than lower tail.
> Because in a 2-tail test we're looking at "half" of the t-distribution,
> we also multiply the p-value by 2.
> 
> Combining all of this gives us:
>
> 
> ~~~
> pvals <- 2 * pt(abs(tvals), df = fit$df, lower.tail = FALSE)
> all.equal(tab$p.value, pvals)
> ~~~
> {: .language-r}
> 
> 
> 
> ~~~
> [1] TRUE
> ~~~
> {: .output}
{: .callout}


# Sharing information

One idea is to take advantage of the fact that we're doing all these tests 
at once. We can leverage this fact to *share information* between model
parameters. 
The insight that we use to perform *information pooling* like this is derived
from our knowledge about the structure of the data. For example, in a
high-throughput experiment like a DNA methylation assay, we know that all of
the features were measured simultaneously, using the same technique. This means
that generally, we expect the base-level variability for each feature to be
broadly similar.

This can enable us to get a better estimate of the uncertainty of model 
parameters than we could get if we consider each feature in isolation.
This enables us to share information between genes to get more robust
estimators.

Remember that the t-statistic for feature $i$, coefficient $j$ $\beta$ 
in a linear model is as follows:

$$
    t_{ij} = \frac{\hat{\beta}_{ij}}{SE\left(\hat{\beta}_{ij}\right)}
$$

It's clear that large effect sizes will likely lead to small p-values,
as long as the standard error for the coefficent is not large.
However, the standard error is affected by the strength of noise, 
as we saw earlier.
If we have a small number of observations, it's common for the noise for some
features to be extremely small simply by chance. This will lead to a very small
p-value, which may give us unwarranted confidence in the level of certainty
we have in the results.

There are many statistical methods in genomics that use this type of approach
to get better estimates by pooling information between features that were
measured simultaneously using the same techniques.
Here we will focus on the package `limma`, which is a quite old software package
used to fit linear models, originally for the gene expression micro-arrays 
that were common in the 2000s, but which is still in use in RNAseq experiments,
among others.
The authors of `limma` made some assumptions about the distributions that these
follow, and pool information across genes to get a better estimate of the 
uncertainty in effect size estimates.

The process of running a model in `limma` is a bit different (and simpler)
than the process we went through before to fit a linear model for each feature:


~~~
library("limma")

design <- model.matrix(~age)
fit_age <- lmFit(methyl_mat, design = design)
fit_age <- eBayes(fit_age)
toptab_age <- topTable(fit_age, coef = 2, number = nrow(fit_age))
plot(toptab_age$logFC, -log10(toptab_age$P.Value),
    xlab = "Effect size", ylab = bquote(-log[10](p)),
    pch = 19
)
~~~
{: .language-r}

<img src="../fig/rmd-02-limmavolc1-1.png" title="A plot of -log10(p) against effect size estimates for a regression of age against methylation using limma." alt="A plot of -log10(p) against effect size estimates for a regression of age against methylation using limma." width="432" style="display: block; margin: auto;" />

Since we now have p-values from linear models with and without this shrinkage,
we can compare these:

~~~
plot(
    toptab_age[coef_df$feature, "P.Value"],
    coef_df$p.value,
    pch = 16, log = "xy"
)
abline(0:1, lty = "dashed")
~~~
{: .language-r}

<img src="../fig/rmd-02-unnamed-chunk-3-1.png" title="plot of chunk unnamed-chunk-3" alt="plot of chunk unnamed-chunk-3" width="432" style="display: block; margin: auto;" />




You can see that the effect of pooling is to shrink large 
estimates downwards and small estimates upwards, all towards
a common value. The degree of shrinkage generally depends on 
the amount of pooled information and the strength of the 
evidence independent of pooling.

Similarly, DESeq2 shares information between genes
to *shrink* estimates of a noise parameter, in that case to model counts.

Shrinkage methods can be complex to implement and understand,
but it's good to understand why these approaches may be more precise 
and sensitive than the naive approach of fitting a model to each feature
separately.


> ## Exercise
> 
> 1. Try to run the same kind of linear model with smoking 
>    status as covariate instead of age, and making a volcano
>    plot.
>    *Note: smoking status is stored as `methylation$smoker`.*
> 2. Notice that `limma` creates an `adj.P.Val` column in the output you just 
>    created. What
>    kind of p-value adjustment is it doing? Bonferroni,
>    Benjamini-Hochberg, or something else?
> 3. You can see in the example in the lesson that this information sharing
>    leads to generally large p-values. Why might this be preferable?
> 
>
> > ## Solution
> > 
> > 1. The following code runs the same type of model with smoking status:
> >    
> >    ~~~
> >    design <- model.matrix(~methylation$smoker)
> >    fit_smoke <- lmFit(methyl_mat, design = design)
> >    fit_smoke <- eBayes(fit_smoke)
> >    toptab_smoke <- topTable(fit_smoke, coef = 2, number = nrow(fit_smoke))
> >    plot(toptab_smoke$logFC, -log10(toptab_smoke$P.Value),
> >        xlab = "Effect size", ylab = bquote(-log[10](p)),
> >        pch = 19
> >    )
> >    ~~~
> >    {: .language-r}
> >    
> >    <img src="../fig/rmd-02-limmavolc2-1.png" title="A plot of -log10(p) against effect size estimates for a regression of smoking status against methylation using limma." alt="A plot of -log10(p) against effect size estimates for a regression of smoking status against methylation using limma." width="432" style="display: block; margin: auto;" />
> > 2. We can use `all.equal` to compare vectors:
> >    
> >    ~~~
> >    all.equal(p.adjust(toptab_smoke$P.Value, method = "BH"), toptab_smoke$adj.P.Val)
> >    ~~~
> >    {: .language-r}
> >    
> >    
> >    
> >    ~~~
> >    [1] TRUE
> >    ~~~
> >    {: .output}
> > 3. Being a bit more conservative when identifying features can help to avoid
> >    false discoveries. Furthermore, when rejecting the null hypothesis is
> >    based more on a small standard error resulting from abnormally low levels
> >    of variability for a given feature, we might want to be a bit more
> >    conservative in our expectations.
> {: .solution}
{: .challenge}




> ## Shrinkage
> 
> Shrinkage is an intuitive term for an effect
> of information sharing, and is something observed
> in a broad range of statistical models.
> Often, shrinkage is induced by a *multilevel*
> modelling approach or by *Bayesian* methods.
> 
> The general idea is that these models incorporate 
> information about the structure of the
> data into account when fitting the parameters.
> We can share information between features
> because of our knowledge about the data structure;
> this generally requires careful consideration about
> how the data were generated and the relationships within.
>
> An example people often use is estimating the effect
> of attendance on grades in several schools. We can
> assume that this effect is similar in different schools
> (but maybe not identical), so we can *share information*
> about the effect size between schools and shink our
> estimates towards a common value.
> 
> For example in `DESeq2`, the authors used the observation
> that genes with similar expression counts in RNAseq data
> have similar *dispersion*, and a better estimate of
> these dispersion parameters makes estimates of
> fold changes much more stable.
> Similarly, in `limma` the authors made the assumption that
> in the absence of biological effects, we can often expect the
> technical variation of each genes to be broadly similar.
> Again, better estimates of variability allow us to
> prioritise genes in a more reliable way.
> 
> There are many good resources to learn about this type of approach,
> including:
> 
> - [a blog post by TJ Mahr](https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/)
> - [a book by David Robinson](https://gumroad.com/l/empirical-bayes)
> - [a (relatively technical) book by Gelman and Hill](http://www.stat.columbia.edu/~gelman/arm/)
{: .callout}


# Screening

To get around the problem of multiple testing, people sometimes reduce the 
number of variables as input. There are some valid and many invalid ways of
doing this. One (invalid) method is to select variables based on correlation 
with the outcome. The p-values we get out of this kind of approach
model are basically meaningless, because we're doing a 2-stage model and only
reporting one set of p-values (ignoring all the non-significant ones). This 
means that we are biasing the results towards significance, and further that
we are not correctly adjusting for the true number of tests we're
performing.


~~~
cors <- apply(methyl_mat, 1, function(col) cor(col, age))
x_cor <- methyl_mat[abs(cors) > quantile(abs(cors), 0.95), ]
design <- model.matrix(~age)
fit_cor <- lmFit(x_cor, design = design)
fit_cor <- eBayes(fit_cor)
toptab_cor <- topTable(fit_cor, coef = 2, number = nrow(fit_cor))
par(mfrow=c(1, 2))
plot(toptab_cor$logFC, -log10(toptab_cor$P.Value),
    xlab = "Effect size", ylab = bquote(-log[10](p)),
    pch = 19
)
feats <- rownames(toptab_cor)
pvals_both <- cbind(
    Original = toptab_age[feats, "adj.P.Val"],
    Screened = toptab_cor[feats, "adj.P.Val"]
)
lims <- range(pvals_both)
plot(pvals_both, pch = 19, xlim = lims, ylim = lims, log = "xy")
abline(h = 0.05, lty = "dashed", col = "firebrick")
abline(v = 0.05, lty = "dashed", col = "firebrick")
abline(coef = 0:1, lty = "dashed")
~~~
{: .language-r}

<img src="../fig/rmd-02-screening-cor-1.png" title="Alt" alt="Alt" width="720" style="display: block; margin: auto;" />

This two-step selection process biases the results towards
significance, and it means that the p-values we
report aren't accurate.

> ## Screening using variance
> 
> One way to screen for variables that *does* work is to use a filter
> or screen that is independent of the test statistic.
> Correlation is not independent of the t-statistic. However,
> Overall variance of a feature is independent of this statistic, because
> the overall variability level does not. However, we might suspect that
> features that don't vary much at all don't vary in our groups of interest,
> or alongside our continuous features (age in this example).
> 
> This approach was introduced by 
> [Bourgon, Gentleman and Huber (2010)](https://www.pnas.org/content/107/21/9546.short)
> and can be shown to be valid. This is because variance and the t-statistic
> are not correlated under the null hypothesis, but are correlated under
> the alternative.
> 
> 
> 
{: .callout}

[^1]: It's not hugely problematic if the assumption of normal residuals is violated. It mainly affects our ability to accurately predict responses for new, unseen observations.

[^2]: "True difference" is a hard category to rigidly define. As we've seen, with a lot of data, we can detect tiny differences, and with little data, we can't detect large differences. However, both can be argued to be "true".

[^3]: Bonferroni correction is also termed "family-wise" error rate control.

[^4]: This is often called "Benjamini-Hochberg" adjustment.

[^5]: People often perform extra controls on FDR-adjusted p-values, ensuring that ranks don't change and the critical value is never smaller than the original p-value.

{% include links.md %}
