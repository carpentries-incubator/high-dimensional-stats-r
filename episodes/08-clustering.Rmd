---
title: "Clustering"
teaching: 0
exercises: 0
questions:
- "What does clustering mean?"
- "Why would we want to find clusters in data?"
- "How can we cluster data with a model?"
- "How can we cluster data without a model?"
- "How can we check if clusters are robust?"
objectives:
- "Understand and perform clustering with K-means, mixture models, and hierarchical clustering."
- "Assess clustering performance with silhouette score and bootstrapping/consensus clustering."
keypoints:
- "KP1"
math: yes
---


```{r settings, include=FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("08-")
if (!file.exists(here::here("data/scrnaseq.rds"))) {
    source(here::here("data/scrnaseq.R"))
}
```

```{r pkgs, echo=FALSE}
## preamble
suppressPackageStartupMessages({
    library("scRNAseq")
    library("mixtools")
    library("irlba")
    library("scater")
    library("scuttle")
    library("scran")
    library("Rtsne")
    library("mclust")
    library("igraph")
    library("bluster")
    library("here")
})
```

# Introduction

High-dimensional data, especially in biological settings, commonly has
many sources of heterogeneity. Some of these are stochastic variation
arising from measurement error or random differences between organisms. In
some cases, this heterogeneity arises from the presence of subgroups in the
data.

```{r data}
library("SingleCellExperiment")
zd <- readRDS(here("data/scrnaseq.rds"))
set.seed(42)
```

# Mixture model introduction

```{r mixture-animation, echo=FALSE, fig.cap="Cap", fig.alt="Alt"}
set.seed(66)
true_means <- c(-1, 4)
true_sds <- c(2, 1)
df <- data.frame(
    x = c(
        rnorm(30, mean = true_means[[1]], sd = true_sds[[1]]),
        rnorm(50, mean = true_means[[2]], sd = true_sds[[1]])
    ),
    cluster = c(rep("a", 30), rep("b", 50))
)
means <- c(1, 1.1)
sds <- c(1, 1)
dfs <- c(1, 1)

label_points_mixture <- function(x, means, sds) {
    sapply(
        x,
        function(i) {
            which.max(
                c(
                    dnorm(i, mean = means[[1]], sd = sds[[1]]),
                    dnorm(i, mean = means[[2]], sd = sds[[2]])
                )
            )
        }
    )
}

iterate_mixture <- function(x, means, sds) {
    components <- label_points_mixture(x, means, sds)
    means[[1]] <- mean(x[components == 1])
    means[[2]] <- mean(x[components == 2])
    sds[[1]] <- sd(x[components == 1])
    sds[[2]] <- sd(x[components == 2])
    list(components = components, means = means, sds = sds)
}

plot_mixture <- function(x, means, sds, components) {
    ggplot(data.frame(x = x)) +
        aes(x) +
        geom_histogram(aes(y = ..density..)) +
        lims(x = range(x) * 1.5) +
        geom_function(
            fun = function(...) dnorm(...) * mean(components == 1),
            args = list(mean = means[[1]], sd = sds[[1]]),
            aes(colour = "component 1")
        ) +
        geom_function(
            fun = function(...) dnorm(...) * mean(components == 2),
            args = list(mean = means[[2]], sd = sds[[2]]),
            aes(colour = "component 2")
        )
}


components <- label_points_mixture(df$x, means, sds)
plot_mixture(df$x, means, sds, components)
out <- iterate_mixture(df$x, means, sds)
plot_mixture(df$x, out$means, out$sds, out$components)
```


# Mixture models proper (multi-dimensional)

https://web.stanford.edu/class/bios221/book/Chap-Mixtures.html

```{r dimred}
library("scater")
zd <- runPCA(zd, ncomponents = 15)
zd <- runTSNE(zd, dimred = "PCA")
```

```{r mixture, echo=FALSE, fig.cap = "Title", fig.alt = "Alt"}
library("mclust")
## model-based
clust <- mclustBIC(reducedDim(zd), modelNames = "VVV")
plot(clust)

model <- Mclust(reducedDim(zd), x = clust)

zd$mixture <- as.character(model$classification)

plotReducedDim(zd, "TSNE", colour_by = "mixture")
```


# K-means

Generalisation of mixture models to have arbitrary density, just using distance.

```{r kmeans-animation, echo = FALSE, fig.cap="Cap", fig.alt="Alt"}
set.seed(66)
true_means <- c(-1, 4)
true_sds <- c(2, 1)
df <- data.frame(
    x = c(
        rnorm(30, mean = true_means[[1]], sd = true_sds[[1]]),
        rnorm(50, mean = true_means[[2]], sd = true_sds[[1]])
    ),
    cluster = c(rep("a", 30), rep("b", 50))
)
means <- c(1, 1.1)
dfs <- c(1, 1)

label_points_kmeans <- function(x, means) {
    sapply(x,
        function(i) {
            which.min(
                c(
                    dist(rbind(i, means[[1]])),
                    dist(rbind(i, means[[2]]))
                )
            )
        }
    )
}

iterate_kmeans <- function(x, means, sds) {
    components <- label_points_kmeans(x, means)
    means[[1]] <- mean(x[components == 1])
    means[[2]] <- mean(x[components == 2])
    list(components = components, means = means)
}

plot_kmeans <- function(x, means, components) {
    ggplot(data.frame(x = x)) +
        geom_histogram(aes(x = x, fill = factor(components))) +
        geom_vline(xintercept = means[[1]]) +
        geom_vline(xintercept = means[[2]]) +
        lims(x = range(x) * 1.5)
}

components <- label_points_kmeans(df$x, means)
plot_kmeans(df$x, means, components)
out <- iterate_kmeans(df$x, means)
plot_kmeans(df$x, out$means, out$components)
```



https://web.stanford.edu/class/bios221/book/Chap-Clustering.html

```{r kmeans, fig.cap = "Title", fig.alt = "Alt"}
## ideas: vary centers, low iter.max, low nstart
cluster <- kmeans(reducedDim(zd), centers = 7, iter.max = 1000, nstart = 100)
zd$kmeans <- as.character(cluster$cluster)

plotReducedDim(zd, "TSNE", colour_by = "kmeans")
```

> ## k-medioids (PAM)
> 
> This is like k-means but 
>
> 
{: .callout}


# Choosing K

We can't use measures like sum of squares, because then we get $K=N$. As with
regression, if we keep adding parameters (in this case, clusters)
the fit will always get better. In fact, it has to! We will get a perfect
fit (zero error) when each point is its own "cluster".

Therefore we again need to use BIC or something.


# Hierarchical clustering

Recall the heatmap I showed in regression lesson.
If we do this without hierarchical clustering, it's very noisy.
```{r, fig.cap="Cap", fig.alt="Alt"}
library("minfi")
library("here")
library("ComplexHeatmap")

methylation <- readRDS(here("data/methylation.rds"))

age <- methylation$Age
methyl_mat <- t(assay(methylation))
small <- methyl_mat[, 1:500]
cor_mat <- cor(small)
col <- circlize::colorRamp2(
    breaks = seq(-1, 1, length.out = 9),
    colors = rev(RColorBrewer::brewer.pal(9, "RdYlBu"))
)
Heatmap(cor_mat,
    column_title = "Feature-feature correlation in methylation data",
    name = "Pearson correlation",
    col = col,
    cluster_rows = FALSE, cluster_columns = FALSE,
    show_row_dend = FALSE, show_column_dend = FALSE,
    show_row_names = FALSE, show_column_names = FALSE
)
```



With clustering you can see some nice groupings.

```{r, fig.cap="Cap", fig.alt="Alt"}
Heatmap(cor_mat,
    column_title = "Feature-feature correlation in methylation data",
    name = "Pearson correlation",
    col = col,
    row_dend_width = unit(0.2, "npc"),
    column_dend_height = unit(0.2, "npc"),
    show_row_names = FALSE, show_column_names = FALSE
)
```

# Distance functions

# Linkage methods

Complete linkage (the default) uses the maximum distance between clusters as
the distance when merging them.

```{r, fig.cap="Cap", fig.alt="Alt"}
distmat <- dist(cor_mat)
clust <- hclust(distmat, method = "complete")
plot(clust)
```

Wardâ€™s method says that the distance between two clusters, A and B, is how much
the sum of squares will increase when we merge them.

Details: https://jbhender.github.io/Stats506/F18/GP/Group10.html

```{r, fig.cap="Cap", fig.alt="Alt"}
clust <- hclust(distmat, method = "ward.D")
plot(clust)
```


```{r metrics, echo = FALSE, eval = FALSE, fig.cap = "Title", fig.alt = "Alt"}

# Graph-based clustering (maybe)

# ```{r graph, fig.cap = "Title", fig.alt = "Alt"}
## graph-based
g <- buildSNNGraph(zd, k = 10, use.dimred = "PCA")
clust <- igraph::cluster_walktrap(g)$membership
reducedDim(zd, "force") <- igraph::layout_with_fr(g)
colLabels(zd) <- factor(clust)
plotReducedDim(zd, colour_by = "label", dimred = "force")
# ```


# bluster::clusterRows - maybe?

## measures: silhouette, bootstrap
## approx silhouette? purity?
```


{% include links.md %}

