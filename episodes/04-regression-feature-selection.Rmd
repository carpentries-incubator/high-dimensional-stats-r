---
title: "Feature selection for regression"
teaching: 45
exercises: 15
questions:
- "Why would we want to find a subset of features
  that are associated with an outcome?"
- "How can we iteratively find a good subset of our features
  variables to use for regression?"
- "What are some risks and downsides of iterative feature
  selection?"
objectives:
- "Understand feature selection for multiple regression in a 
  biomedical context."
- "Understand how to fit a stepwise regression model."
keypoints:
- "Feature selection can help us to understand the mechanisms behind
  an outcome, or to predict an outcome from some easy-to-gather 
  features (or both)."
- "Sets of features can be more predictive and provide
  a better explanation than a single feature alone."
- "Best subset selection is a powerful but expensive way to select 
  features."
- "Forward stepwise regression allows us to find a set of features that
  are associated with an outcome (eg, cancer volume)."
- "Reverse stepwise regression allows us to take a predictive set of 
  features and remove those that are less strongly predictive."
- "Stepwise regression may not be very efficient."
math: yes
---


```{r settings, include=FALSE}
library("here")
source(here("bin/chunk-options.R"))
knitr_fig_path("04-")
```

# Introduction

In the previous lesson, we covered a method of feature selection
using...


# Best subset selection

Best subset selection is one way of finding a subset of features
that can predict an outcome. This works by trying every possible subset
of features to find which is the best combination.
This works, but is really computationally demanding,
because the number of possible combinations of features is actually 
$2^{n - 1}$. This means that for $10$ features, we need to estimate
$2^9=512$ models, and for $100$ features this already rises to 
$2^99 \approx 6.3\times 10^29$. You can imagine that this becomes *very*
slow quite quickly!

First, we'll try it with a small subset of the methylation data
with only 10 features and 37 samples. Programming this by hand would be
quite difficult, but thankfully the `leaps` package implements a quick way
to do it. They term it "exhaustive search" because we are exhausting every
possible combination of features to find the best one.

```{r bestsub}
library("here")
library("minfi")
methylation <- readRDS(here("data/methylation.rds"))

## here, we transpose the matrix to have features as rows and samples as columns
methyl_mat <- t(assay(methylation))
age <- methylation$Age

library("leaps")
## Take a small number of features
small_methyl <- methyl_mat[, 1:10]
fit_bs <- regsubsets(
  x = small_methyl,
  y = age,
  method = "exhaustive"
)
summ <- summary(fit_bs)
## We need to select which features we want. Here we use RSS
## (residual sum of squares) to choose
coefs_bs <- coef(fit_bs, id = which.min(summ$rss))
coefs_bs
## regsubsets doesn't fit the full model for each combination
## to save time.
## To get a model summary, we could use the features identified to fit
## a full model
fit_bs <- lm(
  age ~ .,
  data = as.data.frame(small_methyl[, names(coefs_bs[-1])])
)
summary(fit_bs)
```

If we try to run best subset selection on the full dataset, we run into 
problems:

```{r bestsub_all}
fit_bs <- regsubsets(
  x = methyl_mat,
  y = age,
  method = "exhaustive"
)
```

The function in this case refuses to proceed unless we are sure we want
to wait around for a long time. It's worth noting this may not even finish
at all, as the process is likely to run out of memory given for 5000
features it needs to try $2^{4999}$ models (this number is too big to be 
represented as a floating point number in R!).

```{r bignum}
2^4999
```

This is a general problem with best subset selection (here termed
BS). Forward stepwise selection (FS) is an alternative that is a bit more
manageable.

```{r bstab, echo = FALSE, out.width="500px", fig.cap="Cap", fig.alt = "Alt"}
knitr::include_graphics("../fig/bs_fs.png")
```

Figure taken from [Hastie et al. (2020)](https://www.stat.cmu.edu/~ryantibs/papers/bestsubset.pdf),
published [here](https://doi.org/10.1214/19-STS733).


## Prostate data

Since we have many more features than observations in the 
methylation data from episode one, we'll instead work with the `Prostate`
dataset that's available in the `lasso2` package for this model selection
lesson. Further, this data is actually better suited to this statistical
approach than the methylation data.
Generally, best subset selection and similar approaches work really well 
when you have a relatively low number of features with relatively strong 
impact on the outcome, which is commonly not the case with methylation 
data.

```{r load-prostate}
library("lasso2")
data("Prostate")
```

These data come from a study examining the correlation between levels of 
prostate-specific antigen and other relevant clinical measures for
men receiving prostate surgery. The columns in this data are as follows:

- `lcavol`: log(cancer volume)
- `lweight`: log(prostate weight)
- `age`: age
- `lbph`: log(benign prostatic hyperplasia amount)
- `svi`: seminal vesicle invasion
- `lcp`: log(capsular penetration)
- `gleason`: [Gleason score](https://en.wikipedia.org/wiki/Gleason_grading_system)
- `pgg45`: percentage Gleason scores 4 or 5
- `lpsa`: log(prostate specific antigen)

The first few rows look like this:

```{r kable, echo = FALSE}
knitr::kable(head(Prostate))
```

We'll use `lcavol`, log-transformed cancer volume, as our outcome.
```{r firstfit}
cancer_volume <- Prostate$lcavol
prostate_mat <- as.matrix(Prostate[, -1])
fit_prostate <- lm(cancer_volume ~ prostate_mat)
summ_prostate <- summary(fit_prostate)
summ_prostate
```

## Scaling features

You may notice that the magnitude of the coefficients in this model vary
dramatically! This is because the scale of each feature is very different,
with some being integers with a large range (`age`) and others being numeric
with a smaller range (lcavol). While this doesn't make any difference when it
comes to statistical significance testing for our features,
it does make it difficult to compare 
the relative impact of different features.

Scaling features to have mean zero and standard deviation of one
means that a coefficient value of 1 means that the outcome changes by one
unit for every standard deviation of the input feature. You may not always
want to do this - unscaled features can be easier to understand at times, 
because coefficients then measure the unit change in the outcome variable
in response to unit changes in the input features.

To scale our data, we can use the `scale` function.

```{r scale-prostate}
prostate_scaled <- scale(prostate_mat)
```

> ## Exercise 
> 
> 1. Fit a model with the scaled data and compare with the unscaled data.
> 2. Which coefficients are different between the scaled and unscaled model? Why
>    do you think that is?
> 
> > ## Solution
> > 1. 
> >    ```{r fit-scale, fig.cap="Cap", fig.alt = "Alt"}
> >    fit_prostate_scaled <- lm(cancer_volume ~ prostate_scaled)
> >    summ_scaled <- summary(fit_prostate_scaled)
> >    summ_scaled
> >    plot(summ_prostate$coef[, "Pr(>|t|)"], summ_scaled$coef[, "Pr(>|t|)"],
> >      xlab = "Coefficients without scaling",
> >      ylab = "Coefficients with scaling"
> >    )
> >    ```
> > 2. The intercept is different, because the mean and scale of the input
> >    features have changed! If we were to scale the output, that would also
> >    change the intercept. The intercept quantifies the difference in means
> >    independent of changes in the features.
> > 
> {: .solution}
{: .challenge}

# Inference and prediction

Now that we've made a "baseline" model with all of the features we have 
available, we might want to see if we really need to measure all of these things
to be able to predict cancer volume. For example, we might find that the main
dependence in our data is between PSA and cancer volume, and most of the other
features. This type of procedure can provide a benefit for both *inference* and 
*prediction*.

Inference refers to our ability to use statistical models to 
inform our understanding of a biological system, in this case disease. For
example, if we find that PSA is , that might hint towards some mechanistic
relationship between this antigen and cancer progression that we can investigate
further experimentally.

Prediction refers to our ability to use features measured on unseen data to 
predict our outcome. In this case, this might be to predict cancer volume
from PSA. This can be useful if the features we use to predict are more easy to 
measure than our outcome. In this case, PSA can be measured from blood samples,
while cancer volume requires surgery. Further, predicting cancer volume from
PSA would allow us to prioritise patients likely to have higher cancer volume
for surgery, hopefully resulting in better outcomes for patients with severe
disease.

These two aims aren't always independent. For example, to be sure that the
inference we're making about the system under investigation, we want a strong
relationship (corresponding to high predictive accuracy). Furthermore, it's
always good to examine the features in a predictive model to be sure
they actually make sense. For example, if we found that the features
in a predictive 

To measure predictive accuracy, we can use the residual sum of squares (RSS).
This is the sum of the square of the length of the dashed lines in the plot
below. A low RSS value means that our predictions are very close to the 
values we really observe.

```{r residuals, echo = FALSE, fig.cap="Cap", fig.alt = "Alt"}
set.seed(66)
n <- 25
x <- rnorm(n)
y <- x * 2 + 1 + rnorm(n)
fit <- lm(y ~ x)
plot(x, y, pch = 19)
abline(fit)
for (i in seq_along(x)) {
    yhat <- predict(fit, newdata = data.frame(x = x[[i]]))
    lines(
        x = rep(x[[i]], each = 2), y = c(yhat, y[[i]]),
        col = "firebrick",
        lty = "dashed"
    )
}
```

> ## Exercise 
> 
> Do best subset with the prostate data.
> 
> > ## Solution
> > ```{r fit-bs-prostate}
> > fit_bs <- regsubsets(
> >   x = prostate_mat,
> >   y = cancer_volume,
> >   method = "exhaustive"
> > )
> > summ_bs <- summary(fit_bs)
> > coef(fit_bs, which.min(summ_bs$rss))
> > ```
> {: .solution}
{: .challenge}


# Model metrics

In the example above we used RSS (residual sum of squares) to choose a model.
However when comparing models with different numbers of features, this is
problematic. We could, for example, keep adding features that marginally 
reduce the RSS (because adding a feature will never make it worse!) and
under this framework we'll always select the biggest model.

For example, if we have as many features as observations, the fit is always
perfect ($R^2$ is exactly 1 and $RSS$ exactly 0).

```{r perfectfit}
square_mat <- prostate_mat[90:nrow(prostate_mat), ]
dim(square_mat)
short_volume <- cancer_volume[1:ncol(prostate_mat)]
fit_square <- lm(
  short_volume ~ 0 + .,
  data = as.data.frame(square_mat)
)
summary(fit_square)
sum(residuals(fit_square)^2)
```

There are other ways to measure model performance while accounting for the
complexity of the model. For example, adjusted $R^2$ is similar to the normal
R^2 measure that estimates the variation explained by the model, while also
accounting for the number of features. This is explained
in more detail in [the multiple regression lesson](https://carpentries-incubator.github.io/multiple-linear-regression-public-health/).

There are also measures known as "information criteria" that are useful for
model comparison and selection. These measure the likelihood of observing
the data we have observed under the model under investigation, while penalising
complex models. Two popular examples are BIC (Bayesian information criterion)
and AIC (Akaike information criterion).

> ## BIC and AIC
> 
> BIC and AIC have some notable differences. For one, BIC tends to penalise
> parameters more than AIC, and AIC does not accounts for the number of
> observations.
> Because of this, AIC may select overly complex models with low samples sizes.
>
> Formally, BIC is defined
> 
> $$
>   BIC = p \log(n) - 2 \log(L)
> $$
> 
> where $p$ is the number of features in the model, $n$ is the number of 
> observations, and $L$ is the likelihood of observing the data given our model.
> Similarly,
> 
> $$
>   AIC = 2p - 2\log(L)
> $$
>
> The likelihood of the data given a model is the probability of observing
> the given data under that model. By checking the likelihood of the same data
> under different models, we can narrow down our choice of model.
> 
> In the example below, the maximum likelihood estimate is shown in red,
> and two alternative models are shown in blue and green. The data are less 
> likely to be observed under these models, as we can see in the left panel
> of the plot. We can see this concretely in the right panel, as the likelihood
> of the data is highest in the light region where the maximum likelihood 
> estimate resides, and the points representing the other models reside in
> lower likelihood regions.
>
> This is a difficult concept to grasp at first, but can be a very powerful
> tool for thinking about data and models, and is a foundation of many more
> advanced topics in statistics.
> 
> ```{r likelihood, echo = FALSE, fig.width = 10, fig.cap="Cap", fig.alt = "Alt"}
> library("viridis")
> set.seed(42)
> noise_sd <- 1
> nobs <- 20
> slope <- 2
> intercept <- 1
> maxlim <- max(abs(slope), abs(intercept)) * 2
> maxlim <- max(maxlim, 5)
> lims <- c(-maxlim, maxlim)
> x <- rnorm(nobs, mean = 0, sd = 1)
> noise <- rnorm(nobs, mean = 0, sd = noise_sd)
> y <- (slope * x) + (intercept) + noise
> n <- 200
> 
> s <- seq(-maxlim, maxlim, length.out = n)
> 
> ll <- matrix(ncol = n, nrow = n)
> coef <- matrix(ncol = n, nrow = n)
> for (i in seq_along(s)) {
>     coef[, ] <- s
> }
> loglik <- function(slope, intercept, x, y, noise_sd) {
>     sum(dnorm(y, mean = (slope * x) + intercept, sd = noise_sd, log = TRUE))
> }
> 
> for (i in seq_along(s)) {
>     for (j in seq_along(s)) {
>         ll[i, j] <- loglik(s[[i]], s[[j]], x, y, noise_sd)
>     }
> }
> slope1 <- runif(1, -maxlim, maxlim)
> int1 <- runif(1, -maxlim, maxlim)
> slope2 <- runif(1, -maxlim, maxlim)
> int2 <- runif(1, -maxlim, maxlim)
> 
> ind_mle <- arrayInd(which.max(ll), dim(ll))
> 
> par(mfrow = 1:2)
> plot(
>     x, y,
>     main = "Data and three possible models",
>     pch = 19
> )
> abline(
>     a = coef[ind_mle[[2]]],
>     b = coef[ind_mle[[1]]], col = "firebrick"
> )
> abline(
>     a = int1,
>     b = slope1, col = "dodgerblue"
> )
> abline(
>     a = int2,
>     b = slope2, col = "forestgreen"
> )
> 
> image(s, s, ll,
>     main = "Likelihood of the data given model parameters",
>     xlab = "slope", ylab = "intercept",
>     col = viridis(50, option = "A", direction = 1),
>     xlim = lims, ylim = lims
> )
> abline(v = 0, lty = "dashed")
> abline(h = 0, lty = "dashed")
> points(
>     coef[ind_mle[[1]]], coef[ind_mle[[2]]],
>     pch = 19, cex = 2, col = "firebrick"
> )
> points(
>     slope1, int1,
>     pch = 19, cex = 2, col = "dodgerblue"
> )
> points(
>     slope2, int2,
>     pch = 19, cex = 2, col = "forestgreen"
> )
> 
> ```
{: .callout}


> ## Exercise
>
> Select the best model based on BIC. How does this differ to the best RSS 
> model?
> 
> > ## Solution
> > 
> > 
> > ```{r selbic}
> > coef(fit_bs, id = which.min(summ_bs$bic))
> > coef(fit_bs, id = which.min(summ_bs$rss))
> > ```
> {: .solution}
{: .challenge}


> ## Selecting a model
> 
> What we've done so far is to select models using their performance on the
> data we've used to fit the model. This isn't always ideal, because it assumes
> that the data we're using to fit the model is exactly representative
> of the entire population we're interested in.
> However, usually our experiments are limited to only a small sample of
> a very large population (in this case, 97 patients out of a total population
> of many millions!)
> 
> One way to address this is to split the data in two. We then use part of
> the data to fit the model, and part of the data to evaluate the accuracy of
> the model. This may reduce the amount of data we have available to fit the 
> model, but it means we can be more confident about the performance of the
> model afterwards.
> This is especially important when our goal is prediction. Predictive 
> performance in the data used to train the model is typically very good,
> even when predictive performance on new, unseen data is very poor.
> 
> Predictive performance on unseen data can be thought of a measure of how
> well our findings *generalise*. This very important topic is 
> addressed in much more detail in the Carpentries machine learning course.
> 
{: .callout}

# Forward stepwise selection

Since best subset selection is computationally hard, we can instead do an 
approximation to. One of these is forward stepwise selection.

This is an iterative process. First, we fit a model with just an intercept
and one feature. Then

1. Pick the most significant feature
2. fit a model with that feature and every other
3. if any are a significant improvement, pick the model that has the best
   improvement and return to 2.
4. Otherwise, stop.


> ## Exercise
> 
> 1. Apply forward subset selection to the prostate dataset (hint: try setting
>    `method = "forward"`).
> 2. Select the best model based on BIC.
> 3. Fit a model with `lm` using these features.
> 
> > ## Solution
> >
> > ```{r forward}
> > fit_fwd <- regsubsets(
> >   x = prostate_mat,
> >   y = cancer_volume,
> >   method = "forward"
> > )
> > summ_fwd <- summary(fit_fwd)
> > est_coef_fwd <- coef(
> >   fit_fwd,
> >   id = which.min(summ_fwd$bic)
> > )
> > est_coef_fwd
> > chosen_coef_fwd <- names(est_coef_fwd)
> > fit_fwd_lm <- lm(
> >   cancer_volume ~ .,
> >   data = as.data.frame(prostate_mat[, chosen_coef_fwd[-1]])
> > )
> > summary(fit_fwd_lm)
> > ```
> {: .solution}
{: .challenge}


# Reverse stepwise selection

If we have a model as a starting point and we want to slim it down, we
can do reverse stepwise selection. In this case we might use all of the 
features available because we only have a few, but if we had more features
we might pick all those we think might be related to the outcome.

The process is similar to forward stepwise  selection, but here we start with
a model containing all of the features we might include. We then

1. Fit models by dropping each feature in turn.
2. Choose the model with the smallest drop in performance (or BIC, AIC).
3. If no features have a small drop in performance, stop.
4. Otherwise, return to step 1.

The `leaps` package also allows us to do this process.

> ## Exercise
> 
> Do reverse subset selection and compare the forward and reverse stepwise
> selection models.
> 
> > ## Solution
> >
> > ```{r reverse}
> > fit_bwd <- regsubsets(
> >   x = prostate_mat,
> >   y = cancer_volume,
> >   method = "backward"
> > )
> > summ_bwd <- summary(fit_bwd)
> > est_coef_bwd <- coef(
> >   fit_bwd,
> >   id = which.min(summ_bwd$bic)
> > )
> > est_coef_bwd
> > chosen_coef_bwd <- names(est_coef_bwd)
> > fit_bwd_lm <- lm(
> >   cancer_volume ~ .,
> >   data = as.data.frame(prostate_mat[, chosen_coef_bwd[-1]])
> > )
> > summary(fit_bwd_lm)
> > ```
> {: .solution}
{: .challenge}

> ## Forward-reverse stepwise selection (both at once)
> 
> As well as doing forward stepwise selection and reverse stepwise selection,
> it's also possible to do a combined procedure. Here, we might start with
> only an intercept, and try to add one feature at a time. However, after
> adding each feature, we then also try to drop each of the features we've added
> so far. After dropping
> or adding features, we proceed only if the model improves (when adding)
> or doesn't drastically worsen (if removing).
> 
> This is more computationally demanding than forward and reverse stepwise 
> selection, and it is important to consider how many models we're fitting
> when interpreting the results. With a lot of different combinations,
> some will probably look very good!
> 
{: .callout}

{% include links.md %}
